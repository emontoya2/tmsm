[
  {
    "objectID": "datacs6.html",
    "href": "datacs6.html",
    "title": "6  Case study 6",
    "section": "",
    "text": "Arctostaphylos glauca (Big Berry Manzanita). Source:calscape.org\n\n\n\n\n\nIn plants, the function of the xylem tissue is to transport water and minerals, and this forms part of a network that ensures that the plant can transport essential resources and maintain its growth and development. A study by Pratt et al. (2021) investigated the trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field. The study, which used structural equation modeling, showed the importance of understanding that the roles of different cell types in the xylem are important for recognizing the functional trade-offs that govern xylem traits, and emphasized the critical role of minimum hydrostatic pressures in plant growth and development.\nFor their study, various plant physiological measurements for 29 different plant species were collected. Below is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water \\(\\Delta \\text{Relative water content}/\\Delta \\text{MPa}\\).\n\nThese variables provide insight on how the xylem functions.\nHere, we apply principal component analysis (PCA) to determine if we can identify key patterns in these data, which describe various aspects related to the structure, function, and properties of xylem, as well as underlying factors or dimensions that may be driving the variation in the data. The data are shown in the table below.\n\n\n\n\n\n\n\n\nNumerical and graphical summaries of the plant physiological variables are provided below:\n\n\n\n\n\n(a) Scatterplot matrix showing the pairwise relationships between the plant physiological variables in the dataset\n\n\n\n\n\n\n\n\n(b) correlation matrix table that shows the correlation coefficients between each pair of plant physiological variables in the dataset.\n\n\n\n\nFigure 6.1: Scatterplot matrix and correlation matrix of the plant physiological variables.\n\n\nThe graphical and numerical summaries show that the variables have both positive and negative correlations, with many showing clear linear associations, although the strength of a given linear relationship depends on which pair of variables is considered. There are 84 possible 3D scatterplots that could be examined when dealing with the nine variables, so it is difficult to examine beyond pairwise relationships. That is, graphical or numerical summaries provide limited insight when attempting to explore the interdynamics of the variables when the data set is very large.\nTo better understand the characteristics of xylem tissue in shrub species, it is important to identify the underlying factors or dimensions that drives the variation in the data. By identifying these factors, we can more easily interpret the data set, potentially revealing underlying patterns, and gaining insights into how these shrubs respond to water availability in a semi-arid chaparral environment. Principal component analysis (PCA) is a useful tool for achieving this goal. In this case study, PCA is applied to the plant physiological measurements to simplify the interpretation of the variables and reveal the underlying patterns that drive the variation in the data\nAccording to Kaiser’s rule, three principal components capture most of the variation contained in these nine variables. In fact, about 76% of the variation in the plant physiological variables is explained by the first three principal components, and this is an acceptably large percentage.\nTo simplify the interpretation of the principal components, varimax rotation is applied. Overall, the first principal component (PC) is viewed as an indicator of a shrub’s adaptability to drought conditions with respect to water efficiency, while the second PC is viewed as a measure of the adaptability of shrubs to drought conditions with respect to mechanical strength and water storage. The third PC, on the other hand, may be considered as an indicator of the adaptability of shrubs to drought conditions in terms of their capacity to balance parenchyma and fiber to maintain essential plant functions. These findings provide a view into how these shrubs respond to water availability in a semi-arid chaparral environment.\nIt is important to note that PCA is an exploratory method rather than an inferential one. While it can reveal patterns and relationships in the data, it does not provide statistically significant conclusions or results. Overall, PCA is a powerful method for exploring complex datasets and revealing underlying patterns, but it should not be used as the sole basis for making conclusions.\n\n\n\n\nPratt, RB, AL Jacobsen, MI Percolla, ME De Guzman, CA Traugh, and MF Tobin. 2021. “Trade-Offs Among Transport, Support, and Storage in Xylem from Shrubs in a Semiarid Chaparral Environment Tested with Structural Equation Modeling.” Proceedings of the National Academy of Sciences 118 (33)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Training modules on selected statistical methods",
    "section": "",
    "text": "This repository hosts the source code for this resources. This open source project was developed by Eduardo L. Montoya. If you find the contents useful, please cite or acknowledge this project to encourage further use by others. Please use the following citation:\n\nMontoya, E. L. (2022). Training modules on selected statistical methods. GitHub repository, https://github.com/emontoya2/tmsm"
  },
  {
    "objectID": "index.html#purpose-of-this-resource",
    "href": "index.html#purpose-of-this-resource",
    "title": "Training modules on selected statistical methods",
    "section": "Purpose of this resource",
    "text": "Purpose of this resource\nThe purpose of this project is to help students learn and apply selected statistical methods. Each module focuses on specific areas with integrated R code offering students an informative way to learn and practice these methods. Most R code within a given section are linked to video demonstrating how to run the code in RStudio.\nR is chosen because it is open source, platform independent, remains among the most popular and best statistical software packages, provides great on-line documentation, and is well supported with a large and useful user community from a diverse range of backgrounds. Furthermore, most sections incorporate R code, enabling students to learn and practice the material as they progress through a given module. To use R, the integrated development environment (IDE) RStudio will be used, which has made R more user-friendly for many users."
  },
  {
    "objectID": "index.html#organization-of-the-modules",
    "href": "index.html#organization-of-the-modules",
    "title": "Training modules on statistical methods",
    "section": "Organization of the modules",
    "text": "Organization of the modules\nThe material is organized according to the order topics of this nature are introduced in practice. Modules will then follow similar patterns: Module core material organized into sections that use the case studies to illustrate important concepts and/or applications of methods. Those new to R should work through the appendix. If you have used R in the past, but have not imported .csv files nor used a formula expression of the form response ~ explanatory, then the reader should review Section C.1 and Section C.2.\nAll modules have R code embedded. With in a given module, the R code is self-contained, although a given R function is only described when they are first used. Since the code is only self-contained with in a given module, code to import a particular data set or load a particular R package may appear multiple times in different modules. Also, any subsequent uses of a given function will not provided a description but the reader may search for the name of the function in the search bar provided in the upper left of the module."
  },
  {
    "objectID": "index.html#what-this-resource-is-not",
    "href": "index.html#what-this-resource-is-not",
    "title": "Training modules on selected statistical methods",
    "section": "What this resource is not",
    "text": "What this resource is not\nThis resource does not aim to provide an all-encompassing guide to statistical methods or their implementation in R. Numerous statistical methods and various ways to conduct these methods in R can be utilized in practice, many of which are not covered in these modules. That being said, the concepts and techniques presented are intended to enhance the reader’s ability to apply the covered methods to their research, as well as to connect their statistical knowledge with more advanced methods."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Training modules on selected statistical methods",
    "section": "License",
    "text": "License\nThis resource is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "datacs3.html",
    "href": "datacs3.html",
    "title": "3  Case study 3",
    "section": "",
    "text": "Left: Smog blanketing Bakersfield. Right: Size comparisons of PM particles\n\n\n\n\nThe effects of air pollution are diverse and numerous, such as increased mortality to increased sensitivity of an ecosystem at high concentrations. The pollutants that pose among the highest risk to California are ground level ozone and particulate matter (PM). California continues to mandate ambient air quality standards that tend to be more stringent than national standards. However, reducing air pollution concentrations to acceptable levels remains an on-going challenge in California where seven and eight of its cities (including Bakersfield and others from the Central Valley) rank in the top 10 of the highest levels of ozone and particulate matter (PM) pollution, respectively (American Lung Association, 2017).\nPM refers to tiny particles of solid or semi-solid material found in the atmosphere, and it is one of the six common air pollutants identified by the U.S. Environmental Protection Agency (EPA, 2018), and it varies in size, but most monitoring is for two size ranges referred to as PM2.5 and PM10 (particulate matter that is less than 10 micrometers in diameter). For this case study, daily measurements of PM10 in micrograms per cubic meter (\\(\\mu g/m^3\\)) are considered from various monitoring stations in three counties (Kern, Tulare, and Fresno) for the year 2021. The monitoring stations provide the daily average PM10 levels, measurements of temperature (\\(^oF\\)) and wind speed (knots or kts).\nIn addition, daily measurements of the US Air Quality Index (an index for reporting air quality) from various monitoring stations for same counties and year are considered. The higher the Air Quality Index (AQI) value, the greater the level of air pollution and the greater the health concern. For more details regarding the AQI, visit AirNow (https://www.airnow.gov/). More details regarding PM10, as well as access to PM and AQI data, are publicly available through the EPA Air Data website (https://www.epa.gov/outdoor-air-quality-data).\nThe following questions are explored for the 2021 data:\n\nDo daily PM10 levels differ for the three counties?\nIs daily PM10 related to daily temperature across all three counties?\n\n\n\n\n\n\n\n\n\nThe sample means show that Tulare county had the highest mean daily PM10 as well as the highest average daily temperature. The boxplots show similar behavior with Tulare county having the highest median daily PM10. The boxplots also reflect outlying values for all counties and similar variation in daily PM10 for the three counties. As for the scatterplot, daily PM10 and daily temperature demonstrate a positive linear relationship, with the variability of daily PM10 being less pronounced at the lowest and highest daily temperatures. The scatterplot also reflects clear outlying observations.\n\n\n\n\n\nTable 3.1:  The average and standard deviation of daily PM10 and daily temperature for each county. \n \n county \n    mean PM10 \n    mean temperature \n    sd PM10 \n    sd temperature \n  \n\n\n Fresno \n    41.10 \n    65.87 \n    31.88 \n    14.17 \n  \n\n Kern \n    35.07 \n    67.49 \n    32.26 \n    16.01 \n  \n\n Tulare \n    51.54 \n    67.77 \n    32.37 \n    14.22 \n  \n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 3.1: Left panel displays boxplots of PM10 against county. Right panel is a scatterplot of PM10 against temperature\n\n\nAre the features reflected in the numerical and graphical summaries distinct enough to convince one that these levels of air quality measurements differ by county? Regarding PM10 and temperature, although they appear to be positively associated, is the relationship strong enough to suggest that the positive association shown is due to more than just random variation? Using statistical methods to be addressed later, the data provides convincing evidence that average daily PM10 levels vary across the three counties (p-value \\(\\approx 0\\)), and that the data provides strong evidence that there that daily PM10 (linearly) related to daily temperature across all three counties (p-value \\(\\approx 0\\)).\n\nThis case study makes use of observational data, which provided strong evidence that there is an association between the three counties and daily levels of PM10, as well as strong evidence of an association between daily PM10 and daily temperature. The counties were not randomly selected, so these conclusions only apply to the three counties considered here."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Module: Advanced Exploratory Data Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA).\n\nExploratory Data Analysis (EDA) involves summarizing and analyzing datasets with the aim of discovering patterns, relationships/trends, or anomalies in the data. Part of EDA involves using numerical and graphical summaries to explore insights into the data structure, main characteristics, and potential relationships between variables. EDA is important as it can guide further analysis, hypothesis testing, and modeling decisions.\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that can be employed as part of EDA. PCA is particularly useful when dealing with high-dimensional datasets, such as those with a large number of variables. In such instances, visualizing and identifying patterns, trends, or relationships between variables might be challenging. PCA transforms the original variables into a smaller set of new uncorrelated variables called principal components. These components capture much of the variation in the data and can potentially reveal underlying patterns or structures in the data.\n\nModule chapters:\n\nPCA"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Graduate programs vary with regard to required statistical training. The intention of this resource is to enable graduate students to improve their ability to apply appropriate statistical methodologies."
  },
  {
    "objectID": "preface.html#some-prerequisites",
    "href": "preface.html#some-prerequisites",
    "title": "Preface",
    "section": "Some prerequisites",
    "text": "Some prerequisites\nPlease note that this resource is designed for practitioners and does not assume prior knowledge of statistical concepts. R and RStudio will also have to be installed (see Chapter A for installation instructions)."
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nCSUB Title Vb program for supporting the creation of this resource.\nThe R Core team, RStudio, Quarto, and Github for their continued advancement of these open-source softwares.\nDr. Brandon Pratt for providing the data for the case study on xylem characteristics in shrub species, as well as providing feedback for the case study.\nDr. Tien-Chieh Hung for providing the data for the case study on Delta smelt.\nDr. David Riedman for providing the data for the case study on school shootings."
  },
  {
    "objectID": "preface.html#contributing",
    "href": "preface.html#contributing",
    "title": "Preface",
    "section": "Contributing",
    "text": "Contributing\nContributions are encouraged on any part of the project, including:\n\nImprovements to the text or code, such as clarifying unclear sentences, notation, and fixing typos. This resource is not error-free, so please inform the author of any errors so that they can be corrected.\nChanges to the R code to make it easier to read or to do things in a more efficient way.\nSuggestions on topics to consider adding."
  },
  {
    "objectID": "casestudies.html",
    "href": "casestudies.html",
    "title": "Module: Case Studies",
    "section": "",
    "text": "The chapters in this module cover various small examples of analyzing data from case studies or research projects. The data used in a given study are referenced throughout the modules to demonstrate how statistical software and tools can be utilized.\nModule chapters:\n\nCase Study 1\nCase Study 2\nCase Study 3\nCase Study 4\nCase Study 5\nCase Study 6"
  },
  {
    "objectID": "datacs.html",
    "href": "datacs.html",
    "title": "1  Case study 1",
    "section": "",
    "text": "Figure: Delta smelt Source:https://calfish.ucdavis.edu/species/\n\n\n\n\nThe Delta smelt is a small fish that smells like peeled cucumbers, with lifespan of about a year. It is a species that lives in Sacramento-San Joaquin River Delta, where more than half of fresh water in the state is moved through the delta for usage by residents, as well as the agriculture and industry sectors. The delta smelt is keystone species that is sensitive to changes in it’s habitat and its population is endangered. Its decline is of concern since the Delta smelt’s population serves as an indicator of the health of the Delta’s ecosystem as this fish is an important part of the food chain.\nLight intensity is the amount of illumination at the water surface. The turbidity of water is measure haziness or cloudiness of the water. Both of these environmental factors have been found to influence fish behavior. The effects of these environmental factors on the feeding, growth, and survival of larvae were investigated on the Delta smelt, where such understanding would be vital for improving the conservation of this endangered fish species (Tigan et al. (2020)). For their investigation, three sets of rearing trials were conducted where larvae were cultured under different levels of turbidity (measures in nephelometric turbidity units (NTUs)) and light intensity (measured in \\(\\mu mol/m^2/s\\)) . Delta smelt larvae feeding activity was observed throughout the initial adjustment of light and turbidity levels to ascertain feeding ability and behavior at the different levels.\nFor brevity, we focus on the following question: Do levels of light intensity (measured in \\(\\mu mol/m^2/s\\)) affect survival rates of early-stage Delta smelt larvae?\nWe address this question using data on the percentage of early-stage Delta smelt larvae (0–40 days post hatch) that survived when reared under different conditions (low, medium, high) of light intensities, measured in \\(\\mu mol/m^2/s\\). The data are shown in the table below.\n\n\n\n\n\n\n\n\nNumerical and graphical summaries of the survival of all early-stage Delta smelt larvae (0–40 dph) in the study reared under different light intensities are provided below.\n\n\n\n\n\n(a)\n\n\n\n\nFigure 1.1: Survival of all early-stage Delta smelt larvae (0–40 dph) in the study reared under different light intensities.\n\n\n\n\n\n\n\nTable 1.1:  A comparison of the mean and standard deviation of Survival of all early-stage Delta smelt larvae (0–40 dph) in the study reared under different light intensities. \n \n Light \n    Mean \n    SD \n  \n\n\n low \n    59.606 \n    12.809 \n  \n\n med \n    75.518 \n    12.242 \n  \n\n high \n    58.185 \n    6.345 \n  \n\n\n\n\n\n\n\n\nWhen reared under different conditions of light intensities, the data summaries show that early-stage Delta smelt larvae (0–40 days post hatch) tended to have a higher percentage of survival under medium light intensity (sample mean survival percentage of 75.518) and similar percentages of survival under low and high light intensity. Is the sample mean survival percentage under medium light intensity different enough compared to the mean under low and high light intensity to convince you that survival rates are affected by the strength of light intensity? Using statistical methods to be discussed later, the data provides moderate evidence that survival rates are affected by the strength of light intensity (p-value=.059)\n\nTreatments (low, medium, and high light intensities) were imposed on the subjects (early-stage Delta smelt larvae), but the treatment levels were not randomly assigned. Although the data provides suggestive evidence that light intensity affects survival rates, the analysis can not say whether changes in light intensity cause changes in survival rates. While there is a suggestive association between the treatment and rate of survival, other possible explanations may exist. Because the subjects were not randomly selected from some larger population, our findings that survival rates are affected by the strength of light intensity only apply to those subjects in the sample and not some larger population.\n\n\n\n\nTigan, Galen, William Mulvaney, Luke Ellison, Andrew Schultz, and Tien-Chieh Hung. 2020. “Effects of Light and Turbidity on Feeding, Growth, and Survival of Larval Delta Smelt (Hypomesus Transpacificus, Actinopterygii, Osmeridae).” Hydrobiologia 847 (13): 2883–94. https://doi.org/10.1007/s10750-020-04280-4."
  },
  {
    "objectID": "datacs2.html",
    "href": "datacs2.html",
    "title": "2  Case study 2",
    "section": "",
    "text": "Source: Wikipedia\n\n\n\n\nReward systems are utilized in schools and the workplace, but are rewards operating in the opposite way from what is intended? Do external incentives promote creativity? To address this, researchers investigated whether people would tend to display more creativity when they are thinking about intrinsic or extrinsic motivations Ramsey and Schafer (2013). In this study, undergraduate students of similar creative writing experience were randomly assigned to one of two groups . One group received intrinsic questionnaires, and the other received extrinsic questionnaires.\nThe intent of this treatment (the questionnaires) was to have student establish a thought pattern concerning a type of motivation (intrinsic – doing something because it is rewarding or enjoyable; extrinsic – doing something because we want to earn a reward or avoid punishment). Researchers speculated that those who were thinking about intrinsic motivations would display more creativity than subjects who were thinking about extrinsic motivations. All subjects were instructed to write a Haiku, and those poems were evaluated for creativity by a panel of judges\nThe resulting experimental data is shown below, displaying the creativity score (Score) and the type of questionnaires (Treatment) assigned for a given undergraduate. Do the data provide evidence that creativity scores are affected by type of motivation?\n\n\n\n\n\n\n\n\nNumerical and graphical summaries of the creativity scores for each motivation type are provided below.\n\n\n\n\n\n(a)\n\n\n\n\nFigure 2.1: Boxplot (a) and histogram (b) of creativity by motivation type\n\n\n\n\n\n\n\nTable 2.1:  The median, IQR, mean, and standard deviation of creativity scores for each group \n \n Treatment \n    Mean \n    SD \n  \n\n\n Extrinsic \n    15.74 \n    5.25 \n  \n\n Intrinsic \n    19.88 \n    4.44 \n  \n\n\n\n\n\nThe creativity scores tended to be lower for the students in the extrinsic group (-4.14 difference in sample means), with both groups showing similar variability in scores. Are the scores between the two groups different enough to convince you that those thinking about intrinsic motivations display more creativity than subjects that were thinking about extrinsic motivations? If not, how much larger should the mean be? Answers to such questions require inferential methods that will be discussed in a future chapter.\nFor now, be aware that the data provides convincing evidence that subjects thinking about intrinsic motivations would receive a higher creativity scores than those thinking about extrinsic motivation (two-sided p-value = .006). The 95% confidence interval of (-7.010, -1.277) shows that the mean creativity score for the extrinsic group is expected to be between 1.277 and 7.010 lower than the mean creativity score for the intrinsic group.\n\nWe may conclude that the differences in creativity scores between groups was caused by motivation questionnaire type since the treatment (motivation questionnaire type) was randomly assigned to the undergraduates. We may only infer this for the subjects in the study and not a larger population since the students were not randomly selected from a population.\n\n\n\n\nAmabile, Teresa M. 1985. “Motivation and Creativity: Effects of Motivational Orientation on Creative Writers.” Journal of Personality and Social Psychology 48 (2): 393–99. https://doi.org/10.1037/0022-3514.48.2.393.\n\n\nRamsey, F. L., and D. W. Schafer. 2013. “The Statistical Sleuth: A Course in Methods of Data Analysis.” Cengage Learning 30 (4): 413–14. https://doi.org/10.1080/00224065.1998.11979882."
  },
  {
    "objectID": "datacs4.html",
    "href": "datacs4.html",
    "title": "4  Case study 4",
    "section": "",
    "text": "Source: https://hopkinsdiabetesinfo.org/glossary/body-mass-index/\n\n\n\n\nMotivated by the work of “Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults” (2019), a case study by Wright et al. (2020) explored global patterns of obesity across different regions. The questions in their case study were related to the association between Body mass index (BMI) rates with region (rural vs urban) and countries. BMI may be considered a measurement of health, where BMI is defined as an individual’s weight divided by the individual’s height. The data used for the case study is described “Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults” (2019), where the data consisted of the following variables:+\n\nCountry: Countries in the sample\nSex: Men or women. The data recorded only contained data for groups of individuals described as men or women.\nRegion: Type of region (rural or urban)\nYear: 1985 or 2017\nBMI: Averaged BMI values for the given year, sex, region, and country.\n\nWe use the data to assess if BMI rates for urban and rural differ. The data are shown in table below.\n\n\n\n\n\n\n\n\nNumerical and graphical summaries of the BMI under different regions are provided below:\n\n\n\n\n\n(a)\n\n\n\n\nFigure 4.1: Survival of all early-stage Delta Smelt larvae (0–40 dph) in the study reared under different light intensities.\n\n\n\n\n\n\n\nTable 4.1:  A comparison of the mean and standard deviation of BMI rates for each region \n \n Region \n    Mean \n    SD \n  \n\n\n Rural \n    24.453 \n    2.947 \n  \n\n Urban \n    25.304 \n    2.632 \n  \n\n\n\n\n\nThe graphical and numerical summaries show that the BMI rates for urban areas tend to be higher in the sample. However, are the samples means different enough to suggest that BMI rates are affected by region? Using statistical methods to be addressed later, the data provides convincing evidence that region affects BMI rates (p-value \\(<\\) 0)\n\nThis case study make use of observational data, which provided convincing evidence that there is an association between BMI rates with both region and sex. The sample were not randomly selected, so these conclusions only apply to the men and women in the sampled countries.\n\n\n\n\n“Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults.” 2019. Nature 569 (7755): 260–64.\n\n\nWright, Carrie, Leah Jager, Margaret Taub, and Stephanie Hicks. 2020. “Exploring Global Patterns of Obesity Across Rural and Urban Regions.” Open Case Studies Version v1.0.0. https://github.com/opencasestudies/ocs-bp-rural-and-urban-obesity."
  },
  {
    "objectID": "datacs5.html",
    "href": "datacs5.html",
    "title": "5  Case study 5",
    "section": "",
    "text": "Students from Miguel Contreras Learning Center High School in Los Angeles demonstrate in front of City Hall after walking out of school to protest U.S. gun violence on May 31, 2022. Photo by Lucy Nicholson, Reuters Source: https://calmatters.org/newsletters/whatmatters/2022/07/mass-shooting-california-gun-laws/\n\n\n\n\nSchool shootings occur more often in the US than any other country (Mosechkin and Krukovskiy (2019)). The K-12 School Shooting Database (Riedman (2022)) contains data on every instance a gun is brandished, is fired, or a bullet hits K-12 school property. Recently, Hilaire et al. (2022) used this data source to investigate the relationship between perpetrators’ race and how shootings are reported by the media. They found that there were differences by race in the characteristics of school shootings and media reporting of school shootings. Reeping et al. (2022) found that more permissive firearm laws and higher rates of gun ownership were associated with higher rates of school shootings. The latest raw data (Jan 1970-Nov 2022) was provided by the founder and maintainer of the K-12 School Shooting Database, David Riedman. Using this data, we explore whether the level of media coverage of the incident (Local, Regional, or National) differs across age groups and examine if the number of victims is associated with the type of weapon(s) used.\nIn all, the data consists of 46 variables. The variables included:\n\n\nSchool_Level: education-level of the school attacked as (1) high school, (2) junior high school, (3) middle school, (4) elementary school, (5) other.\n\nYear: year\n\nMonth: month\n\nNumber_Victims: Number of victims killed or wounded\n\nMedia_Attention: Highest level of media coverage (local, regional, or national)\n\nTime_Period: Time period in which the incident occurred (After school, Afternoon classes, … , Sport Event)\n\nagegroup: Age of the shooter (child, teen, adult)\n\nState: State where in incident occurred.\n\nWeapon_Type: The type of weapon used ()\n\nThe reader is referred to K-12 School Shooting Database data methodology page for more details regarding variables in the data set.\nThe data are shown in table below.\n\n\n\n\n\n\n\n\nNumerical and graphical summaries of the relevant variable is provided below:\n\n\n\n\n\n(a) Violin boxplot\n\n\n\n\nFigure 5.1: Graphically summary\n\n\n\n\n\n\n\n\n\n\nTable 5.1:  A comparison of the median and interquartile range of the number victims under each weapon type used by the shooter. Only data in which the weapon was known was used. \n \n Weapon_Type \n    Median \n    IQR \n  \n\n\n Handgun \n    1.0 \n    1.0 \n  \n\n Multiple Handguns \n    1.5 \n    3.0 \n  \n\n Multiple Rifles \n    3.5 \n    5.0 \n  \n\n Other \n    0.0 \n    1.0 \n  \n\n Rifle \n    1.0 \n    2.5 \n  \n\n Shotgun \n    1.0 \n    3.0 \n  \n\n\n\n\n\n\n\n\nThe violin boxplot shows that the number of victims differs depending on the type of weapon used, with more outlying observations under rifles or shotguns. Using statistical methods to be addressed later, the data provides very strong evidence that the number of victims varies significantly with the type of weapon used (p-value \\(\\approx\\) 0).\n\nThis case study makes use of observational data, which provided convincing evidence that there is an association between both sets of variables considered. The sample were not randomly selected, so these conclusions only apply to the sample.\n\n\n\n\nHilaire, Breahannah, Laurie O Campbell, Viki P Kelchner, Eric D Laguardia, and Cassandra Howard. 2022. “Not Another School Shooting: Media, Race, and Gun Violence in k-12 Schools.” Education and Urban Society, 00131245221092739.\n\n\nMosechkin, Ilya N, and Vladimir Y Krukovskiy. 2019. “Victimological Measures for Preventing School Shootings: Expert View.” International Journal of Criminal Justice Sciences 14 (2): 256–66.\n\n\nReeping, Paul M, Louis Klarevas, Sonali Rajan, Ali Rowhani-Rahbar, Justin Heinze, April M Zeoli, Monika K Goyal, Marc A Zimmerman, and Charles C Branas. 2022. “State Firearm Laws, Gun Ownership, and k-12 School Shootings: Implications for School Safety.” Journal of School Violence 21 (2): 132–46.\n\n\nRiedman, David. 2022. “K-12 School Shooting Database.” https://k12ssdb.org."
  },
  {
    "objectID": "drawingconclusions.html",
    "href": "drawingconclusions.html",
    "title": "8  Drawing statistical conclusions",
    "section": "",
    "text": "AI art generated from the text “statistical conclusions”"
  },
  {
    "objectID": "drawingconclusions.html#relationships-among-variables",
    "href": "drawingconclusions.html#relationships-among-variables",
    "title": "8  Drawing statistical conclusions",
    "section": "\n8.2 Relationships among variables",
    "text": "8.2 Relationships among variables\nWhen two variables show some relationship or connection with one another, they are called associated variables. If it is believed that a variable might affect or be associated with changes in the another variable, then in such cases, the first variable is designated the explanatory variable and the second the response variable.\nFor example, can the height of a tree provide a reasonable estimate of its volume? Do certain types of diets significantly affect the early growth of chicks? Such questions can be explored using numerical and graphical summaries of data. However, whether the trends or patterns seen in these graphs can be attributed mostly to sampling, natural variability, or random error is uncertain.\n\n\n\n\n\n\nNote\n\n\n\nRecall the case study given in Chapter 1 This data set has three variables (two categorical and one numerical variable): level of exposure of light intensity (Light), turbidity level exposure (Turbidity), and the percentage of larvae that survived (Survival). The questions posed where: Do levels of light intensity (measured in \\(\\mu mol/m^2/s\\)) affect survival rates of early-stage larvae early-stage Delta smelt larvae? What about levels of turbidity (measures in nephelometric turbidity units (NTUs))? Since we would like to know whether these factor variables might affect the percentage of larvae that survived, Light and Turbidity are explanatory variables and Survival is the response variable.\n\n\nTwo common ways to visualize the relationship between variables are scatter plots and box plots. In R, we create these plots using the xyplot() and bwplot() functions, respectively. Below are examples of these plots being used to explore the relationships between variables in two different datasets. Both of these functions are also discussed in Chapter 10.\n\n\n\n\n\n\nR functions\n\n\n\n### xyplot( y ~ x , data , xlab, ylab, main, xlim, ylim, col, pch )\n# y: Replace y with the name of the response numerical variable.\n# x: Replace x with the name of the explanatory numerical variable.\n# data: Set equal to dataframe name\n# xlab: set equal to the label for x-axis (optional).\n# ylab: set equal to the label for y-axis (optional).\n# xlim: set equal to the  range of the x-axis (e.g., c(0, 1)  provides\n#       a lower bound 0 and upper bound of 1) (optional).\n# ylim: set equal to the  range of the y-axis (optional).\n# main: set equal to the title of the plot (optional).\n# col:  Set equal to desired color for points (optional).  \n# pch:  Set equalt o a number between 1-25 to change the point symbol (optional).  \n#\n#\n### bwplot( y ~ x , data , xlab, ylab, main, xlim, ylim, box.ratio, \n             varwidth, horizonal, panel  )\n# x: Replace x with the name of the explanatory categorical variable.\n# horizontal: Set equal to TRUE if boxplots are to be drawn \n#            horizontally.  If set equal to true, change y~x\n#            to x ~ y (optional).\n# box.ratio: Set equal to a number between 0 and 1 to control the \n#            width of the rectangles. (optional)\n# fill: Set equal to color to fill the box. Be default the box\n#       is clear (optional).\n# panel: Set equal to panel.violin to create violin plot (optional).\n# pch: Set equal to \"|\" to denote median by a line rather than dot (optional).\n#\n\n\n\nCode# Load the `lattice` package to create plots\nlibrary(lattice) \n\n# The built-in `trees` dataset is used to\n# create a scatter plot of tree volume \n# versus tree height using the `xyplot` function.\nxyplot( Volume  ~  Height , \n        data= trees , \n        ylab= \"Tree Volume (cubic ft)\" , \n        xlab= \"Height (ft)\" )\n\n\n\nFigure 8.4: Tree volume vs. Tree height\n\n\n\n\n\nCode\n# Data set `ChickWeight' contains variables weight and Diet\n# It is a built-in data set.  bwplot() creates a boxplot \n# for a categorical variable (Diet) and a numerical \n# variable (weight).\nbwplot( weight ~ Diet , \n        data= ChickWeight , \n        ylab= \"Chick weight (gm)\" , \n        xlab= \"Diet type\" )\n\n\n\nFigure 8.5: Chick weight vs. Diet type\n\n\n\n\nThe graphical summaries above suggest that chicks on diets 3 and 4 tend to have higher weights than those on diets 1 or 2 in this sample. However, inferential methods would be necessary to determine whether these differences between diets are due to natural or sampling variability. The same goes for the trend seen in the scatter plot."
  },
  {
    "objectID": "drawingconclusions.html#basics-of-experimental-design",
    "href": "drawingconclusions.html#basics-of-experimental-design",
    "title": "8  Drawing statistical conclusions",
    "section": "\n8.3 Basics of experimental design",
    "text": "8.3 Basics of experimental design\nThe following concepts of experimental design enable researchers to conclude that differences in the observed response data are likely due by the treatments and not reasonably attributable to only chance:\n\nRandomization: Researchers randomly assign experiment units into treatment or conditions (also called randomized group assignment). This ensures that an experiment does not favor one experimental condition over any other and tries to create “equivalent” experimental groups in the sense that the treatment groups are as much alike as possible.\nReplication: The more experimental data collected, the more precise our estimate of the effect of the treatment on the response.\nControlling: Researchers assign treatments to cases, and attempt their best to control any other differences in the groups.\nBlocking: A technique to include another factor variable that is not of primary interest in the experiment because it is believed it contributes to variation in the response. The experimental units may be grouped or blocked based on this factor variable. Randomization is then applied within each block to the treatment conditions. Blocks should be homogeneous.\n\nFor those interested in learning more about the design and analysis of experiments, Math 4220 is an applied course that explores various strategies for constructing and executing experiments that can be applied across the social, physical, and life sciences. This course enhances the ability of one to design experiments, carry them out, and analyze the resulting data."
  },
  {
    "objectID": "drawingconclusions.html#scope-of-inference",
    "href": "drawingconclusions.html#scope-of-inference",
    "title": "8  Drawing statistical conclusions",
    "section": "\n8.4 Scope of inference",
    "text": "8.4 Scope of inference\nThe scope of inference of a study (from a statistician’s perspective) addresses two questions:\n\nDo these results provide evidence for a causal relationship? (cause-and-effect or causation)\nCan the results of the study be generalized to a population? (generalizability)\n\nOne can only establish if changes in the explanatory variable cause changes in the response variable through a randomized experiment. The answer to the second question is provided by the sampling method used to obtain the data. If the sample was randomly selected, then results from a study can be reasonably generalize to the population from which the sample was taken. In terms of a randomized experiment, the same applies to the treatments considered. Otherwise, the results can not be generalized to a population.\n\n\nFigure 8.6: The table reflects what can be concluded if an association is established (scope of inference)\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall the case study given in Section 1.1. The treatments (low, medium, and high light intensities) were imposed on the subjects (early-stage Delta smelt larvae), but the treatment levels were not randomly assigned to the experimental units. Therefore, results from the study can not say whether changes in light intensity causes changes in survival rates. While there is an association between these variables, other possible explanation may exist for this relationship.\nFurthermore, since the fish were not randomly selected for the study from some larger population, nor were the light intensities randomly selected, any conclusions from the study apply only to the subjects in the sample and the treatments considered. Therefore, the findings cannot be generalized to a larger population of fish or light intensity in general."
  },
  {
    "objectID": "summaries.html",
    "href": "summaries.html",
    "title": "Module: Basic Exploratory Data Analysis",
    "section": "",
    "text": "Module Chapter 7 covers data at the most basic level. Once data are obtained, the next step is to conduct basic exploratory data analysis, which consists of summarizing the data graphically and numerical. This allows one to explore and analyze a data set to discover patterns, trends, distributions, anomalies, etc. The intent is to explore and learn from the data, as opposed to assessing statistical hypotheses.\nThere does not exist an algorithm or method that automatically tells one how the data should be summarized. Therefore, it is important to have a basic understanding of the properties for different methods of summarization. With practice, one can develop the ability to summarize data effectively to brings insight regarding your research question(s).\nIt is worth noting that there three common plotting systems in R to graphically summarize data:\nThe Lattice system is preferred here module because it mostly follows the R formula expression, providing a suite of functions for summarizing data that is intuitive and easy to use. That is, using this system provides a suite of functions for summarizing data that follows the general form task( y ~ x , data )."
  },
  {
    "objectID": "summaries.html#module-outline",
    "href": "summaries.html#module-outline",
    "title": "Module: Basic Exploratory Data Analysis",
    "section": "Module outline",
    "text": "Module outline\n\nThe chapters in this module have three focuses regarding data:\n\nExploring categorical data\n\nExploring numerical data\n\nOutline of topics:\n\nGraphical summaries\nNumerical summaries"
  },
  {
    "objectID": "summaries.html#module-learning-objectives",
    "href": "summaries.html#module-learning-objectives",
    "title": "Module: Basic Exploratory Data Analysis",
    "section": "Module learning objectives",
    "text": "Module learning objectives\n\nCompute commonly used measures of center and spread.\nConstruct and interpret appropriate graphical summaries for numerical variables.\nConstruct frequency tables and contingency tables to describe the distribution of one categorical variable and two categorical variable, respectively.\nConstruct appropriate graphical summaries for categorical variables.\nConstruct side-by-side box plots for assessing relationships between variables."
  },
  {
    "objectID": "catsum.html",
    "href": "catsum.html",
    "title": "9  Summarizing categorical data",
    "section": "",
    "text": "AI art generated from the text “Summarizing categorical data”\nHere we cover the following graphically and numerical summaries:"
  },
  {
    "objectID": "catsum.html#sec-catevar",
    "href": "catsum.html#sec-catevar",
    "title": "9  Summarizing categorical data",
    "section": "\n9.1 Single categorical variable",
    "text": "9.1 Single categorical variable\nA bar graph or plot may be used to visualize the distribution of levels for a single categorical variable. The height of the bar for each category is equal to the frequency (number of observations), relative frequency of observations, or percentage of observations in the category. Generally, bar graphs leave some space in between each bars to highlight that there is no ordering in the classes. These graphs provides a nice visual of number observations in each level of a given categorical variable.\nThe function bargraph() creates a bar graph.\n\n\n\n\n\n\nR functions\n\n\n\n### bargraph( ~ x , data,  ylab, xlab, type)\n# x: Replace x with the name of the variable of interest\n# data: Set equal to the name of the dataframe being used\n# xlab: Set equal to the label for x-axis (optional)\n# xlab: Set equal to the label for y-axis (optional)\n# main: Set equal to the title of the plot (optional)\n# type: Set equal to \"frequency\" (default) or \"proportion\"\n\n\nNote that the type argument specifies whether to display frequencies or proportions. Many of the R functions used in this resource will have identical arguments, therefore while the arguments will be provided for any functions used, arguments such as x, data, xlab, … will generally no longer be described.\n\n\n\n\n\n\nNote\n\n\n\nThe case study data given in Chapter 1 consists of three variables labeled as Light, Turbidity, and Survival in the data frame dssurve. The case study data given in Chapter 3 consists of several variables, including the categorical variable county. Here, the categorical variables are summarized graphically.\n\n\nR code\nVideo\n\n\n\n\nCode# Import data from case study on delta smelt\ndssurv <- read.csv( \"datasets/dssurv.csv\" )\n\n# Load the dplyr package. This package provides mutate()\nlibrary(dplyr)\n\n# Note:  mutate( 'dataframe name' ,'new variable' = 'function of variable in data frame', ...)\n# Convert variables Light and Turbidity to factors\ndssurv <- mutate( dssurv, \n                  Light= as.factor( Light ) ,\n                  Turbidity= as.factor( Turbidity ) )\n\n\n# Import data from case study on air pollution\npmdf <- read.csv(\"datasets/dailyPM10.2021.csv\")\n\n# Convert variable county to a factor\npmdf <- mutate( pmdf, \n                county= as.factor( county ) )\n\n\nThe following code creates a bar graph of the variables Light and county:\n\nCode# Load the 'mosaic' package which provides the 'bargraph' function.\nlibrary(mosaic)\n\nbargraph( ~ Light ,     # Create a bar graph for the 'Light' variable.\n          data=dssurv , # Use the 'dssurv' data frame.\n          type=\"proportion\" ,           # Display proportions of each category.\n          ylab=\"Relative frequency\" ,   # Set the y-axis label.\n          xlab=\"Light intensity\" ,      # Set the x-axis label.\n          main=\"Using labels on axis\" ) # Set the title of the plot.\n\nbargraph( ~ county ,   # Create a bar graph for the 'county' variable.\n          data=pmdf,   #Use the 'pmdf' data frame.\n          type=\"frequency\" ,            # Display the frequency of each category.\n          ylab=\"Frequency\" ,            # Set the y-axis label.\n          xlab=\"County\",                # Set the x-axis label.\n          main=\"Frequency bar graph\" )  # Set the title of the plot.\n\n\n\n\n\nFigure 9.1: Frequency bar graph\n\n\n\n\n\n\nFigure 9.2: Relative frequency bar graph\n\n\n\n\n\n\nNote that medium light intensity was less common than other light intensities. For the PM10 data, Kern county has more monitoring stations compared to Fresno and Tulare.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar graphs provide a clear visual of the distribution of levels of the categorical variables. To numerically summarize a categorical variables, one can use the tally() function to create the basis for a frequency or relative frequency table:\n\n\n\n\n\n\nR functions\n\n\n\n### tally( ~ x , data,  format, useNA)\n# format: set equal to \"count\" (default), \"proportion\", or \"percent\"\n# useNA: should the table count missing values?  Set equal to \"no\" \n#        (never consider missing values), \"ifany\" (only if the \n#        count is positive) , or \"always\" (even for zero counts).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe following code helps create a frequency or relative frequency table for the variables Light and county:\n\n\nR code\nVideo\n\n\n\n\nCode# Use the tally() function from the mosaic package to \n# compute a relative frequency table for the \"Light\" variable in \n# the dssurv dataset.\ntally( ~ Light , \n       data=dssurv , \n       format=\"proportion\" ) \n#> Light\n#> high  low  med \n#>  0.4  0.4  0.2\n\n# Use the tally() function to compute a frequency table for \n# the \"county\" variable.\ntally( ~ county , \n       data=pmdf ,\n       format=\"count\" ) \n#> county\n#> Fresno   Kern Tulare \n#>   1046   1727    342\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tally() output provides a clear numerical summary of the data by providing the count/proportion/percentage of observations in each each level for the variable of interest. The output from tally() can then be used to form a relative frequency table. Such tables make it easier to see the frequency counts for a categorical variables than by looking at the actual data."
  },
  {
    "objectID": "catsum.html#two-or-more-categorical-variables",
    "href": "catsum.html#two-or-more-categorical-variables",
    "title": "9  Summarizing categorical data",
    "section": "\n9.2 Two or more categorical variables",
    "text": "9.2 Two or more categorical variables\nA bar graph can also graphically summarize two categorical variables by providing a bar plot for a given variable at each level of another categorical variable. Such graphs are generally referred as comparative plots or conditional plots. Here, we create three comparative bar graphs using bargraph(). For each, we have to specify a grouping variable:\n\n\n\n\n\n\nR functions\n\n\n\n### Creates a bar graph at each level of another categorical variable:\n### bargraph( ~ x  | gfactor , data,  ylab, xlab, type)\n# gfactor: Replace with a grouping (categorical) variable.\n#         This variable should have two or more levels, and \n#         the resulting graph will show the distribution of \"x\" \n#         at each level of the grouping variable.\n#\n###\n###\n### Creates grouped/clustered bar graph:\n### bargraph( ~ x , groups, data,  ylab, xlab, type, auto.key)\n# groups: Set equal to the grouping (categorical) variable.  \n#         This variable should have two or more levels, and \n#         the resulting graph will show the distribution of \"x\" \n#         at each level of the grouping variable.\n# auto.key: A list of the form list( x, y , corner = c(0, 0)), where\n#           x is set equal to the location on the x-axis (from 0 to 1). Similarly for y.\n#           corner is set equal to one of c(0,0) (bottom left corner of legend), c(1,0), \n#           c(1,1) and c(0,1).\n#\n###\n###\n### Creates a stacked bar graph:\n### bargraph( ~ x , groups , data,  ylab, xlab, type, stack)\n# stack: Set equal to TRUE for a stacked barchart. Default is FALSE\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe case study in Chapter 5 contains many variables dealing with characteristics related to school shootings. The following code creates a bar graphs of Media_Attention for each level of AgeGroup:\n\n\nR code\nVideo\n\n\n\n\nCode# Import data \n# Note: na.strings=c(\"\",\"NA\") tells R to treat any blank \n# and NA entries in the csv file as NA values. \nssd <- read.csv(\"datasets/ssdsample.csv\" , na.strings=c(\"\",\"NA\")) \n\n# Next, we use the mutate() function to \n# convert AgeGroup, Media_Attention, and During_School \n# variables in the ssd dataframe to factors:\nssd <- mutate( ssd, AgeGroup= as.factor( AgeGroup ),\n                      Media_Attention = as.factor( Media_Attention ),\n                      During_School= as.factor( During_School ) )\n\nbargraph( ~ Media_Attention | AgeGroup ,  \n          data=ssd , \n          type=\"proportion\" ,\n          ylab=\"Proportion\" , \n          xlab=\"Type of media attention\" ,\n          main=\"Bar graph for each age group\" )\n\nbargraph( ~ Media_Attention , \n          group= AgeGroup , \n          data= ssd , \n          type= \"proportion\" ,\n          ylab= \"Proportion\" , \n          xlab= \"Type of media attention\" , \n          auto.key = list(x = .6, y = .76, corner = c(0, 0)) , \n          main=\"Grouped bar graph\" )\n\n\nbargraph( ~ Media_Attention , \n          group= AgeGroup ,  \n          data=ssd , \n          type=\"proportion\" ,\n          ylab=\"Proportion\" , \n          xlab=\"Type of media attention\" , \n          stack = TRUE , \n          auto.key = list(x = .6, y = .76, corner = c(0, 0)) ,\n          main=\"Stacked bar graph\")\n\n\n\n\n\n(a) Barplot for each age group\n\n\n\n\n\n\n(b) Barplot grouped by age group\n\n\n\n\n\n\n(c) Barplot stacked by age group\n\n\n\n\nFigure 9.3: Examples of bargraphs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll three graphs convey the proportion of observations in each category of Median_Attention for a given age group. For example, note that the highest media attention being the national level appears to be about the same for all age groups, but it is not the case for local and regional levels.\n\n\ntally() may also numerically summarize two or three categorical variables to provide the basis for a two-way or three-way contingency table. If one variable is designated a response variable and the other an explanatory variable, then the response variable is specified for y and the explanatory for x:\n\n\n\n\n\n\nR functions\n\n\n\n### tally(y ~ x , data,  format=\"count\", useNA)\n# y: replace y with the name of the response categorical variable.\n# x: replace x with the name of the explanatory categorical variable.\n# NOTE: format must set equal to count\n#\n###\n###\n### tally(y ~ x + z , data,  format=\"count\")\n# x: replace z with the name of the additional explanatory categorical variable.\n#\n\n\nFor two-way or three-way tables, if count is set to proportion or percent in tally(), it will provide marginal/conditional proportions/percentages of observations in each level of one variable, conditional on the levels of another variable. Comparing the unconditional proportions to their conditional counterparts allows one to assess if the data suggest an association between two categorical variables. An unconditional proportion is a proportion measured out of the total sample size, while a conditional proportion is a proportion measured under a specific level of another variable.\n\n\n\n\n\n\nNote\n\n\n\nFor two-way table, if count is equal proportion or percent, it will provide the marginal/conditional proportions/percentage of observations in each each level of Median_Attenion for each level of AgeGroup:\n\n\nR code\nVideo\n\n\n\n\nCode# Conditional proportions\ntally( Media_Attention ~ AgeGroup ,  \n      data= ssd , \n      format= \"proportion\" , \n      useNA= \"no\" ) \n#>                AgeGroup\n#> Media_Attention      Adult      Child       Teen\n#>   International 0.03859649 0.00000000 0.04511278\n#>   Local         0.64912281 0.62500000 0.52631579\n#>   National      0.13333333 0.12500000 0.21553885\n#>   Regional      0.17894737 0.25000000 0.21303258\n\n# Unconditional proportions\ntally( ~ Media_Attention ,  \n       data= ssd , \n       format= \"proportion\" , \n       useNA= \"no\" ) \n#> Media_Attention\n#> International         Local      National      Regional \n#>    0.03562945    0.60807601    0.16627078    0.19002375\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the chances of Media_Attention being “National” varies across age groups. Such behavior is consistent with these variables being potentially associated."
  },
  {
    "objectID": "numsum.html",
    "href": "numsum.html",
    "title": "10  Summarizing numerical data",
    "section": "",
    "text": "AI art generated from the text “Summarizing numerical data”\nHere we cover the following graphically and numerical summaries:"
  },
  {
    "objectID": "numsum.html#sec-onenumvar",
    "href": "numsum.html#sec-onenumvar",
    "title": "10  Summarizing numerical data",
    "section": "\n10.1 Single numerical variable",
    "text": "10.1 Single numerical variable\nOften it helps to summarize the distribution by quantifying its center and spread. The mean or average is the most common way to measure the center of a distribution. A common way to measure spread or variability of the distribution is the standard deviation.\nHere, we denote the mean of sample or sample mean by \\(\\bar{x}\\). The sample standard deviation is denoted by \\(s\\). The function mean() and sd() computes the mean and standard deviation of a sample of data, respectively:\n\n\n\n\n\n\nR functions\n\n\n\n### mean( ~ x , data, na.rm)\n# x: Replace x with the name of the variable of interest\n# data: Set equal to dataframe name\n# na.rm: Should missing values be remove? The \n#       default is FALSE. Set equal to TRUE if\n#       missing values are to be excluded.\n#\n### sd( ~ x , data, na.rm)\n#\n\n\nMany of the R functions used in these modules will have identical arguments. Therefore, while the arguments will be provided for any functions used, arguments such as x, data, … will generally no longer be described.\nOther measures of the center and spread of the distribution used in practice are the median and interquartile range (IQR) range, respectively. The median, denoted \\(M\\), is the (middle) value separating the upper half from the lower half of the ordered sample of data. The IQR measures the spread of the middle half of the data. Measures of center are generally used to describe what a “typical” value in the data, where as measures of spread quantify the amount of variability in the sample. The function median() and iqr() computes the median and IQR of a sample of data:\n\n\n\n\n\n\nR functions\n\n\n\n### median( ~ x , data, na.rm)\n#\n### IQR( ~ x , data, na.rm)\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe case study data given in Section 1.1 consist of a single numerical variable: Survival. Here we compute the measures of center and spread discussed above for Survival:\n\n\nR code\nVideo\n\n\n\n\nCode# import data\ndssurv <- read.csv( \"datasets/dssurv.csv\" )\n\nlibrary(mosaic) # provides formula interface for the functions below.\n\n# Note: this dataset does not have any missing values\nmean( ~ Survival , \n      data= dssurv )\n#> [1] 62.22\n\nmedian( ~ Survival , \n        data= dssurv )\n#> [1] 59.56\n\nsd( ~ Survival , \n    data= dssurv )\n#> [1] 12.07902\n\nIQR( ~ Survival , \n     data= dssurv )\n#> [1] 11.45\n\n\nWhich summary statistics better describes the center and spread of the distribution depends on the shape of the distribution provided by graphical summaries (to be discussed later).\n\n\n\n\n\n Mean \n    Median \n    Std. deviation \n    IQR \n  \n\n 62.22 \n    59.56 \n    12.079 \n    11.45 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistograms provide a visualization of the distribution of a single numerical variable, where observations are placed into bins of equal length along the horizontal axis so that a bin represents an interval of numbers. Like bar graphs, the histogram consists of bars with widths determined by the bins. The higher the bar, the higher the relative proportion of observations that fall in the respective bin\nGenerally, attention should be paid to certain features of the distribution, such as\n\nOverall pattern of graph\n\nsymmetric?\nskewed right (a few unusually large measurements, the longer tail is on the right side)?\nskewed left (a few unusually small measurements, the longer tail is on the left side)?\n\n\nOutlying, extreme observations, or values that are far away from the bulk of the data?\n\nThe function histogram() creates a histogram:\n\n\n\n\n\n\nR functions\n\n\n\n### histogram( ~ x , data , xlab , ylab , main , type , nint , xlim , ylim, col )\n# xlab: Set equal to the label for x-axis (optional).\n# ylab: Set equal to the label for y-axis (optional).\n# main: Set equal to the title of the plot (optional).\n# type: Set equal to \"percent\",  \"count\", or \"density\" (default).\n# nint: Set equal to the number of desired bins (optional).\n# xlim: Set equal to the  range of the x-axis (e.g., c(0, 1)  provides\n#       a lower bound 0 and upper bound of 1) (optional). \n# ylim: Set equal to the  range of the y-axis (optional). \n# col: Set equal to desired color. Default color is cyan (optional).  \n#\n\n\nA frequency histogram is obtained by type=\"count\". A relative frequency histogram is a histogram where relative frequencies are plotted on the y-axis. The relative frequencies are provided as percentages with type=\"percent\". A density histogram (type=\"density\") is just a modified relative frequency histogram in that the area of each rectangle equals the relative frequency of the corresponding class, and the area of the entire histogram equals 1.\n\n\n\n\n\n\nNote\n\n\n\nThe case study given in Chapter 4 provides the BMI values for various regions and countries. Here we create a histogram of the variable BMI:\n\n\nR code\nVideo\n\n\n\n\nCode\n# Import data\nbmidf <- read.csv(\"datasets/BMIcsdata.csv\")\n\nlibrary(lattice) # provides histogram()\n\nhistogram( ~ BMI , \n           data= bmidf ,  \n           type= \"count\" , \n           xlab= \"Body Mass Index\" , \n           main= \"Frequency histogram\" )\n\nhistogram( ~ BMI , \n           data= bmidf , \n           type= \"density\" , \n           xlab= \"Body Mass Index\" , \n           main= \"Density histogram\" , \n           col= \"green\" )\n\nhistogram( ~ BMI , \n           data= bmidf , \n           type= \"density\" , \n           xlab= \"Body Mass Index\" , \n           main= \"Using specified number of bins\" , \n           nint= 20 , \n           col= \"pink\")\n\n\n\n\n\nFigure 10.1: Frequency histogram\n\n\n\n\n\n\nFigure 10.2: Density histogram\n\n\n\n\n\n\nFigure 10.3: Density histogram with additional options\n\n\n\n\n\n\nThe distribution of BMI looks roughly symmetric.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA boxplot is another common graphical summary that provides a visual of the five-number summary (the maximum and minimum values, the lower and upper quartiles, and the median). The function bwplot() creates a boxplot.\n\n\n\n\n\n\nR functions\n\n\n\n### bwplot( ~ x , data , xlab, ylab, main, xlim, ylim, box.ratio, fill, panel,  pch)\n# box.ratio: Set equal to a number between 0 and 1 to control the \n#            width of the rectangles. (optional)\n# fill: Set equal to color to fill the box. Be default the box\n#       is clear.\n# panel: Set equal to panel.violin to create violin plot.\n(optional)\n# pch: Set equal to \"|\" to denote median by a line rather than dot. (optional)\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code below creates a boxplot of the variable BMI:\n\n\nR code\nVideo\n\n\n\n\nCode# bwplot() is from the lattice package.\nlibrary( lattice )\n\nbwplot( ~ BMI , \n        data= bmidf ,  \n        xlab= \"Body Mass Index\" , \n        main= \"Boxplot of BMI\" )\n\nbwplot( ~ BMI , \n        data= bmidf ,  \n        xlab= \"Body Mass Index\" , \n        main= \"Boxplot of BMI with additional options\" , \n        box.ratio= .5 , \n       fill= \"orange\" , \n       pch= \"|\")\n\n\n\n\n\nFigure 10.4: Boxplot\n\n\n\n\n\n\nFigure 10.5: Boxplot of BMI with additional options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe dot inside the box represents the median. The two boundaries of the box are called the first quartile (the 25th percentile) and the third quartile (the 75th percentile). Extending out from the box, the whiskers reflect the range of data outside of the box and reach the minimum and the maximum values, unless unless there are potential outliers (points beyond the whiskers). When the median is closer the first (third) quartile than the median, the distribution is potentially skewed left (right).\nAn extension of the boxplot is a violin boxplot or plot. This plot is a hybrid of a box plot and a kernel density plot (think of a kernel density plot as a smooth density histogram). The violin boxplot conveys the same information as the boxplot, but provides better picture of distribution. The code below creates a violin boxplot.\n\n\n\n\n\n\nNote\n\n\n\nThe code below creates a violin plot of the variable BMI:\n\n\nR code\nVideo\n\n\n\n\nCode\n# bwplot() is from the lattice package.\nbwplot( ~ BMI , \n        data= bmidf ,  \n        xlab= \"Body Mass Index\" , \n        main= \"Violin plot of BMI\" , \n        panel = panel.violin )\n\n\n\n\n\n\n\nThe middle part of the plot is the widest because it has the most data and has the highest data density. The lower and upper ends of the violin plot are thinner because there are fewer data points in these regions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile-quantile plots, also known as QQ-plots, allow one to view the distributional similarity between two sets of sample data. With a QQ-plot, the observed values of a sample are plotted against the expected values of a theoretical distribution, usually a normal distribution. If the sample data are approximately normally distributed, the QQ-plot will show points on a scatterplot that approximately or roughly follow a straight or linear line. We limit our discussion to normal QQ-plots, which plot the sample quantiles of a sample against the quantiles of a normal distribution. The function normqqplot creates the normal QQ-plot.\n\n\n\n\n\n\nR functions\n\n\n\n### normqqplot( ~ x , data , ylab, main)\n# x: Relace x with the name of the variable of interest\n# data: Set equal to dataframe name only if x is stored in the dataframe.\n#       Otherwise, do not include this argument if the variable is \n#       in R's memory (outside a dataframe)\n# Note: xlab will always have the label 'Normal quantiles`. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code below creates a normal QQ-plot of the variable BMI:\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's R's memory.\nsource(\"rfuns/normqqplot.R\")\n\n# Use the normqqplot() function to create a QQ plot \n# of the BMI variable in the bmidf dataset.\nnormqqplot( ~ BMI , \n             data= bmidf ,  \n             ylab= \"sample quantiles of BMI\"  )\n\n\n\n\n\nFigure 10.6: Boxplot\n\n\n\n\n\n\nWhile the points of the QQ-plot follow a linear trend in the middle, they taper off to form a slightly “u” shaped pattern. This is a sign that the distribution of the data is skewed right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe behavior of the QQ-plot gives a sense of the normality of data and can also provide some guidance on the distribution of non-normal data, depending on the trend exhibited by the points and how they deviate from the linear line. If the tails of a QQ-plot wander off in opposite directions but follow a linear line in the middle, it is a sign that the distribution of the data resembles a heavy-tailed distribution. On the other hand, if the points follow a “u” or “n” shape, it indicates that the data distribution is skewed, with clearer “u” or “n” shapes indicating stronger skewness."
  },
  {
    "objectID": "numsum.html#sec-multinumvar",
    "href": "numsum.html#sec-multinumvar",
    "title": "10  Summarizing numerical data",
    "section": "\n10.2 Two or more numerical variable",
    "text": "10.2 Two or more numerical variable\nA scatterplot graphically summarizes the relationship between two numerical variables. When describing a scatterplot, look for the following features:\n\nWhat is the direction of the pattern? Positive or negative association\nAre there any unusual observations? Clusters or outliers.\nLinear relationship, non-linear relationship, or no relationship (no linear or non-linear trend)?\n\nxyplot() creates a scatterplot.\n\n\n\n\n\n\nR functions\n\n\n\n### xyplot( y ~ x , data , xlab, ylab, main, xlim, ylim, col, pch )\n# y: replace y with the name of the response numerical variable.\n# x: replace x with the name of the explanatory numerical variable.\n# col:  Set equal to desired color for points. (optional) \n# pch:  Set equalt o a number between 1-25 to change the point symbol. (optional) \n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe case study given in Chapter 3 consists of several variables (PM10, AQI, temp …, county, and City.Name). Here, we graphically summarize the relationship between daily PM10 levels and daily temperature:\n\n\nR code\nVideo\n\n\n\n\nCode# Import data\npmdf <- read.csv(\"datasets/dailyPM10.2021.csv\")\n\n# create a scatterplot of PM10 vs temperature\nxyplot(PM10 ~ temp,                # response variable (y-axis) is PM10, explanatory variable (x-axis) is temperature\n       data = pmdf,                # data is set to pmdf \n       ylab = \"Daily PM10\",        # label for the y-axis\n       xlab = \"Daily temperature\", # label for the x-axis\n       main = \"PM10 vs temperature\" ) # main title of the plot\n\n\n\n\n\n\n\nThe relationship is clearer on the square root scale:\n\nCode\n### mutate( dataframe ,'new variable' = 'function of variable in data frame', ...)\npmdf <- mutate( pmdf, PM10sqrt= sqrt( PM10 ) ,\n                      tempsqrt= sqrt( temp ) )\n\n# pmdf now has two additional variables: \"square root versions' of \n# PM10 and temp\nxyplot( PM10sqrt ~ tempsqrt , \n        data= pmdf , \n        ylab= \"Daily PM10 (square root scale)\" , \n        xlab= \"Daily temperature (square root scale)\" , \n        main= \"PM10 vs temperature (on transformed scale)\" , \n        col=\"black\" )\n\n\n\n\n\n\n\nThe scatterplots suggest that there is a positive association between daily PM10 and temperature levels, as well as outlying observations but less so on the transformed scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot conveys the direction and type of relationship. If the data shows a linear trend, then correlation may be used to numerically summarize the strength and direction of the relationship. Correlation is a sample statistic generally referred to as the correlation coefficient or sample correlation, denoted by \\(r\\). \\(r\\) will always be between -1 and 1, with \\(|r|\\) closer to one reflecting a stronger linear association, with positive (negative) values quantifying positive (negative) associations. The following guidelines regarding the size and the strength of the relationship will aid interpretation:\n\n\n\\(|r|\\geq 0.9\\): Very strong linear association\n\n\\(0.7 \\leq |r| < 0.9\\): Strong linear association\n\n\\(0.4 \\leq |r| < 0.7\\): Moderate linear association\n\n\\(0.2 \\leq |r| < 0.4\\): Weak linear association\n\n\\(|r| <0.2\\): Very weak linear association\n\nThe cor() computes the correlation coefficient:\n\n\n\n\n\n\nR functions\n\n\n\n### cor( y ~ x , data )\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use the sample correlation to quantify the strength and direction of the relationship between PM10 and temp:\n\nCode\nlibrary(mosaic) # provides formula interface for the functions below.\n\n\n# The variable `temp` has missing values but currently \n# the standard argument to remove missing values does \n# not work.  Instead, we us na.omit() on the dataframe\n# so that all missing values are omitted from the data\n# for the purpose of this function. \ncor( PM10 ~ temp , \n     data= na.omit( pmdf ) )\n#> [1] 0.3680937\n\ncor( PM10sqrt ~ tempsqrt , \n     data= na.omit( pmdf ) )\n#> [1] 0.4875372\n\n\nThe sample correlations shows that there this weak and moderate linear association between the variables, with the variables on the transformed scaled having a moderate association.\n\n\nIf the explanatory variable is a factor/categorical variable, then boxplots, violin plots, and strip plots can summarize the relationship between the response and the explanatory variable. In the strip plot, values of the response as shown as dots along the factor level, and generally the dots with the same value can overlap.\n\n\n\n\n\n\nR functions\n\n\n\n### bwplot( y ~ x , data , xlab, ylab, main, xlim, ylim, box.ratio, \n             varwidth, horizonal, panel  )\n# y: Replace y with the name of the response categorical variable.\n# x: Replace x with the name of the explanatory categorical variable.\n# horizontal: Set equal to TRUE if boxplots are to be drawn \n#            horizontally.  If set equal to true, change y~x\n#            to x ~ y.\n#\n### stripplot( y ~ x , data , xlab, ylab, main, xlim, ylim, jitter, horizonal)\n# jitter: Set equal to TRUE to add jitter (a small bit of meaningless noise) to obervations of \n#         the response. Default is FALSE.\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe data for the case study given in Chapter 3 consist of several variables (PM10, AQI, …, county, and City.Name). Here, we graphically explore the relationship of AQI and county.\n\nCode\n# import data\npmdf <- read.csv(\"datasets/dailyPM10.2021.csv\")\n\n### We tell R that  'county'  is a factor variable by using\n### as.factor() within mutate()\n### mutate( dataframe ,'new variable' = 'function of variable in data frame', ...)\npmdf <- mutate( pmdf , \n                county = as.factor( county )  )\n\n# 'county' has three levels. To have different color\n# boxes for each level use set col to a vector with\n# three elements reflecting the color for each box, but\n# this option is optional. \n\n# Create a box plot of AQI vs. county\nbwplot( AQI ~ county , \n        data= pmdf ,  \n        xlab= \"County\" , \n        ylab= \"AQI\" ,\n        main= \"AQI vs county\" , \n        fill= c( \"gray\", \"brown\", \"cyan\" ) , \n        varwidth= TRUE )\n\n# Create a horizontal box plot of AQI vs. county\nbwplot( county ~ AQI , \n        data= pmdf ,  \n        xlab= \"County\" , \n        ylab= \"AQI\" ,\n        main= \"AQI vs county\" , \n        horizontal = TRUE )\n\n# Create a violin plot of AQI vs. county\nbwplot( AQI ~ county , \n        data= pmdf ,  \n        xlab= \"County\" , \n        ylab= \"AQI\" ,\n        main= \"AQI vs county\" , \n        panel= panel.violin )\n\n# Create a strip plot of AQI vs. county\nstripplot( AQI ~ county , \n           data= pmdf ,  \n           xlab= \"County\" , \n           ylab= \"AQI\" ,\n           main= \"Strip plot\", \n           jitter= TRUE  )\n\n\n\n\n\nFigure 10.7: Vertical boxplots\n\n\n\n\n\n\nFigure 10.8: Horizontal boxplots\n\n\n\n\n\n\n\n\nFigure 10.9: Violin plots\n\n\n\n\n\n\nFigure 10.10: Strip plots\n\n\n\n\n\n\nAll three counties have outlying AQI values. The distribution of AQI for all three counties are roughly symmetric if the outliers are ignored.\n\n\nThe plots above are comparative plots or conditional plots since they graphically summarize a numerical variables at each level of a factor variable. The factor variable in such summaries is typically called a grouping factor variable.\nConditional plots may also be created by having the graphical summary appear in a single panel for each level of the factor variable, which can be achieved by using the | gfactor element within the formula expression, where gfactor represents the grouping factor variable.\n\n\n\n\n\n\nR functions\n\n\n\n### bwplot( ~ y | gfactor , data , xlab, ylab, main, xlim, ylim, box.ratio, \n             varwidth, horizonal, panel  )\n# y: the variable of interest.\n# gfactor: The grouping variable. \n#\n### stripplot( ~ y| gfactor , data , xlab, ylab, main, xlim, ylim, \n#                                    jitter, horizontal)\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere, the relationship of AQI and county is explored by graphically summarizing AQI the at the factor levels of county.\n\nCode# For this type of conditional vertical violin plot, use the following formula\n# expression: y ~ 1 | gfactor  \nbwplot( AQI~ 1 | county , \n        data= pmdf ,  \n        xlab= \"County\", \n        ylab= \"AQI\" ,\n        main= \"AQI vs county\" , \n        panel= panel.violin, \n        horizontal= FALSE )\n\n# formula expression: ~ x | gfactor\nbwplot(~ AQI | county , \n       data= pmdf ,  \n       xlab= \"County\", \n       ylab= \"AQI\" ,\n       main= \"AQI vs county\" , \n       fill= c( \"gray\", \"brown\", \"cyan\" ) , \n       varwidth= TRUE )\n\n\n\n\n\nFigure 10.11: Vertical boxplots\n\n\n\n\n\n\nFigure 10.12: Horizontal boxplots\n\n\n\n\n\n\n\n\nMost functions to graphically or numerically summarize data conditional on the levels of another factor/grouping variable (gfactor) use the general layout of task( y ~ x | gfactor, data ) or task( ~ y | gfactor, data )."
  },
  {
    "objectID": "twosample.html",
    "href": "twosample.html",
    "title": "Module: Two sample methods",
    "section": "",
    "text": "This module discusses two-sample inference but first discusses the basic frame work for inference through the introduction to things that one should consider when comparing aspects of:\n\nTwo distinct populations (e.g, delivery times of Uber Eat vs. Doordash)\nTwo different treatments applied to one population (e.g., the effect of taking a drug vs. a placebo).\n\nThis involves the process of sampling data from a population or obtaining data via a randomize experiment to determine what can be inferred about the true effect or population based on sample results. Statistical inference helps us answer two questions about the population or experiment:\n\nHow strong is the evidence of an effect?\nHow large is the effect?\n\nThe first question is addressed using hypothesis testing, while the second question is address by a confidence interval.\nWhile this framework is illustrated through two-sample method, this framework extends to any inferential procedures discussed in this resource.\n\nModule chapters:\n\nHypothesis testing and confidence interval framework\nInference for comparing two independent means"
  },
  {
    "objectID": "siframework.html",
    "href": "siframework.html",
    "title": "11  Statistical inference framework via a case study",
    "section": "",
    "text": "Statistical inference is the practice of making decisions from data in the presence of uncertainty. The common procedures for making statistical inferences include"
  },
  {
    "objectID": "siframework.html#hypothesis-testing-framework",
    "href": "siframework.html#hypothesis-testing-framework",
    "title": "11  Statistical inference framework via a case study",
    "section": "\n11.1 Hypothesis testing framework",
    "text": "11.1 Hypothesis testing framework\nThis case study is adapted from Ramsey and Schafer (2013) and is discussed in Chapter 2: Reward systems are integrated into schools and in the workplace, but are rewards operating in precisely the opposite way from what is intended? Do external incentives affect creativity? To address this, we will use a study that investigated whether people tend to display different creativity levels when they are thinking about intrinsic or extrinsic motivations. Undergraduate students of similar creative writing experience from a private university in New England were randomly assigned to one of two groups. One group received an intrinsic and the other an extrinsic questionnaire to classify their motivation. All subjects were instructed to write a haiku, and those poems were evaluated for creativity by a panel of judges.\n\n\n\n\n\n\n\n\nBased on the collected data, do subjects thinking about intrinsic motivations display different creativity than subjects who were thinking about extrinsic motivations?\n\n\n\nThe sample mean (based on a sample of 24) of the intrinsic group mean is 19.88 and standard deviation 4.44. For the extrinsic group the sample mean (based on a sample of 23) is 15.74 and standard deviation is 5.25. The difference between the sample means is 4.14. Is the difference large enough to convince you that subjects thinking about intrinsic motivations display different levels of creativity than subjects who were thinking about extrinsic motivations? Or is this differences of 4.14 small enough that one may wonder if the size of difference is more due to chance (random/sampling/natural variability) and not due to a motivation type effect.\nIf the study was reproduced, the results would differ. But if results are different each time, how does one determine how large (in magnitude) the differences in sample means should be in order to say that subjects thinking about intrinsic motivations display different levels creativity than subjects who were thinking about extrinsic motivations?\nIn terms of the null and alternative hypothesis we have:\n\n\\(H_o:\\) There is no difference in creativity under extrinsic and intrinsic motivations.\n\\(H_a:\\) Subjects thinking about intrinsic motivations display different levels of creativity than subjects who were thinking about extrinsic motivations.\n\nAt this stage, consider how the effect intrinsic motivation on creativity can be quantified. One approach is to quantify the effect by using the difference between the true mean creativity score under intrinsic and extrinsic motivation. Let the true mean under intrinsic motivation be denoted as \\(\\mu_{int}\\). Similarly, we denote the true mean under extrinsic motivation as \\(\\mu_{ext}\\). If motivation type has no effect on creativity, then we should expect \\(\\mu_{int}-\\mu_{ext}=\\delta=0\\), where \\(\\delta\\) represents the effect of intrinsic motivation on creativity. If subjects thinking about intrinsic motivations display more or less creativity than subjects who were thinking about extrinsic motivations, then this would correspond to \\(\\mu_{int}-\\mu_{ext}\\neq 0\\) or \\(\\delta \\neq 0\\). Thus, \\(H_0\\) and \\(H_a\\) may be expressed as\n\\[H_0: \\mu_{int}-\\mu_{ext}=0 \\qquad H_a:\\mu_{int}-\\mu_{ext} \\neq 0\\]\nor\n\\[H_0: \\delta=0 \\qquad H_a:\\delta \\neq 0\\]\nAny hypothesis test is always conducted under the assumption that the null hypothesis is true. The observed data will either provide sufficient evidence to reject this assumption or fail to reject this assumption. A test statistic is simply a sample statistic (numerical summary of the data). An extreme value of the test statistic will be consistent with the idea reflected in \\(H_a\\).\nWhat is considered extreme depends on the sign of \\(H_a\\). If the sign is \\(>\\), then extreme means really large. If the sign is \\(<\\), then extreme means really small. A \\(\\neq\\) sign means extreme is really large or really small. To evaluate the evidence against the \\(H_0\\), we compare the observed value of the test statistic to what one would expect to obtain (for the value of test statistics) if \\(H_0\\) were true (i.e., under the null hypothesis). This will allow one to determine if the observed value of the test statistic is extreme or not if in fact \\(H_0\\) were true.\nA test statistic could be any numerical summary statistic, but the form of the test statistic is generally chosen to have the following qualities:\n\nAn extreme value of the test statistic is consistent with the idea reflected in \\(H_a\\)\nThe distribution of test statistic under \\(H_0\\) can be obtained or is known. That is, to determine if the value of the observed test statistic is extreme, we need to know what range of values the test statistic can take under the null hypothesis. This distribution is generally referred to as a Null distribution.\n\nA common test statistic for testing the above hypothesis is of the form:\n\\[T=\\frac{(\\bar{y}_{int}-\\bar{y}_{ext}) -(\\mu_{int}-\\mu_{ext})}{\\sqrt{\\frac{s_{int}^2}{n_{int}}+\\frac{s_{ext}^2}{n_{ext}}}}\\]\nwhere \\(\\bar{y}\\), \\(s\\), and \\(n\\) represent the sample mean, sample standard deviation, and sample size for a given sample. Under \\(H_0\\), \\(\\mu_{int}-\\mu_{ext}=0\\), so the observed value of the test statistic is\n\\[T=\\frac{(\\bar{y}_{int}-\\bar{y}_{ext}) }{\\sqrt{\\frac{s_{int}^2}{n_{int}}+\\frac{s_{ext}^2}{n_{ext}}}}=\\frac{19.88-15.74}{\\sqrt{\\frac{4.44^2}{24}+\\frac{5.25^2}{23}}}= 2.926\\]\nWhile a value of \\(T=2.926\\) (or larger) is consistent with the idea that \\(\\mu_{int}\\neq\\mu_{ext}\\), is \\(T=2.926\\) large enough to convince you that in fact \\(\\mu_{int}\\neq\\mu_{ext}\\), or is its value (\\(T=2.926\\)) simply due to chance (sampling/natural variability)? To decide between supporting or not supporting what is reflected in \\(H_a\\), we compute the chances or probability of observing \\(T=2.926\\) or larger or \\(T=-2.926\\) or smaller (since the sign in \\(H_a\\) is \\(\\neq\\)), under the assumption that motivation type has no effect on creativity (\\(H_0\\) true). If this probability is very low, we would have reason to reject \\(H_0\\).\nTo compute this probability, we require the Null distribution of \\(T\\). That is, if \\(\\mu_{int}=\\mu_{ext}\\), what range of values can \\(T\\) take, and how often does a given range of values of \\(T\\) occur? If the study were replicated many many times in a scenario where the type of motivation has no effect on creativity (i.e., \\(\\mu_{int}=\\mu_{ext}\\)), then \\(T\\) could be computed for each replication to obtain the Null distribution of \\(T\\). Such an approach is unrealistic. In practice, one of two methods can be used to obtain the Null distribution.\n\nSimulation-based approach – simulate sample data sets many times under the assumption that motivation type has no effect on creativity. Then calculate the proportion of these simulated sample data sets that provided a value of \\(T\\) of 2.926 or greater and a value of \\(T\\) of -2.926 or smaller.\nTheory-based approach – Under certain assumptions, statistical theory provides a distribution model for the distribution of \\(T\\) under \\(H_0\\). This model can then be used to determine the proportion of times that \\(T\\) is 2.926 or greater or \\(T\\) is -2.926 or smaller\n\n\n11.1.1 A simulation-based approach\nA simulation-based approach is used here to illustrate the hypothesis testing framework. The simulation approach used here is called a randomization test. If \\(\\mu_{int}=\\mu_{ext}\\), then motivation type has no effect on creativity. So under \\(H_0\\), a student in the intrinsic group would obtain about the same score as a student in the extrinsic group. In fact, we could just randomly shuffle the students between the groups (extrinsic, intrinsic), since these groups are the same in terms of creativity, as it is not affected by motivation type. Note that this randomization (randomly shuffling students between groups) simulates the assignment of students to groups in a scenario of when a students’ creativity score is independent of motivation type.\nSince the random shuffling of students between the two groups is independent of the type of motivation (intrinsic or extrinsic) that they are thinking about, any difference between the groups in terms of creativity is due to chance. That is, the size of the corresponding value of \\(T\\) is due to randomly shuffling students between the groups (not due to motivation type). Another way to view it is that chance enters through the random assignment of units to treatments and nothing else under \\(H_0\\). The figure below shows the results of one such simulation. The resulting size of \\(T\\) under this simulation represents one observed value of \\(T\\) due to chance when the null hypothesis is assumed to be true.\n\n\n\n\nThe result of randomly shuffling students between groups\n\n\n\n\nThis simulation process is repeated enough times so that we have a good idea of the shape of the Null distribution of \\(T\\). The figure below shows a histogram of \\(T\\) obtained from 10,000 simulations, with the x-axis representing the range of the value of \\(T\\) under \\(H_0\\). How often would one observe a value of \\(T\\) of -2.962 or smaller, or observe a value of \\(T\\) of 2.962 or greater?\n\n\n\n\n\nFigure 11.1: Histogram of test statistic T, calculated from 10,000 different randomizations. The area to the right of the observed value of test statistic (T=2.926) is shaded light pink.\n\n\n\n\n\nOut of the 10,000 randomizations, only 58 produced a value of 2.926 or larger. That is, it appears that a value of \\(T\\) of at least 2.926 under \\(H_0\\) would only occur about 0.6% of the time according to the above histogram. Such a low chance or probability reflects that observing such a large value of \\(T\\) under \\(H_0\\) from chance alone is very rare. Since it is nearly impossible to observe a value of \\(T=2.926\\) or greater (as well as \\(-2.926\\) or smaller) under \\(H_0\\), this reflects that the value of \\(T=2.926\\) inconsistent with \\(H_0\\), and we reject \\(H_0\\) in favor of \\(H_a\\). Here, we conclude that students tend to display different levels of creativity when they are thinking about intrinsic motivations rather than extrinsic motivations.\n\n11.1.2 Theory-based approach\nUnder certain conditions, statistical theory provides a distribution model for the null distribution of \\(T\\). This model can then be used to determine the proportion of times that a value of \\(T\\) of 2.926 or larger would be observed under \\(H_0\\). Under these certain conditions, most of the theory-based approaches to be discussed work because of a theorem in Statistics called the Central Limit Theorem (CLT). To paraphrase the CLT, it states that if we repeatedly take independent random samples from each population (intrinsically and extrinsically motivated students), the distribution of the difference in sample means (\\(\\bar{x}_{int}-\\bar{x}_{ext}\\)) tends to resembles a normal distribution as the sample size increases. This occurs regardless of the shape of the distribution of the populations. As a consequence, the distribution of the test statistic,\n\\[T=\\frac{(\\bar{y}_{int}-\\bar{y}_{ext}) -(\\mu_{int}-\\mu_{ext})}{\\sqrt{\\frac{s_{int}^2}{n_{int}}+\\frac{s_{ext}^2}{n_{ext}}}},\\]\nis a t-distribution with degrees of freedom \\(df=min(n_{int}-1, n_{ext}-1)=min(23-1, 24-1)=22\\)\n\n\n\n\n\nFigure 11.2: The distribution model for T: A t-distribution with 22 degrees of freedom. The area to the right of the observed value of test statistic (T=2.926) is shaded light pink.\n\n\n\n\n\nUsing the distribution model for T under \\(H_0\\), an extreme value of \\(T\\) (2.926 or greater, or -2.926 or smaller) would only occur about 0.8% of the time according to the above distributional model. Such a probability (.008) reflects that observing such a large value of \\(T\\) under \\(H_0\\) from chance alone is very rare. This indicates that the value of \\(T=2.926\\) inconsistent with \\(H_0\\), and we reject \\(H_0\\) in favor of \\(H_a\\). We conclude that students tend to display different levels of creativity when they are thinking about intrinsic motivations rather than extrinsic motivations.\n\n11.1.3 Two-sided and one-sided hypothesis test.\nThe hypothesis test conducted in the case study is what is called a two-sided hypothesis test. If a one-sided test would have been conducted, say with alternative \\(H_a:\\delta > 0\\), then one is exploring only one direction of possibilities. A one-sided hypothesis test may be appropriate when the interest is in a single direction, but generally, we want to consider all possibilities. While one may suspect a certain directional relationship, formulating \\(H_a\\) to confirm to ones belief about the study will generally inflate what is called a Type 1 Error rate (discussed later in this section). Further, such practices subject themselves to confirmation bias1. With a two-sided hypothesis, it allows one to consider the possibility that the data may reflect something that we may not expect. It is recommended to always use a two-sided hypothesis as to keep an open mind when analyzing data and statistical evidence. In the modules, only two-sided tests are considered.\n\n11.1.4 p-values\nHere, we used the simulation based probability of \\(.006\\) to discuss the p-value, although the same applies to the theory-based probability of \\(.008\\). The probability of \\(0.006\\) is called a p-value. The p-value is a measure of the strength of the evidence against the null hypothesis. A small p-value indicates that the observed data are unlikely to occur, if \\(H_0\\) is true. That is, the null hypothesis is not a reasonable assumption, and we reject \\(H_0\\).\nConsider a p-value as a measure of evidence against \\(H_0\\). The smaller the p-value (closer to 0), the stronger the evidence against \\(H_0\\). The larger the p-value, the weaker to evidence against \\(H_0\\), with a p-value larger than \\(.10\\) signifying no evidence against the null hypothesis. Note that the p-values measures evidence against \\(H_0\\), not evidence for \\(H_0\\) (the absence of evidence is not evidence of absence). The chart below provides some guidance on interpreting the size of the p-value.\n\n\n\n\n\n\np-value\nInference\n\n\n\np-value>.10\nLittle to no evidence against the null hypothesis\n\n\n0.07\\(\\leq\\)p-value< 0.10\nSome evidence against the null hypothesis\n\n\n0.05\\(\\leq\\)p-value< 0.07\nModerate evidence against the null hypothesis\n\n\n0.001\\(\\leq\\)p-value< 0.05\nStrong evidence against the null hypothesis\n\n\np-value \\(\\leq\\) 0.001\nVery strong evidence against the null hypothesis\n\n\n\nFor the case study, the p-value was \\(0.0058\\). When the p-value is small compared to a set threshold, results are statistically significant. This means the data provide such strong evidence against null hypothesis, that we reject the null hypothesis in favor of the alternative hypothesis. This threshold is called the significance or \\(\\alpha\\) level, which is generally denoted by \\(\\alpha\\). The value of \\(\\alpha\\) is set by the practitioner or researcher, but in practice \\(\\alpha\\) is generally set to \\(0.01\\), \\(0.05\\), and \\(0.10\\), with \\(0.05\\) being the most common. By choosing an \\(\\alpha\\) level, you are conveying to the reader the strength of evidence required to reject \\(H_0\\). Regardless of the test used, the decision rule is as follows:\n\nIf p-value \\(\\leq \\alpha\\), we reject \\(H_0\\).\nIf p-value \\(> \\alpha\\), we fail to reject \\(H_0\\).\n\nIf \\(\\alpha=.05\\), \\(H_0\\) is rejected, so we may support what is reflected in \\(H_a\\). That is, based on the size of the p-value, we have strong evidence that subjects thinking about intrinsic motivations display more creativity than subjects who were thinking about extrinsic motivations at \\(\\alpha=.05\\).\nA word of caution with the idea of “statistically significant”. Note that while we concluded the result was statistically significant, it only indicated that the required amount of evidence against \\(H_0\\) was obtained. However, this result gives us no information regarding the size of the difference (the effect) in creativity scores between the groups. Any effect, no matter how inconsequential, can provide a small p-value if the sample size is extremely large. Also, a large effect may produce a large p-value if the sample size is small. Note that one may select an \\(\\alpha\\) that is smaller or larger than \\(0.05\\) depending on the consequence of the conclusion reached from a test. Regardless of the \\(\\alpha\\) chosen, the size of the effect and what is practical should be conveyed as well. For those interested on more discussion about the p-value, potential misuse of p-values, and caution with statistical significance, consider reading Baker et al. (2016), Amrhein, Greenland, and McShane (2019), and Lakens (2021).\n\n11.1.5 General steps in hypothesis testing\nThe following general steps should be followed when conducting a hypothesis test:\n\nCheck assumptions about the data, and write hypotheses.\n\n\nThe assumptions will vary depending on the test.\nWrite the null and alternative hypotheses in terms of the population parameters or treatment effects.\n\n\nCalculate the test statistic.\n\n\nThis calculation can be done using RStudio or another statistical software.\n\n\nDetermine the p-value using the calculated test statistic.\n\n\nThis will be done using RStudio or another statistical software.\n\n\nMake a decision based on the p-value and the chosen significance level.\n\n\nIf p-value \\(\\leq \\alpha\\), reject the null hypothesis. Otherwise, we fail to reject \\(H_0\\).\n\n\nState a conclusion in the context of the problem.\n\n\nThe conclusion should be in terms of what is reflected in \\(H_a\\) in context.\n\nWhen applying hypothesis tests in the modules, the testing procedures will generally follow these steps."
  },
  {
    "objectID": "siframework.html#confidence-interval-framework",
    "href": "siframework.html#confidence-interval-framework",
    "title": "11  Statistical inference framework via a case study",
    "section": "\n11.2 Confidence interval framework",
    "text": "11.2 Confidence interval framework\nA confidence interval (CI) is an approach for estimating a parameter or the treatment effect, and it provides an interval of values (constructed based on the sample data) to provide a range of plausible values of the parameters or effects of interest. In this case study, we construct a CI for \\(\\mu_{int} - \\mu_{ext}=\\delta\\). Associated with every CI is a confidence level. The confidence level specifies the “success rate” of the CI in capturing/containing the unknown value of the parameter in repeated sampling. That is, if samples are repeatedly obtained and a CI is computed each time, the confidence level specifies the percentage of those CIs that will contain the parameter or effect of interest. Thus, the confidence level associated with a CI tells one how “confident” one should be that the interval captures the value of the parameter or effect of interest (but not the confidence in any one particular interval).\nCIs generally follow the same basic form, which depends on the sample statistic(s) and the margin of error (MOE) of the sample statistic(s):\n\\[(sample ~ statistic(s)) \\pm MOE \\] Here, the CI for \\(\\mu_{int} - \\mu_{ext}\\) takes the form\n\\[(\\bar{y}_{int} - \\bar{y}_{ext}) \\pm MOE_{(\\bar{y}_{int} - \\bar{y}_{ext})}\\]\nwhere \\(MOE_{(\\bar{y}_{int} - \\bar{y}_{ext})}\\) denotes the MOE for \\((\\bar{y}_{int} - \\bar{y}_{ext})\\). For a given sample size, the MOE provides a upper bound to the difference between a particular estimate (i.e., a given \\(\\bar{y}_{int} - \\bar{y}_{ext}\\)) and the parameter (\\(\\mu_{int} - \\mu_{ext}\\)) that it estimates. Common choices for confidence levels are 90%, 95%, and 99%, with 95% being the most common. For the case study, a 95% confidence level for \\(\\mu_{int} - \\mu_{ext}=\\delta\\) is (1.196, 7.092). Thus, we are 95% confident that the difference between the true mean creativity scores for the intrinsic and extrinsic students is between 1.196 and 7.092. Since the interval only contains positive values, we can conclude that thinking about intrinsic motivations has a positive effect on creativity scores at 95% confidence."
  },
  {
    "objectID": "siframework.html#decision-errors-and-confidence-levels",
    "href": "siframework.html#decision-errors-and-confidence-levels",
    "title": "11  Statistical inference framework via a case study",
    "section": "\n11.3 Decision errors and confidence levels",
    "text": "11.3 Decision errors and confidence levels\nHypothesis tests are not flawless. There is always a chance that the wrong decision will be made. The decision errors are called Type I error and Type II error. A Type I error is made when \\(H_0\\) is rejected when in fact it should not have been rejected. A Type II error occurs when failing to reject \\(H_0\\) when in fact it should have been rejected. When making a decision in a hypothesis test (HT) one will not know if a Type I error or Type II error is made, but the good news is that one can control the chances of making a Type I error and reduce the chances of making a Type II error.\nThe probability of making a Type I error is denoted by \\(\\alpha\\), the significance level in a HT. By setting \\(\\alpha=.05\\), you ensure that the chances of making a Type I error is at most \\(\\alpha\\). The smaller \\(\\alpha\\), the smaller the chances of making a Type I error but at the same time the smaller \\(\\alpha\\) is set, the more evidence that is required to reject \\(H_0\\). It is worth noting that the level of \\(\\alpha\\) is related to the confidence level in a CI. For example, \\(\\alpha=.05\\) corresponds to a \\((1-\\alpha)100=(.95)100=95\\%\\) confidence interval. A \\(95\\%\\) confidence level means that the interval will fail to capture the true parameter or effect of interest at most \\(5\\%\\) of the time, and this failure corresponds to making a Type I error (wrong decision) in a HT at most \\(5\\%\\) of the time.\nWhile Type II error can not be directly controlled, it may be reduced by collecting more data (the more data you have the easier it should be to detect the desired effect and make the correct decision) or increasing \\(\\alpha\\) (making it easier to reject \\(H_0\\) decreases the chance of falsely failing to reject \\(H_0\\)). Thus, there is a trade-off in trying to keep \\(\\alpha\\) small and reducing the chances of making a Type II error."
  },
  {
    "objectID": "siframework.html#statistical-power",
    "href": "siframework.html#statistical-power",
    "title": "11  Statistical inference framework via a case study",
    "section": "\n11.4 Statistical power",
    "text": "11.4 Statistical power\nStatistical power is the chance or probability that you correctly reject the null hypothesis (correct decision). The opposite of this correct decision is a Type II error. Simply put, statistical power conveys how powerful the hypothesis testing procedure can detect a desired effect in a sample when it the desired effect exists. The higher the statistical power, the greater the chance it will find the desired effect. If you fail to reject \\(H_0\\), then either the desired effect is not present or your testing procedure failed to detect an effect (even though it exist). Such failures happen when the test has low statistical power.\nWhile there is no general approach for obtaining the desired statistical power, there are things one can do to increase statistical power. The following conditions increase the ability of a testing procedure to detect the desired effect:\n\nIncrease the sample size. The larger your sample, the more precise the estimate of the parameter or effect. This, in turn, leads to a larger test statistic (and more likely to reject \\(H_0\\)) and the CI will be narrower.\nIncrease the desired effect size. The larger the effect, the easier it should be to detect.\nIncrease \\(\\alpha\\). The larger the value of \\(\\alpha\\), the easier it becomes to reject \\(H_0\\).\n\nOf these conditions, researchers typically aim to increase the size of the sample to increase statistical power as the desired effect is either unknown or fixed in advance.\nNote that many of the equations/formulas used to compute the p-value, construct the CI, and other elements were not provided as software will be used to compute any required information from the data. The form of the test statistic was provided to help motivate the meaning of an extreme observed test statistic.\n\n\n\n\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. “Scientists Rise up Against Statistical Significance.” Nature 567 (7748): 305–7.\n\n\nBaker, Monya et al. 2016. “Statisticians Issue Warning on p Values.” Nature 531 (7593): 151–51.\n\n\nLakens, Daniël. 2021. “The Practical Alternative to the p Value Is the Correctly Used p Value.” Perspectives on Psychological Science 16 (3): 639–48.\n\n\nRamsey, F. L., and D. W. Schafer. 2013. “The Statistical Sleuth: A Course in Methods of Data Analysis.” Cengage Learning 30 (4): 413–14. https://doi.org/10.1080/00224065.1998.11979882."
  },
  {
    "objectID": "twoindptmeans.html",
    "href": "twoindptmeans.html",
    "title": "12  Two sample methods",
    "section": "",
    "text": "Figure 12.1: AI art generated by keywords “two-sample t-test”\nConsider comparing independent populations in regard to the mean or median of some attribute, or two treatments by comparing the true mean or median response under each treatment in an experiment. In this section, the following hypothesis tests (HT) for such comparisons are illustrated:\nFor confidence intervals (CIs), an overview of the two-sample \\(t\\)-based CI for comparing means and Wilcoxon-based CI for comparing medians is provided. Equations/formulas used in HT or CIs will be kept at a minimum as the focus is on using software to carry out these methods. That being said, those new to or not comfortable with HT or CIs should consider reviewing Chapter 11. These methods will be illustrated using a case study.\nFor any statistical inferential method to provide meaningful results, several assumptions are made about the data on which the method is intended to be used.\nIn these sections, we use R functions to conduct the inferential methods described earlier and provide detailed explanations of their use. While other functions may be used for tasks such as importing data and summarizing data numerically or graphically, we do not discuss them in this section as they have been covered in a previous module."
  },
  {
    "objectID": "twoindptmeans.html#two-sample-t-based-methods-for-comparing-two-means",
    "href": "twoindptmeans.html#two-sample-t-based-methods-for-comparing-two-means",
    "title": "12  Two sample methods",
    "section": "\n12.1 Two sample t-based methods for comparing two means",
    "text": "12.1 Two sample t-based methods for comparing two means\nThe independent two-sample t-test is a commonly used hypothesis test for comparing two groups or populations. There are two versions of this test: Student’s t-test and Welch’s t-test. Both tests can be used to compare whether the difference between the true averages (from two independent populations) is statistically significant or to determine if an effect (when comparing two treatments) is significant. This helps to determine if the difference or effect is due to chance or not. The t-based confidence interval for the difference in true averages provides a range of plausible values for the difference at a specified level of significance. These methods are generally referred to as t-based methods.\nStudent’s two-sample t-based methods assume assumptions 1, 2, 3, and 4 hold. The version of the t-based methods used here are called the Welch two-sample t-based methods, which do not require homogeneity of variances assumption. The Welch two-sample t-test statistic takes the following form:\n\\[T=\\frac{(\\bar{y}_{A}-\\bar{y}_{B}) -(\\mu_{A}-\\mu_{B})}{\\sqrt{\\frac{s_{A}^2}{n_{A}}+\\frac{s_{A}^2}{n_{B}}}},\\]\nwhere \\(n\\) and \\(s\\) denote the sample size and sample standard deviation from each group.\nThe data should consist of two variables of interest: a numerical variable that measures the outcome or response of interest and a factor or grouping variable with two levels. The grouping variable distinguishes the different populations or treatments (A or B). The hypothesis takes the form\n\\[H_0: \\mu_A - \\mu_B=0 \\qquad H_a: \\mu_A -\\mu_B \\neq 0\\]\nOne may also consider the test in terms of an effect \\(\\delta_\\mu=\\mu_A - \\mu_B\\),\n\\[H_0: \\delta_\\mu=0 \\qquad H_a: \\delta_\\mu \\neq 0\\]\nUnder \\(H_0\\), the null distribution of T is a t-distribution with a certain degree of freedom1, and it is used to compute the p-value. The function two.mean.test()2 computes the test statistic and p-value for the two-sample t-test, as well as provide a confidence interval for \\(\\mu_A - \\mu_B\\).\n\n\n\n\n\n\nR functions\n\n\n\n### two.mean.test( y ~ x , data , first.level,\n###            direction, conf.level,\n###            welch)\n# y: Replace y with the name of the resposne (measured) \n#  variable of interest.\n# x: Replace x with the name of the factor or grouping\n#  variable that distinguishes the different populations \n#  or treatments.\n# data: Set equal to the dataframe name.\n# first.level: Set equal to the level/category from the \n#       grouping variable. It determines how the difference \n#       in sample means is computed.  It should be consistent \n#       with the formulation of the hypothesis.\n# direction: Set equal to the sign in the alternative: \"two.sided\",\n#             \"greater\" , or \"less\".\n# conf.level: Set equal to the confidence level for the CI (default is .95). \n#               The function will always provide a CI by default.\n# welch: Set equal to TRUE (default) for Welch's t-test. Set\n#        to FALSE for Student's t-test.\n\n\nMany of the R functions used in this resource will have identical arguments. Therefore, while the arguments will be provided for any functions used, common arguments such as data, direction, … will generally only be described once within a given module.\n\n\n\n\n\n\nNote\n\n\n\nWe apply two-sample t-based methods (hypothesis testing and CI) to the data described in the case study provided in Chapter 3. The goal is to determine if there is a significant difference in PM10 concentration between Kern and Fresno counties and to estimate the difference between the true means.\nThe data provided in the case study consists of several variables (PM10, AQI, …, county, and City.Name). A subset of the data was created so that it included only PM10 (daily PM10 levels) and county (county in which the monitoring site is located) for the two counties of interest. To start, the data are imported and numerical and graphical summaries of the data would be created. However, graphical and numerical data summaries are covered in other modules. The focus here is to assess model assumptions either numerically or graphically.\n\n\nR code\nVideo\n\n\n\n\nCode# Import the data and save the data in a \n# dataframe called 'KernFresnoPM10df'\nKernFresnoPM10df <- read.csv(\"datasets/KernFresnoDailyPM10.2021.csv\")\n\n# Load the 'dplyr' package, which provides the 'mutate()' function.\nlibrary(dplyr) \n\n# Tell R that 'county' should be treated as a factor variable by using mutate().\nKernFresnoPM10df <- mutate( KernFresnoPM10df, \n              county= as.factor( county ) )\n\n\n# Print a summary of the data frame, including \n# variable names and other information.\nsummary( KernFresnoPM10df ) \n#>       PM10             AQI             temp           Latitude    \n#>  Min.   :  0.00   Min.   :  0.0   Min.   : 35.13   Min.   :35.05  \n#>  1st Qu.: 18.00   1st Qu.: 17.0   1st Qu.: 54.04   1st Qu.:35.44  \n#>  Median : 31.00   Median : 29.0   Median : 64.64   Median :35.64  \n#>  Mean   : 37.34   Mean   : 32.1   Mean   : 66.78   Mean   :35.98  \n#>  3rd Qu.: 48.00   3rd Qu.: 44.0   3rd Qu.: 80.00   3rd Qu.:36.79  \n#>  Max.   :437.00   Max.   :316.0   Max.   :102.00   Max.   :36.99  \n#>                                   NA's   :391                     \n#>    Longitude        windspeed          state              county    \n#>  Min.   :-119.8   Min.   : 0.7708   Length:2773        Fresno:1046  \n#>  1st Qu.:-119.7   1st Qu.: 2.4333   Class :character   Kern  :1727  \n#>  Median :-119.0   Median : 3.6500   Mode  :character                \n#>  Mean   :-118.9   Mean   : 4.4577                                   \n#>  3rd Qu.:-118.1   3rd Qu.: 5.2583                                   \n#>  Max.   :-117.7   Max.   :22.4625                                   \n#>                                                                     \n#>   City.Name        \n#>  Length:2773       \n#>  Class :character  \n#>  Mode  :character  \n#>                    \n#>                    \n#>                    \n#> \n\n\n# Load the 'lattice' package, which provides the 'bwplot()' function.\nlibrary(lattice) \n\n# Create a box plot of Air Quality Index (AQI) by county levels using the 'bwplot()' function\nbwplot(AQI ~ county,                   #  y ~ x\n       data = KernFresnoPM10df,        # set the data frame to use\n       xlab = \"County\",                # set the x-axis label\n       ylab = \"AQI\",                   # set the y-axis label\n       main = \"AQI Distribution by County\", # set the plot title\n       fill = c(\"gray\", \"brown\"))      # fill the boxes with specified colors\n\n\n\n\n\n\nCode\n# Load the 'mosaic' package, which allows using \n# formula expressions in 'mean()' and 'sd()'\nlibrary( mosaic ) \n\n# Compute the sample mean for each county\nmean( AQI ~ county , \n   data= KernFresnoPM10df )\n#>   Fresno     Kern \n#> 35.20076 30.21656\n\n# Compute the sample standard deviation for each county\nsd( AQI ~ county , \n  data= KernFresnoPM10df )\n#>   Fresno     Kern \n#> 22.18129 23.16972\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference in samples means between Kern and Fresno county is\n\\[30.217-35.201=\\bar{x}_K-\\bar{x}_F=-4.984.\\]\nWhile the observed difference serves as one estimate of \\(\\mu_K - \\mu_F\\), a confidence interval (CI) provides a better sense of the size of the difference between \\(\\mu_K\\) and \\(\\mu_F\\). Additionally, a hypothesis test is used to determine whether this difference is significant, but this depends on certain assumptions about the data. While the samples are not necessarily random, they are independent from each other in the sense that the measurements in Kern county are not affected by the measurements in Fresno county. Although random sampling is ideal, the t-based methods can still be used with non-random samples, but any conclusions drawn only apply to the specific sample and not necessarily to a larger population3. According to the summary output, the sample size for each group is large. A common rule of thumb is that a sample is considered large if it contains more than 30 observations4. The boxplots indicate that the variability in PM10 concentration appears to be about the same for both counties, and that there are several outliers present in both counties.\nAlthough the boxplots suggest the variability is about the same in both groups, for illustration, we also assess numerically using the sample standard deviations. The standard deviations for Fresno and Kern are 22.18 and 23.17, respectively, and they appear to be similar. One rule of thumb for comparing the variability in two groups is to take the ratio of their sample standard deviations (22.18/23.17 or 23.17/22.18). If the ratio falls between 0.5 and 2, then the variability in both groups is considered similar enough. However, it is important to note that graphical assessment of assumptions is generally sufficient and this rule of thumb should not be used if outliers are present.\nOverall, not all data assumptions are met. Alternative methods to be discussed would be more appropriate for this data, but for illustration we analyze the data using the two-sample t-based methods. Let \\(\\mu_K\\) denote the true mean PM10 levels in Kern county. \\(\\mu_F\\) is defined analogous for Fresno county. Since an aim is to determine if there is significant difference between PM10 levels in Kern and Fresno county, the hypothesis is\n\\[H_0: \\mu_K - \\mu_F=0 \\qquad H_a: \\mu_K- \\mu_F \\neq 0\\]\nThe following code uses two.mean.test() to compute the test statistic, the corresponding p-value, and a 99% confidence interval:\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function two.mean.test() so that it's R's memory.\nsource(\"rfuns/two.mean.test.R\")\n\n# Recall that the data was imported and stored in \"KernFresnoPM10df\"\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: The county that appears first in the hypothesis\n#                (Kern county), so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n# conf.level: Set equal .99 since we seek a 99% CI\n# welch: The Welch t-test is the default method here, so using \n#     the default value (TRUE)\n\n\ntwo.mean.test( PM10 ~ county ,          # y ~ x\n               data = KernFresnoPM10df ,# specify the data frame to use\n               first.level = \"Kern\" ,   # specify the first level in the hypothesis\n               direction = \"two.sided\" ,# specify a two-sided alternative\n               conf.level = .99 )       # set the confidence level to .99\n#>      Theoretical-based two-sample test for independent samples \n#>                               \n#> formula:  PM10 ~ county \n#> sample mean of  Kern  group: 35.06717 \n#> sample mean of  Fresno  group: 41.09943 \n#> sample sd of  Kern  group: 32.26383 \n#> sample sd of  Fresno  group: 31.88051 \n#> \n#> difference between groups: ( Kern  group ) - (  Fresno  group ) \n#> obs t-test statistic: -4.807498              p-value = 0 \n#> df=  2225.184 \n#> direction: two.sided \n#> \n#> Confidence level:  0.99 \n#> CI:(  -9.267081 ,  -2.797435 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output includes the following information:\n\nThe sample mean and standard deviation under each level of the grouping/factor variable.\nThe value of the observed statistic: \\(T=-4.10\\)\np-value\nA confidence interval\n\nBased on the output, the observed value of the test statistic is \\(T=-4.10\\). The p-value is extremely small (0). Since p-value\\(\\leq\\alpha=.01\\), we have significant evidence to reject \\(H_0\\). Based on the size of the p-value, we may also say that there is very strong evidence to reject \\(H_0\\). By setting \\(\\alpha = 0.01\\), we require “stronger evidence” to reject \\(H_0\\) than if we had used a larger significance level. Therefore, we have strong evidence that there is significant difference between PM10 levels in Kern and Fresno county at the designated monitoring sites.\nThe output also provides a 99% CI for the difference in true means. The CI is \\((-9.270, -2.794)\\), and we are 99% confidence that the difference in true means is anywhere between \\(-9.270\\) and \\(-2.794\\). Since the interval does not contain \\(0\\), it is clear that the true average PM10 levels at Fresno county are higher than in Kern county at the designated monitoring sites at 99% confidence. Note that this conclusion only applies to the monitoring sites, since PM10 levels were not measured at random locations within each county."
  },
  {
    "objectID": "twoindptmeans.html#wilcoxon-based-methods-comparing-two-medians-theory-based-non-parametric-approach",
    "href": "twoindptmeans.html#wilcoxon-based-methods-comparing-two-medians-theory-based-non-parametric-approach",
    "title": "12  Two sample methods",
    "section": "\n12.2 Wilcoxon-based methods comparing two medians (theory based non-parametric approach)",
    "text": "12.2 Wilcoxon-based methods comparing two medians (theory based non-parametric approach)\nWhen assumption 2 and/or 3 do not hold, t-based methods are no longer applicable. The Wilcoxon-based methods do not require the normality assumption when the samples are small nor is it affected by outliers. Further, the distributions from which each data were sampled do not have to be known5 for these methods, but these methods do require for the distributions to have approximately the same shape. Since the distribution does not have to be a known distribution, it called a non-parametric method6. In short, these methods require assumptions 1 and 4, but also require for the distributions from which each data were sampled to have approximately the same shape and for each sample to be of at least size 10.\nThe Wilcoxon rank sum test (also called the Mann-Whitney test) can be applied to compare whether the difference between true medians (from two independent populations) are really all that different from zero or to determine if an effect (when comparing two treatments via medians) is significant or not as opposed to the size of the difference or effect being due instead to random chance.\nThe test statistic for the Wilcoxon rank sum test depends on the ranks of the observed data, and its value will be provided by software. However, some details regarding the computation of the test statistic are provided below:\n\nList the observations for both samples from smallest to largest across both groups.\nAssign the numbers \\(1\\) to \\(n_A+n_B=N\\) to the observations (across both groups) with 1 assigned to the smallest observations and \\(N\\) to the largest observation. These are called ranks of the observations.\nIf there are ties (due to repeated values) in the combined data set, the ranks for the observations in a tie are taken to be the average of the ranks for those observations.\nLet \\(W\\) denote the sum of the ranks for the observations from group A.\n\nThe test statistic then takes the form\n\\[U=W-\\frac{n_A(n_A+1)}{2}\\]\nWhereas the null hypothesis of the two-sample t test is equal means, the null hypothesis of the Wilcoxon test7 is taken as equal medians:\n\\[H_0: \\eta_A - \\eta_B=0 \\qquad H_a: \\eta_A -\\eta_B \\neq 0\\]\nOne may also consider the test in terms of an effect \\(\\delta_\\eta=\\eta_A - \\eta_B\\),\n\\[H_0: \\delta_\\eta=0 \\qquad H_a: \\delta_\\eta \\neq 0\\]\nUnder \\(H_0\\), the null distribution of U is called d distribution of the Wilcoxon rank sum statistic and it is used to compute the p-value8. The function two.wilcox.test9 computes the test statistic and p-value10, as well as provide a confidence interval (CI) as an estimated measure of how these distributions differ.\nThe CI is provided by this method is for the difference in medians between two randomly chosen observations (one from each group)11. This still provides some sense of how different the medians are from each other. However, if both distributions from which the data were sampled are symmetric, then the CI provided is for a difference in medians.\n\n\n\n\n\n\nR functions\n\n\n\n### two.wilcox.test( y ~ x , data , first.level,\n###            direction, conf.level)\n# y: Replace y with the name of the the resposne (measured) \n#  variable of interest\n# x: Replace x with the name of the factor or grouping\n#  variable that distinguishes the different populations \n#  or treatments\n# data: Set equal to the dataframe being used.\n# first.level: Set equal to a level/category from the grouping \n#              variable. It determines how the difference in \n#              sample means is computed.  It should be consisent \n#              with the formulation of the hypothesis.\n# direction: Set equal to the sign in the alternative: \"two.sided\", \n#            \"greater\" , or \"less\"\n# conf.level: Set equal to the confidence level for the CI (default \n#             is .95). The function will always provid a CI by default.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe apply the two-sample Wilcoxon-based methods (hypothesis testing and CI) to the data described in the case study provided in Chapter 3 to determine if there is a significant difference in PM10 concentration between Kern and Fresno county and to get a sense of how different the true medians are. The four assumptions of the data discussed earlier in this chapter were explored when illustrating the t-based methods. Although the data consists of large samples and the independence and equal variance assumptions are reasonable, the presence of many outliers in the data should be noted. The Wilcoxon-based methods are not affected by outliers but the distributions from which each sample were obtained should have the same shape. This is explored via a violin boxplot.\n\nCode# Import data. \nKernFresnoPM10df <- read.csv(\"datasets/KernFresnoDailyPM10.2021.csv\")\n\n# Load the 'dplyr' package.\nlibrary(dplyr) # provides the mutate() function\n\n\n# Use 'mutate' to convert 'county' into a factor variable.\nKernFresnoPM10df <- mutate( KernFresnoPM10df, \n              county= as.factor( county ) )\n\nlibrary(lattice) # Provides the bwplot() function\n# Create a violin plot \nbwplot( PM10 ~ county , \n    data= KernFresnoPM10df , \n    xlab= \"County\" , \n    ylab= \"PM10\" ,\n    main= \"PM10 vs county\" , \n    panel= panel.violin )\n\n\n\n\n\n\n\nThe violin boxplot makes it clearer that the distribution of PM10 in both counties approximately have the same shape.\nLet \\(\\eta_K\\) denote the true median PM10 levels in Kern county. \\(\\eta_F\\) is defined analogous for Fresno county. Since an aim is to determine if there is significant difference between PM10 levels in Kern and Fresno county, the hypothesis is\n\\[H_0: \\eta_K - \\eta_F=0 \\qquad H_a: \\eta_K- \\eta_F \\neq 0\\]\nThe following code uses two.wilcox.test to compute the test statistic, the corresponding p-value, and a 99% confidence interval:\n\nCode# Source the function so that it's R's memory.\nsource(\"rfuns/two.wilcox.test.R\")\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: In the hypothesis, Kern county appeared first in the\n#       difference, so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n\nlibrary(mosaic) # allows the null distribution to be plotted.\n\ntwo.wilcox.test( PM10 ~ county , \n         data= KernFresnoPM10df , \n         first.level= \"Kern\" ,\n         direction= \"two.sided\" , \n         conf.level = .99 )\n#>      Theoretical-based two-sample test for independent samples \n#>                               \n#> formula:  PM10 ~ county \n#> sample median of  Kern  group: 28 \n#> sample median of  Fresno  group: 34 \n#> sample IQR of  Kern  group: 30 \n#> sample IQR of  Fresno  group: 29 \n#> \n#> method:  Wilcoxon rank sum test with continuity correction \n#> difference between groups: ( Kern  group ) - (  Fresno  group ) \n#> obs test statistic: U=  759219.5              p-value = 0 \n#> obs standardized test statistic: Z=  -7.048 \n#> direction: two.sided \n#> \n#> difference in location (pseudomedian):  -6.000007 \n#> confidence level:  0.99 \n#> CI:(  -8.000049 ,  -3.999966 )\n\n\n\n\nThe output includes the following information:\n\nThe sample median and IQR under each level of the grouping/factor variable.\nThe value of the observed statistic\np-value\nA confidence interval\n\nBased on the output, the observed value of the test statistic is \\(U=759219.5\\). The p-value is extremely small (0). Since p-value\\(\\leq\\alpha=.01\\), we have significant evidence to reject \\(H_0\\). Therefore, we have strong evidence that there is significant difference between PM10 levels in Kern and Fresno county at the designated monitoring sites.\nThe output also provides a 99% CI for median of difference between all pairs of observations between both groups (not the difference in group medians). Based on the CI, we are 99% confident that the median difference between two randomly chosen PM10 levels from each county is anywhere between -8.000 and -4.000. Since the interval does not contain \\(0\\), it is clear that the PM10 levels at Fresno county are higher than in Kern county at the designated monitoring sites at 99% confidence. Note that this conclusion only applies to the monitoring sites, since PM10 levels were not measured at random locations within each county."
  },
  {
    "objectID": "twoindptmeans.html#a-randomization-test-using-a-t-test-test-statistic",
    "href": "twoindptmeans.html#a-randomization-test-using-a-t-test-test-statistic",
    "title": "12  Two sample methods",
    "section": "\n12.3 A randomization test using a t-test test statistic",
    "text": "12.3 A randomization test using a t-test test statistic\nRandomization methods seek to determine if the observed data in each group is different due to an effect of a factor, compared to what would be expected by random chance if the groups were the same and the factor had no effect on the response. These methods can test whether the observed data in each group are different from a randomization distribution generated by randomly allocating the observed data into the two groups. If the observed data are in fact the same (no effect), then it should be just as likely as any ordering generated by random allocation of the data between groups. In other words, if the factor being studied has no effect on the response, then the behavior of the data should be similar to what we would expect if the observations were randomly assigned to the two groups (randomized group assignment). If, however, the groups are different due to a significant factor effect, then the behavior of the data would not be similar to what we would expect if the observations were randomly assigned to the two groups.”\nIf assumption 2, 3, and/or 3 do not hold, t-based or Wilcoxon methods are no longer applicable. Randomization technically does not require any assumptions, although some assumptions or conditions may be needed depending on how the effect is being quantified (differences in mean, medians, or other measures). Here, we use randomization tests to determine if an effect is significant when using a difference in means and difference in medians. Keep in mind that randomization tests do not have sample size requirements. However, the less data you have, the less power the test has to detect a significant effect or significant difference between two groups.\nComparing means\nHere we use a randomization test when quantifying the attribute using the mean. As a test statistic, we use the form of the test statistic provided by Welch’s t-test for the randomization test. The interest here is in determining if an effect (when measures as a difference in means) is significant. Since the test statistic uses both the sample mean and sample standard deviation, this version of the randomization test will require assumption 2. The R function used to conduct the randomization test has similar syntax to that of the t-based methods, but with additional arguments.\n\n\n\n\n\n\nR functions\n\n\n\n### two.mean.test( y ~ x , data , first.level,\n###            direction, randtest, \n###            nshuffles)\n# randtest: Set equal to TRUE to obtain the test statistic and p-value\n#      for the randomization test (for comparing means).\n# nshuffles: Set equal to the desired number of randomizations. \n\n\n\n\n\n\n\n\nNote\n\n\n\nDiscussion of the t-based methods showed that outliers are present in the data from the case study discussed in Chapter 3, so this test is not appropriate for the data. However, for illustration we carry out the test on the data. The aim is to determine if there is significant county effect on PM10 levels when considering Kern and Fresno county. Let \\(\\delta_\\mu=\\mu_K - \\mu_F\\). The hypothesis is\n\\[H_0: \\delta_\\mu=0 \\qquad H_a: \\delta_\\mu \\neq 0\\]\nThe following code uses two.mean.test() to compute the test statistic and the corresponding p-value for a randomization test involving a difference in means.\n\n\nR code\nVideo\n\n\n\n\nCode# Data was imported earlier in the section \n# and 'mutate' converted 'county' into a factor variable.\n\n# Source the function two.mean.test() so that it's R's memory.\nsource(\"rfuns/two.mean.test.R\")\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: In the hypothesis, Kern county appeared first in the\n#       difference, so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n# randtest: Set equal to TRUE. \n# nshuffles: Set equal to 30,000 \n\n\ntwo.mean.test(PM10 ~ county ,           # specify the variables for the test\n              data = KernFresnoPM10df , # specify the data frame to use\n              first.level = \"Kern\" ,    # specify the first level in the hypothesis\n              direction = \"two.sided\" , # specify a two-sided alternative\n              randtest = TRUE ,         # perform a simulation-based test\n              nshuffles = 30000 )       # set the number of randomizations to 30,000\n\n\n\n#>   Simulation based two-sample test for independent samples \n#>               \n#> formula: PM10 ~ county \n#> sample mean of Kern group: 35.06717 \n#> sample mean of Fresno group: 41.09943 \n#> sample sd of Kern group: 32.26383 \n#> sample sd of Fresno group: 31.88051 \n\n#> difference between groups: ( Kern group ) - ( Fresno group ) \n#> obs t-test statistic: -4.807498       p-value = 0 \n#> df= N/A \n#> direction: two.sided \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output is nearly identical as when conducting a Welch t-test in that it provides some summary statistics, the observed value of the test statistic, and p-value. These type of randomization methods do not provide CIs so one can not assess the size of the effect. At \\(\\alpha=.01\\), we obtain the same decision and conclusion as with the previous tests.\n\n\nComparing medians\nHere we quantify the attribute using the median. For the test statistic in the randomization test, we use the form of the test statistic provided by the Wilcox rank sum test. The interest here is in determining if an effect (when measures as a difference in medians) is significant. The R function to carry out this test is the same function as with the wilcoxon-based methods, but with some additional arguments to conduct the randomization test.\n\n\n\n\n\n\nR functions\n\n\n\n### two.wilcox.test( y ~ x , data , first.level,\n###            direction, conf.level)\n###            randtest, nshuffles)\n# randtest: Set equal to TRUE to obtain the test statistic and p-value\n#      for the randomization test (for comparing medians).\n# nshuffles: Set equal to the desired number of randomizations. \n\n\n\n\n\n\n\n\nNote\n\n\n\nDiscussion of the wilcoxon-based methods showed that the data described the case study provided in Chapter 3 are appropriate for the data. However, given the large sample, the Wilcox rank sum test approximates the p-value using the standard normal distribution. This approximation generally works well, but the quality of the approximation declines when the underlying distributions of the two groups are highly skewed or have heavy tails, or as the number of ties increases. Note below that the histogram shows that the PM10 levels in both counties are right skewed. Further, there are over 2000 ties in the data. Therefore, this randomization test provides a more accurate result for this data.\n\nCode# Data was imported earlier in the section \n# and 'mutate' converted 'county' into a factor variable.\n\n# Load the 'lattice' package\nlibrary(lattice) # provides the histogram() function\nhistogram(~ PM10 | county ,         # ~ \"variable\" | \"grouping factor\"\n          data = KernFresnoPM10df , # Specify the data frame to use\n          xlab = \"PM10\" ,           # Label the x-axis\n          ylab = \"Frequency\" ,      # Label the y-axis\n          main = \"PM10 by county\" ) # Add a title to the plot\n\n\n\n\n\n\n\nTo determine if there is significant county effect on PM10 levels when comparing Kern and Fresno county, let \\(\\delta_\\eta=\\eta_K - \\eta_F\\). The hypotheses are\n\\[H_0: \\delta_\\eta=0 \\qquad H_a: \\delta_\\eta \\neq 0\\]\nThe following code uses two.wilcox.test() to compute the test statistic and the corresponding p-value for a randomization test involving a difference in medians\n\n\nR code\nVideo\n\n\n\n\nCode# Data was imported earlier in the section \n# and 'mutate' converted 'county' into a factor variable.\n\n# Source the function two.wilcox.test() so that it's R's memory.\nsource(\"rfuns/two.wilcox.test.R\")\n\n# The function will require the following information:\n# y: replace with 'PM10'\n# x: replace with 'county'\n# data: set equal to a 'KernFresnoPM10df'\n# first.level: In the hypothesis, Kern county appeared first in the\n#       difference, so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n# randtest: Set equal to TRUE. \n# nshuffles: Set equal to 30,000 \n\ntwo.wilcox.test(PM10 ~ county ,           # Specify the variables for the test\n                data = KernFresnoPM10df , # Specify the data frame to use\n                first.level = \"Kern\" ,    # Specify the first level in the hypothesis\n                direction = \"two.sided\" , # Specify a two-sided alternative\n                randtest = TRUE ,         # Perform a simulation-based test\n                nshuffles = 30000 )       # Set the number of randomizations to 30,000\n#>   Simulation based two-sample test for independent samples \n              \n#> formula: PM10 ~ county \n#> sample median of Kern group: 28 \n#> sample median of Fresno group: 34 \n#> sample IQR of Kern group: 30 \n#> sample IQR of Fresno group: 29 \n\n#> method: randomization test \n#> difference between groups: ( Kern group ) - ( Fresno group ) \n#> obs test statistic: U= 759219.5       p-value = 0 \n#> direction: two.sided \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output is nearly identical as when conducting the Wilcox rank-sum test in that it provides summary statistics, the observed value of the test statistic, and p-value. These types of randomization methods do not provide CIs. At \\(\\alpha=.01\\), we obtain the same decision and conclusion as with the previous tests."
  },
  {
    "objectID": "twoindptmeans.html#sample-size-estimation-and-power-analysis-for-anova",
    "href": "twoindptmeans.html#sample-size-estimation-and-power-analysis-for-anova",
    "title": "12  Two sample methods",
    "section": "\n12.4 Sample size estimation and power analysis for ANOVA",
    "text": "12.4 Sample size estimation and power analysis for ANOVA\nIdeally, estimating sample size for a study is one of the first steps that researchers take prior to collecting data. Knowing the sample size required to detect a desired effect at the beginning of a project allows one to manage their data collection efforts. Further, this allows for one to determine how much statistical power the test will have to detect an effect if it exists.\nThe R function pwr.t2n.test() from the pwr R package can be used to calculate statistical power for a two sample t-test when the sample sizes, significance level (\\(\\alpha\\)), and effect size are provided. The function pwr.t.test from the same package will provide the sample sizes required for a given power level, significant level (\\(\\alpha\\)), and effect size. Note that variability in each sample is assumed to be about the same. It is assumed the appropriate assumptions about the data are met12.\n\n\n\n\n\n\nR functions\n\n\n\n### power.t2n.test( n1 , n2 , d , sig.level , \n###                    power, alternative)\n# n1: Set equal to the number of observations in first sample.\n# n2: Set equal to the effect size.\n# d: Set equal to the effect size.\n# sig.level: Set equal to the desired alpha value (significance level)\n# power: Set equal to the desired power (a number between 0 and 1). \n# alternative: The direction of the alternative. Set equal to \"two.sided\".\n#\n#\n### power.t.test(power, d , sig.level , \n###                  alternative , type)\n# power: Set equal to the desired power.\n# type: Set equal to \"two.sample\" .\n#\n\n\nThe effect size refers to Cohen’s \\(d\\), which is defined as the difference between the true means divided by the pooled standard deviation. Cohen’s \\(d\\) is typically interpreted as follows:\n\nSmall effect size: \\(d = 0.20\\)\nMedium effect size: \\(d = 0.50\\)\nLarge effect size: \\(d = 0.80\\) or higher.\n\nThese are suggested guidelines and may vary slightly depending on the specific field of research or context of the study.\n\n\n\n\n\n\nNote\n\n\n\nRefer to the data described in the case study provided in Chapter 3. Let’s suppose the goal is to determine the statistical power of a t-test if one wanted to detect a medium-sized effect when the sample size from each county is 100 at \\(\\alpha=.01\\).\n\nCode\nlibrary(pwr) # provides pwr.t2n.test\n\n# Note:  The output of this function call will be the  the statistical power for the t-test.\npwr.t2n.test(n1 = 100 ,          # Set the sample size for group 1\n             n2 = 100 ,          # Set the sample size for group 2\n             d = 0.50 ,          # Set Cohen's $d$\n             sig.level = 0.01 ,  # Set the significance level for the test\n             alternative = \"two.sided\") # Specify a two-sided alternative hypothesis\n#> \n#>      t test power calculation \n#> \n#>              n1 = 100\n#>              n2 = 100\n#>               d = 0.5\n#>       sig.level = 0.01\n#>           power = 0.8238225\n#>     alternative = two.sided\n\n\nThe statistical power of this test is \\(.824\\). To further increase the power, one may increase the effect size (the larger it is, the easier it is to detect), increase the value of \\(\\alpha\\) (make it easier to reject \\(H_0\\) and find a significant effect), and/or increase the sample sizes. The sample data consisted of at least 1000 observations from each county, so set each sample to \\(1000\\):\n\nCodepwr.t2n.test(n1 = 1000 ,\n             n2 = 1000 , \n             d = .50 , \n             sig.level = 0.01 ,\n             alternative = \"two.sided\" )\n#> \n#>      t test power calculation \n#> \n#>              n1 = 1000\n#>              n2 = 1000\n#>               d = 0.5\n#>       sig.level = 0.01\n#>           power = 1\n#>     alternative = two.sided\n\n\nIf instead you seek the required sample size for a given power, the argument power could be set to the desired power in the function pwr.t.test:\n\nCode# Note:  The output of this function call will be the required sample size for the t-test.\npwr.t.test(power = 0.90 ,         # Set the desired power level\n           d = 0.50 ,             # Set the standardized mean difference between groups\n           sig.level = 0.01 ,     # Set the significance level for the test\n           type = \"two.sample\" ,  # Set a two-sample t-test\n           alternative = \"two.sided\") # Specify a two-sided alternative hypothesis\n#> \n#>      Two-sample t test power calculation \n#> \n#>               n = 120.7055\n#>               d = 0.5\n#>       sig.level = 0.01\n#>           power = 0.9\n#>     alternative = two.sided\n#> \n#> NOTE: n is number in *each* group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis module provides a few methods for independent two-sample inference. Parametric methods (such as t-based methods) are generally more powerful (assuming all assumptions are reasonably met) than nonparametric methods (such as Wilcoxon-based methods). Nonparametric tests are based on fewer assumptions compared to their parametric counterparts. The cost of fewer assumptions is that nonparametric tests are generally less powerful than their parametric counterparts (i.e., when the alternative is true, they may be less likely to reject \\(H_0\\)). Nonparametric tests are used in cases where parametric tests are not appropriate.\n\n\n\n\nRamsey, F. L., and D. W. Schafer. 2013. The Statistical Sleuth: A Course in Methods of Data Analysis. \"Cengage Learning\"."
  },
  {
    "objectID": "anovalin.html",
    "href": "anovalin.html",
    "title": "Module: ANOVA, linear regression, and logistic regression",
    "section": "",
    "text": "This module discusses three topics\n\nSingle factor analysis of variance (ANOVA).\nSimple linear regression\nSimple logistic regression\n\nThese methods study the associations between two variables.\nSingle factor ANOVA is a method for assessing the differences between group means (more than two). This method is also known as a one-way ANOVA. In this scenario, we have a numerical response variable and a categorical explanatory variable with more than two levels. In ANOVA, the categorical explanatory variable is typically referred to as the factor variable.\nSimple linear regression models the linear relationship between an explanatory variable and a numerical response variable. Simple logistic regression is like simple linear regression but the response is a binary outcome.\nInferential methods will be covered for ANOVA, linear regression, and logistic regression.\n\nModule chapters:\nANOVA\nLinear regression\nLogistic regression"
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "13  Analysis of variance (ANOVA)",
    "section": "",
    "text": "AI art generated from the text “Analysis of variance”\nAnalysis of Variance (ANOVA) is a statistical method that analyzes the variability of the observations in a dataset and compares it between groups or factor levels to determine if there is a statistically significant difference among them. This section covers the basics of ANOVA, and the following inferential methods for ANOVA:\nEquations/formulas used in hypothesis testing or confidence intervals will be kept at a minimum as the focus is on the use of software to carry out these methods. That being said, those new to or not comfortable with hypothesis testing or confidence intervals may want to consider reviewing Chapter 11.\nTo ensure that statistical inferential methods provide meaningful results, several assumptions are made about the data on which the method is intended to be used.\nThe terms “factor levels”, “treatments”, and “groups” mean the same thing and will be used interchangeably, although “factor levels” or “treatments” are terms more associated with experiments and “groups” is a term more associated with observational studies. In these sections, R functions provided for conducting the aforementioned inferential methods are used and described. Other functions to import data, numerically or graphically summarize the data, and other tasks may be used but such functions are not described as they have been described and illustrated in detail in previous modules."
  },
  {
    "objectID": "anova.html#single-factor-or-one-way-anova",
    "href": "anova.html#single-factor-or-one-way-anova",
    "title": "13  Analysis of variance (ANOVA)",
    "section": "\n13.1 Single factor or one-way ANOVA",
    "text": "13.1 Single factor or one-way ANOVA\nANOVA partitions the total variability in the data (the sample variance of the all the data) in terms of variability between treatments and the variability within treatments. The variability between groups may be quantified by a measure called the mean square between treatments, denoted by \\(MST\\) or MS(Treatment)1. The variability within each treatment or group is quantified by the mean square error, denoted by \\(MSE\\) or MS(Error). The higher the MS(Treatment), the more the variability in the data are due to the differences among factor levels or groups. The higher the MS(Error), the more the variability in the data are due to differences within the groups. In ANOVA, the hypothesis are\n\\(H_0: \\mu_1= \\cdots=\\mu_k\\) (The mean response is the same across all factor levels or treatments)\n\\(H_a:\\) At least one treatment means (\\(\\mu_1\\), \\(\\mu_2\\), …, or \\(\\mu_k\\)) \\(\\mu_k\\) is different from the rest (The mean response is the not same across all factor levels or treatments)\nTo test the hypothesis, ANOVA compares the variability between groups to the variability within groups and it assesses whether the differences between group means are statistically significant. The ratio of these two measures provides the test statistic for the F-test in single factor ANOVA:\n\\[F=\\frac{ MS(Treatment)}{ MS(Error)}\\]\nThis test assumes assumptions 1-4 are reasonable for the data. Under \\(H_0\\), the mean response is assumed to be the same across all groups, so any differences between groups can be attributed to random chance or natural variation. If \\(H_0\\) is true and the mean response is the same across all groups, the variability in the data should be mainly due to variability within groups, so the MS(Treatment) should be small and the MS(Error) should be large (corresponding to a small value of \\(F\\)). However, if the mean response varies greatly by group, then the variability in the data may be attributed more to this variability between groups rather than due to variability within groups. In this case, the MS(Treatment) would be large and the MS(Error) would be small (corresponding to a large value of \\(F\\)). Under \\(H_0\\), the null distribution of \\(F\\) is an \\(F\\)-distribution with a numerator degree of freedom \\(k-1\\) and denominator degree of freedom \\(n-k\\), and the null distribution is used to compute the p-value.\nIf instead the variability in each factor level or groups or treatments differs from any other factor level or group, then the F-test for single factor ANOVA is not appropriate. However, a test called Welch’s F-test can be utilized. Welch’s F-test requires assumptions 1-3 be reasonable for the data but it does not require assumption 4. Welch’s F-test uses as a test statistics a ratio of mean squares as the standard F-test but each are weighted by group variances and it is denoted as \\(F_w\\)2, where\n\\[F_w=\\frac{ MS(Treatment)^w}{ MS(Error)^w}\\]\nBoth \\(MS(Treatment)^w\\) and \\(MS(Error)^w\\) are not the mean squares used to define the F-test for single factor ANOVA, but rather a weighted version of sorts to account for unequal spread under each treatment. This helps to correct for unequal variances and ensures that the F-test is more accurate. Under \\(H_0\\), the null distribution of \\(F_w\\) is an F distribution with a numerator degree of freedom \\(k-1\\) and denominator degree of freedom is a non-integer value (resulting for a complicated formula), and this null distribution is used to compute the p-value for Welch’s F-test.\nThe function sfaov()3 obtains the information needed to conduct an F-test or Welch’s F-test and more using the data.\n\n\n\n\n\n\nR functions\n\n\n\n### sfaov( y ~ x , data , welch )\n# y: Replace y with the name of the resposne (measured) \n#  variable of interest.\n# x: Replace x with the name of the factor or grouping\n#  variable that distinguishes the different populations \n#  or treatments.\n# data: Set equal to the dataframe name\n# welch: Set to TRUE (default) if 'Equal variance or spread' \n#        assumption is reasonable. Otherwise, set equal to FALSE.\n#\n\n\nMany of the R functions used in these modules will have identical arguments. Therefore, while the arguments will be provided for any functions used, arguments such as y, x, data, and welch will generally no longer be described in this module.\n\n\n\n\n\n\nNote\n\n\n\nWe apply an ANOVA F-test to the data described in the case study provided in Chapter 1 to determine if light intensities have a significant effect on survival rates of early-stage Delta smelt larvae. The data provided in the case study consists of three variables (Light, Turbidity, and Survival) with Light and Survival being the variables of interest. There are three levels of light intensity: 4.5 (low), 6.75 (medium), and 9 (high). Our aim here is to determine whether light intensity has a significant effect on the survival of the larvae at \\(\\alpha=.10\\).\nTo begin, we import the data, and then one would explore the data using numerical and graphical data summaries. Numerical and graphical data explorations were covered in other modules. However, we also use these type of summaries to assess model assumptions.\n\n\nR code\nVideo\n\n\n\n\nCode# Import data \nsurvdf <- read.csv(\"datasets/dssurv.csv\")\n\n# Obtain the names of the variables in the data frame\nnames( survdf )\n#> [1] \"Light\"     \"Turbidity\" \"Survival\"\n\n# Load the dplyr package\nlibrary(dplyr) # provides the mutate() function\n\n# Convert the \"Light\" and \"Turbidity\" variables to factors \n# using the mutate() function. \nsurvdf <- mutate(survdf, \n         Light = as.factor( Light ), \n         Turbidity = as.factor( Turbidity ) )\n\n# Summarize the variables in the data frame using the summary() function\nsummary( survdf )\n#>   Light   Turbidity    Survival    \n#>  high:8   high:8    Min.   :45.02  \n#>  low :8   low :8    1st Qu.:55.79  \n#>  med :4   med :4    Median :59.56  \n#>                     Mean   :62.22  \n#>                     3rd Qu.:67.24  \n#>                     Max.   :89.09\n \n# Load the lattice package for graphing functions\nlibrary(lattice)  \nbwplot( Survival ~ Light ,              # y ~ x\n    data= survdf ,                      # Specify dataframe\n    xlab= \"Light intensity level\" ,     # Set x-axis label\n    ylab= \"Survival rate\" ,             # Set y-axis label\n    main= \"Survival vs Light intensity\" , # Specify plot title\n    fill= c(\"gray\", \"brown\", \"cyan\") )  # Specify color for each plot\n\n\n\n\n\n\nCode\n\n# Source the function \"normqqplot.R\" so that \n# it's in R's memory.\nsource(\"rfuns/normqqplot.R\") # this function creates a normal QQ-plot\n\n# Create a normal QQ-plot under each group\nnormqqplot( ~ Survival | Light ,    # ~ x | gfactor\n       data= survdf , \n       ylab=\"Sample quantiles of survival percentage by light intensity\" ) \n\n\n\n\n\n\nCode\n# Create a normal QQ-plot across all groups\nnormqqplot( ~ Survival,             # ~ x  \n      data= survdf , \n      ylab=\"Sample quantiles of survival percentage\" )\n\n\n\n\n\n\nCode\n# Load the mosaic package \nlibrary( mosaic ) # provides formula expression in sd()\n\n# Calculate the mean \"Survival\" for each \"Light\" level\nmean( Survival ~ Light , \n       data= survdf )\n#>     high      low      med \n#> 58.18500 59.60625 75.51750\n\n# Calculate the standard deviation \"Survival\" for each \"Light\" level\nsd( Survival ~ Light , \n    data= survdf )\n#>      high       low       med \n#>  6.345415 12.809045 12.242073\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference in samples means under intensity level 6.75 differs from that intensity level 4.5 and 9. We use an ANOVA F-test to determine if at least one of the true means significantly differs from the rest. However, first the assumptions about the data are considered. While samples are not random, there are no obvious reasons why independence would not hold for these observations in that that survival rates under a given intensity should no have no connection to the survival rate under any other intensity level. The summary() output shows that the sample for each group is small. It can be difficult to assess the normality assumptions with such few observations, especially under light intensity 6.75. When considering the data across all groups, the QQ-plot suggests the data show some skewness. The researcher should either feel comfortable assuming normality based on prior experience or a literature review. Otherwise, it is difficult to say if this assumption about the data are reasonable for small samples.\nThe boxplots suggest that the variability is not the same for the three groups. To further assess this assumption numerically, we use the sample standard deviations. The sample standard deviations under light intensity 4.5 and 6.75 are similar (12.809 and 12.242), but the sample standard deviation under high light intensity is much less. To determine if the variability in both groups is similar enough, we can use a rule of thumb - take the ratio of the largest and smallest sample standard deviations (12.809/6.345 or 6.345/12.809), and if the ratio is between .5 and 2, then the variability in both groups is considered similar enough. Both graphical and numerical assessments suggest that assumption 4 is not reasonable for the data. However, keep in mind that graphical assessments of assumptions are generally enough, and this rule of thumb should not be used if outliers are present.\nOverall, not all data assumptions are met. Please note that possible remedial measures can be applied when one or more assumptions are not met, and these measures are discussed in Section 14.4 in the context of linear regression and they also apply to the ANOVA setting. Alternative methods to be discussed would be more appropriate for this data, but for illustration, we analyze the data using the F-test in single factor ANOVA. Let \\(\\mu_1\\) denote the true mean survival rate under low light intensity. \\(\\mu_2\\) and \\(\\mu_3\\) are defined similarly for light intensities medium and high, respectively. Since the aim is to determine whether light intensity has a significant effect on the survival of the larvae, the hypothesis is\n\\[H_0: \\mu_1=\\mu_2=\\mu_3 \\qquad H_a: \\text{Not all } μ_i\\text{'s are equal}\\]\nIn the context of ANOVA, we may also define the hypothesis in terms of an effect. Let \\(\\tau_i= \\mu - \\mu_i\\), where \\(\\tau_i\\) measures the effect of the \\(i\\)th factor level. The \\(i\\)th effect is the difference between the overall mean and the mean of \\(i\\)th factor level. In terms of effects, the hypothesis is\n\\[H_0: \\tau_1=\\tau_2=\\tau_3=0 \\qquad H_a: \\text{Not all }\\tau_i\\text{'s are 0}\\]\nThe following code uses sfaov() to compute the test statistic and the corresponding p-value for an F-test in single factor ANOVA:\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's R's memory.\nsource(\"rfuns/sfaov.R\")\n\n# The function will require the following information:\n# y: Replace with 'Survival'.\n# x: Replace with 'Light'.\n# data: set equal to a 'survdf'.\n# welch: default is FALSE so it does not \n#    need to be provided.\nsfaov( Survival ~ Light , \n       data= survdf )\n#> One-way analysis of means (assuming equal variances) \n#>  \n#> data:  Survival ~ Light \n#> ============== \n#> ANOVA Table \n#>           Df Sum Sq Mean Sq F value p-value\n#> Treatment  2  892.2  446.10   4.034   0.037\n#> Error     17 1880.0  110.59                \n#> ============== \n#> \n#> R-squared=  0.475\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe R output provides part of what is called an ANOVA table. An ANOVA table is a table that summarizes the results of an ANOVA test. It displays the sources of variation (Treatment [between groups], Error [within groups]), degrees of freedom, sums of squares, mean squares, F-test test statistic, and p-value4. Below is a general form of ANOVA table.\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares (SS)\nDegrees of Freedom (df)\nMean Square (MS)\nF-test Statistic\np-value\n\n\n\nTreatment (Between)\nSS(Treatment)\ndf(Treatment)\nMS(Treatment)\nF\np-value\n\n\nError (Within)\nSS(Error)\ndf(Error)\nMS(Error)\n\n\n\n\nTotal\nSS(Total)\ndf(Total)\n\n\n\n\n\n\nThe output also provides the coefficient of determination or \\(R^2\\). \\(R^2\\) is a number between 0 and 1, and it measures the proportion of variability in the response that can be explained or accounted for by the factor variable, and it is defined as SS(Treatment)/SS(Total). The higher \\(R^2\\), the larger the proportion of the total variation that is accounted for by the factor variable. However, either a high or low value of \\(R^2\\) does not necessarily imply statistical significance.\nBased on the output, the observed value of the test statistic is \\(F=4.034\\). The p-value is \\(.037\\). Since p-value\\(\\leq\\alpha=.10\\), we have significant evidence to reject \\(H_0\\) at \\(\\alpha=.10\\). Based on the size of the p-value, we may also say that there is strong evidence to reject \\(H_0\\). Note that by setting \\(\\alpha=.10\\), it is conveyed to the reader that at most “some evidence” is needed to reject \\(H_0\\) and support what is reflected in \\(H_a\\). Overall, at \\(\\alpha=.10\\), light intensity has a significant effect on the survival of the larvae. Although the data provides some evidence that light intensity affects survival rates, we can not say if changes in light intensity causes changes in survival rates because the treatments (low, medium, and high light intensities) were not randomly assigned. Further, because the subjects were not randomly selected from some larger population, the findings that survival rates are affected by the strength of light intensity only applies to those subjects in the sample and not some larger population.\nRecall that not all data assumptions are met (assumptions 3 and 4 are questionable). Welch’s F-test does not require assumption 4. We will assume assumption 3 is met for the data to illustrate Welch’s F-test. To carry out Welch’s F-test, we use sfaov() but with an additional argument of welch=TRUE:\n\n\nR code\nVideo\n\n\n\n\nCode \n# The function will require the following information:\n# y: Replace with 'Survival'.\n# x: Replace with 'Light'.\n# data: Set equal to a 'survdf'.\n# welch: Set equal to TRUE.\nsfaov( Survival ~ Light , \n       data= survdf, \n       welch = TRUE )\n#> One-way analysis of means (not assuming equal variances) \n#>  \n#> data:  Survival ~ Light \n#> ============== \n#> ANOVA Table \n#>              Df Sum Sq Mean Sq F value p-value\n#> Treatment 2.000  892.2  446.10   3.236   0.101\n#> Error     7.037 1880.0  110.59                \n#> ============== \n#> \n#> R-squared=  0.475 \n#> \n#> Note: The Df for the Error component, F value and p-value provided are for Welch's F-test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sfaov() output is nearly identical when using sfaov() to conduct the standard F-test but with the F value, p-value, and df(Treatment) corresponding to Welch’s F-test. The rest of the information is defined as before. Note that \\(R^2\\) still has the same meaning, as do MS(Treatment) and MS(Error), and their computations are independent of Welch’s F-test.\nNote that Welch’s F-test provides \\(F_w=4.034\\), with a corresponding p-value of \\(0.101\\). Since p-value\\(>\\alpha=.05\\), we do not have significant evidence to reject \\(H_0\\) at \\(\\alpha=.05\\). The size of the p-value reflects that that there is very little evidence to reject \\(H_0\\). Overall, at \\(\\alpha=.05\\), Welch’s F-test shows that there is not enough evidence to conclude that light intensity has a significant effect on the survival of the larvae."
  },
  {
    "objectID": "anova.html#kruskalwallis-test-single-factor-anova-for-ranks",
    "href": "anova.html#kruskalwallis-test-single-factor-anova-for-ranks",
    "title": "13  Analysis of variance (ANOVA)",
    "section": "\n13.2 Kruskal–Wallis test (single-factor ANOVA for ranks)",
    "text": "13.2 Kruskal–Wallis test (single-factor ANOVA for ranks)\nKruskal–Wallis test is sometimes referred to as a single-factor ANOVA for ranks. The Kruskal-Wallis test is similar to a single factor ANOVA test, but does not make assumptions about the normality and is useful when the data are ordinal (e.g. Likert item data). However, it does require that the distribution of the data under each factor level or group be roughly the same shape. It is a non-parametric test alternative to the F-test that tests whether there are significant differences between the medians of two or more treatments or groups.\nLike ANOVA, it partitions the total variability in the data but when using the ranks of the data (not the data themselves). For the Kruskal–Wallis test, the hypotheses are\n\\(H_0: \\eta_1= \\cdots=\\eta_k\\) (The median response is the same across all factor levels or treatments)5\n\\(H_a:\\) At least one treatment medians (\\(\\eta_1\\), \\(\\eta_2\\), …, or \\(\\eta_k\\)) is different from the rest (The median response is the not same across all factor levels or treatments)\nThe Kruskal–Wallis test statistic is:\n\\[H=\\frac{MSranks(Treatment)\\times (k-1)}{ (SSranks(Total))/(n-1)}\\]\nWhere MSranks(Treatment) denotes the mean of square treatment of the ranked data, and SSranks(Total) is the total sum of squares of the ranked data. Under \\(H_0\\), the null distribution of \\(H\\) is a \\(\\chi^2\\)-distribution degree of freedom \\(k-1\\), and the null distribution is used to compute the p-value.\nThe function sfkw()6 obtains the information needed to conduct the Kruskal–Wallis test using the data.\n\n\n\n\n\n\nR functions\n\n\n\n### sfkw( y ~ x , data )\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere, we apply the Kruskal–Wallis test to the data described in the case study provided in Chapter 1 to determine if light intensities have a significant effect on survival rates of early-state Delta smelt larvae. Recall that Light and Survival are the variables of interest, and that there are three levels of light intensity: 4.5 (low), 6.75 (medium), and 9 (high). Our aim here is to determine whether light intensity has a significant effect on the survival of the larvae at \\(\\alpha=.10\\) using the Kruskal–Wallis test.\nThe data was imported and data assumptions were assessed when the F-test was applied in the previous section. Below we examine the medians and further explore the distribution of the data.\n\nCodelibrary(lattice) # provides the bwplot() function\n\n# Create violin boxplot\nbwplot( Survival ~ Light , \n     data= survdf , \n     xlab= \"Light intensity\", \n     ylab= \"Survival\" ,\n     main= \"Survival vs Light intensity\" , \n     panel= panel.violin, \n     ylim=c(30, 120) )\n\n\n\n\n\n\nCode\n\nlibrary( mosaic ) # allows formula expression in median and iqr\n\nmedian( Survival ~ Light , \n        data= survdf )\n#>   high    low    med \n#> 58.720 56.420 75.525\n\niqr( Survival ~ Light , \n     data= survdf )\n#>    high     low     med \n#>  6.2400  9.7375 16.3225\n\n\nThe difference in samples medians under intensity level 6.75 differs from intensity levels 4.5 and 9. The violin plots suggest that the data under each treatment do not all follow the same distribution. This means we can not assess if the medians under each treatment differ but we formulate the hypothesis in terms of the mean of the ranks:\n\\(H_0:\\) The mean ranks of survival under each light intensity are the same\n\\(H_a:\\) Not all the mean ranks of survival under each light intensity are the same\nThe following code uses sfkw() to compute the test statistic and the corresponding p-value for the Kruskal–Wallis test:\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's R's memory.\nsource(\"rfuns/sfkw.R\")\n\n# The function will require the following information:\n# y: Replace with 'Survival'\n# x: Replace with 'Light'\n# data: Set equal to a 'survdf'\nsfkw( Survival ~ Light , \n      data= survdf )\n#> Kruskal-Wallis test (single-factor ANOVA on ranks) \n#>  \n#> data:  Survival ~ Light \n#> ============== \n#> Kruskal-Wallis test statistic=  5.668 \n#> p-value=  0.059 \n#> Null distribition is chi-squared with df=  2 \n#> ============== \n#>  \n#> Eta-squared=  0.2157563\n\n\n\n\n\n\n\n\n\n\n\n\nThe output provides the Kruskal-Wallis test statistic and corresponding p-value. The output also provides a measure akin coefficient of determination in traditional ANOVA called rank Eta or eta-squared. The measure eta-squared is appropriate for when the data are ranks, and it provides the proportion of the total variation in the response variable that is explained by the explanatory variable. Again, Eta-squared can be considered a non-parametric analogue to \\(R^2\\).\nBased on the output, the observed value of the test statistic is \\(H=5.668\\). The p-value is \\(0.059\\). Since p-value\\(\\leq\\alpha=.10\\), we have have significant evidence to reject \\(H_0\\) at \\(\\alpha=.10\\). Note that the size of the p-value suggests there is some moderate evidence to reject \\(H_0\\). Overall, at \\(\\alpha=.10\\), light intensity has a significant effect on the survival of the larvae (in terms of mean ranks of survival rates)."
  },
  {
    "objectID": "anova.html#multiple-comparisons",
    "href": "anova.html#multiple-comparisons",
    "title": "13  Analysis of variance (ANOVA)",
    "section": "\n13.3 Multiple comparisons",
    "text": "13.3 Multiple comparisons\nIf \\(H_0\\) is rejected, it implies significant differences between the treatments or groups. However, this result does not indicate which specific groups differ from each other. Multiple comparisons refer to the process of making all pairwise comparisons between two or more groups when there are significant differences between the groups. These comparisons can be accomplished using a two-sample methods on all possible pairs. The scenario of testing many pairs of groups is called multiple comparisons.\nThere are a few problems associated with blindly carrying out two-sample methods on all possible pairwise comparisons\n\nThe comparisons are not being done simultaneously and are being carried out individually with no regard to the other tests (comparisons).\nThe more pairwise comparisons conducted, the higher the chances of making at least one type I error among all the comparisons.\n\nMultiple comparisons in ANOVA are used to determine which specific groups or treatments are significantly different from each other. These procedures adjust the p-value and confidence level of each comparison so that the family-wise error rate (the probability of making at least one Type I error across all multiple comparisons), is at most \\(\\alpha\\). We discuss a few multiple comparison methods that control the family wise error rate (FWER).\n\n13.3.1 Tukey’s HSD method\nIn ANOVA, the most common method for multiple comparisons is called Tukey’s Honest Significant Difference (HSD) method. Tukey’s HSD method is used in single-factor ANOVA, and it adjusts the p-value or confidence level for each pairwise comparison to control the FWER for all possible pairwise comparisons, rather than for individual comparisons.\nRecall that the function sfaov() can be used to conduct single-factor ANOVA. This function can also then conduct Tukey’s HSD method to adjust the p-values or confidence intervals for all pairwise comparisons among group or treatment means.\n\n\n\n\n\n\nR functions\n\n\n\n### sfaov( y ~ x , data , PWC, conf.level )\n# PWC: Set equal to true to conduct all pairwise comparisions using\n#   Tukey's HSD method. Default is FALSE \n# conf.level: Set equal to desired confidence level for all pairwise comparision. Default is .95\n# \n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that we applied the ANOVA F-test to the data described in Chapter 1 and determined that light intensities have a significant effect on survival rates of early-stage Delta smelt larvae at \\(\\alpha=.10\\). Thus, the mean survival rates under each intensity are not all the same, but it is not clear how they differ. To determine how they differ, we apply Tukey’s HSD method at \\(\\alpha=.10\\)\nThe following code uses sfaov() to help conduct the ANOVA F-test and pairwise multiple comparisons using Tukey’s HSD method at \\(\\alpha=.10\\)7.\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's R's memory.\nsource(\"rfuns/sfaov.R\")\n\n# The function will require the following information:\n# y: Replace with 'Survival'.\n# x: Replace with 'Light'.\n# data: Set equal to a 'survdf'.\n# PWC: Set equal to TRUE.\n# conf.level: Set equal to .90.\nsfaov( Survival ~ Light , \n    data= survdf , \n    PWC= TRUE , \n    conf.level=.90 )\n#> One-way analysis of means (assuming equal variances) \n#>  \n#> data:  Survival ~ Light \n#> ============== \n#> ANOVA Table \n#>           Df Sum Sq Mean Sq F value p-value\n#> Treatment  2  892.2  446.10   4.034   0.037\n#> Error     17 1880.0  110.59                \n#> ============== \n#> \n#> R-squared=  0.475 \n#> \n#> \n#> Sample group means: high-58.18  low-59.61  med-75.52  \n#> \n#> Tukey multiple comparisons of means \n#>              diff        lwr      upr      p adj\n#> low-high  1.42125 -10.142221 12.98472 0.96061638\n#> med-high 17.33250   3.170198 31.49480 0.03898149\n#> med-low  15.91125   1.748948 30.07355 0.06006956\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output from sfaov() now provides the results of Tukey’s HSD method which displays the difference in the sample treatment means, confidence interval, and p-value for each pairwise comparison. Note that the first confidence interval compares low to medium light intensity (CI for \\(\\mu_1-\\mu_2\\) and p-value for testing \\(\\mu_1-\\mu_2\\neq 0\\)). The comparisons that follow are for \\(\\mu_1-\\mu_3\\) and \\(\\mu_2-\\mu_3\\). Tukey’s HSD pairwise comparisons reveal significant differences in mean survival rates between different levels of light intensity at FWER of \\(.10\\). Specifically, the mean survival rate under medium light intensity is significantly different from both low and high light intensity (in terms of pairwise comparisons). Additionally, the mean survival rates between low and high light intensity were not significantly different from each other.\n\n\n\n13.3.2 Games-Howell test\nThe Games-Howell test is a method for performing pairwise multiple comparisons when using Welch’s F-test. This test is somewhat similar to Tukey’s HSD method. Recall that the function sfaov() can be used to conduct Welch’s F-test. This function can also then conduct the Games-Howell test to adjust the p-values or confidence intervals for all pairwise comparisons among group or treatment means.\n\n\n\n\n\n\nR functions\n\n\n\n### sfaov( y ~ x , data , PWC, conf.level, welch )\n# PWC: Set equal to true to conduct all pairwise comparisions using\n#      Tukey's HSD method. Default is FALSE.\n# conf.level: Set equal to desired confidence level for all pairwise comparision. Default is .95.\n# welch: Set equal to TRUE for Welch's F-test.\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that we applied the Welch’s F-test to the data described in Chapter 1 and it was determined that light intensities did not have a significant effect on survival rates of early-stage Delta smelt larvae at \\(\\alpha=.10\\). Thus, the data did not provide enough evidence to conclude that survival rates under each intensity are different. However, to illustrate the procedure in R, we perform multiple pairwise comparisons using the Games-Howell test at \\(\\alpha=.10\\).\nThe following code uses sfaov() to conduct the Welch’s F-test and all pairwise multiple comparisons using the Games-Howell test at \\(\\alpha=.10\\):\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's in R's memory.\nsource(\"rfuns/sfaov.R\")\n\n# The function will require the following information:\n# y: Replace with 'Survival'.\n# x: Replace with 'Light'.\n# data: Set equal to a 'survdf'.\n# PWC: Set equal to TRUE.\n# conf.level: Set equal to .90.\n# welch: Set equal to TRUE.\nsfaov( Survival ~ Light , \n       data= survdf , \n       PWC= TRUE, \n       conf.level= .90, \n       welch= TRUE )\n#> One-way analysis of means (not assuming equal variances) \n#>  \n#> data:  Survival ~ Light \n#> ============== \n#> ANOVA Table \n#>              Df Sum Sq Mean Sq F value p-value\n#> Treatment 2.000  892.2  446.10   3.236   0.101\n#> Error     7.037 1880.0  110.59                \n#> ============== \n#> \n#> R-squared=  0.475 \n#> \n#> Note: The Df for the Error component, F value and p-value provided are for Welch's F-test \n#> \n#> Sample group means: high-58.18  low-59.61  med-75.52  \n#> \n#> Games-Howell multiple comparisons of means \n#>       groups    diff    lwr     upr  p adj\n#> 1 low : high   1.421 15.222 -12.380  0.958\n#> 2  low : med -15.911  7.067 -38.889  0.168\n#> 3 high : med -17.332  6.385 -41.050  0.120\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe R output from sfaov() now provides the results of the Games-Howell test and it displays the difference in the sample treatment means, confidence interval, and p-value for each pairwise comparison. Note that the first confidence interval compares low to high light intensity (CI for \\(\\mu_1-\\mu_3\\) and p-value for testing \\(\\mu_1-\\mu_3\\neq 0\\)). The comparisons that follow are for \\(\\mu_1-\\mu_2\\) and \\(\\mu_3-\\mu_2\\). The Games-Howell test for pairwise comparisons reveal that there are no significant differences in mean survival rates between different levels of light intensity at FWER=\\(.10\\).\n\n\n\n13.3.3 Dunn test\nA common multiple pairwise comparisons method to compliment the Kruskal-Wallis test is Dunn’s test with Holm’s correction. Holm’s correction is what is called a step-down method that adjusts the p-values all pairwise comparisons among group or treatment means to account for multiple testing. Recall that the function sfkw() can be used to conduct the Kruskal–Wallis test. This function can also then conduct the Dunn’s test with Holm’s correction to adjust the p-values for all pairwise comparisons among group or treatment means8.\n\n\n\n\n\n\nR functions\n\n\n\n### sfkw( y ~ x , data , PWC )\n# PWC: Set equal to true to conduct all pairwise comparisions using\n#      Tukey's HSD method. Default is FALSE \n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that we applied the Kruskal-Wallis test to the data described in Chapter 1 and determined that light intensities did have an significant effect on survival rates of early-stage Delta smelt larvae at \\(\\alpha=.10\\). The following code uses sfkw() to help conduct the Kruskal-Wallis test and all pairwise multiple comparisons using Dunn’s method with Holm’s correction at \\(\\alpha=.10\\):\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's in R's memory.\nsource(\"rfuns/sfkw.R\")\n\n# The function will require the following information:\n# y: replace with 'Survival'\n# x: replace with 'Light'\n# data: set equal to a 'survdf'\n# PWC: set equal to TRUE\nsfkw( Survival ~ Light , \n      data= survdf , \n      PWC= TRUE )\n#> Kruskal-Wallis test (single-factor ANOVA on ranks) \n#>  \n#> data:  Survival ~ Light \n#> ============== \n#> Kruskal-Wallis test statistic=  5.668 \n#> p-value=  0.059 \n#> Null distribition is chi-squared with df=  2 \n#> ============== \n#>  \n#> Eta-squared=  0.2157563 \n#> \n#> Sample group medians: high-58.72  low-56.42  med-75.525  \n#> \n#>   Comparison         Z    P.unadj      P.adj\n#> 1 high - low  0.295804 0.76737978 0.76737978\n#> 2 high - med -2.035693 0.04178115 0.08356230\n#> 3  low - med -2.277216 0.02277331 0.06831993\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output from sfkw() now provides the results of Dunn’s test, displaying adjusted p-values (P.adj) for each comparison. Note that the first comparison is comparing low to medium light intensity with the last comparison being between medium and high light intensity. Dunn’s test for pairwise comparisons reveals significant differences in survival rates between different levels of light intensity at FWER of \\(.10\\). Specifically, the survival rate under medium light intensity is significantly different from both low and high light intensity (in terms of pairwise comparisons). Additionally, survival rates between low and high light intensity were not significantly different from each other. Note that the comparisons are being done in terms of mean ranks since the data distribution under each treatment did not follow the same shape."
  },
  {
    "objectID": "anova.html#sample-size-estimation-and-power-analysis-for-anova",
    "href": "anova.html#sample-size-estimation-and-power-analysis-for-anova",
    "title": "13  Analysis of variance (ANOVA)",
    "section": "\n13.4 Sample size estimation and power analysis for ANOVA",
    "text": "13.4 Sample size estimation and power analysis for ANOVA\nIdeally, estimating the sample size for a study is one of the first things that researchers do prior to collecting data. Knowing the required sample size to detect a desired effect at the beginning of a project allows one to manage their data collection efforts. Furthermore, this allows for one to determine how much statistical power the test will have to detect an effect if it exist.\nThe R function pwr.anova.test() from the pwr R package calculates statistical power for a balanced single-factor ANOVA, given the sample size, significant level (\\(\\alpha\\)), and effect size. Note that all treatments are assumed to have the same number of observations, which is known as a “balanced” design. It is assumed that the appropriate assumptions about the data are met9.\n\n\n\n\n\n\nR functions\n\n\n\n### pwr.anova.test( n , delta , sd1 , sd2 , sig.level , \n###           power, alternative)\n# k: Set equal to the number of groups/treatments.\n# n: Set equal to the number of observations per group.\n# f: Set equal to the effect size.\n# sig.level: Set equal to the desired alpha value (significance level).\n# power: Set equal to the desired power (a number between 0 and 1). \n#\n\n\nThe effect size refers to Cohen’s \\(f\\), which is defined as the standard deviation of the k treatment means divided by standard deviation (across all groups). Cohen’s f is typically interpreted as follows:\n\nSmall effect size: \\(f = 0.10\\)\nMedium effect size: \\(f = 0.25\\)\nLarge effect size: \\(f = 0.40\\) or higher.\n\nThese are suggested guidelines and may vary slightly depending on the specific field of research or context of the study.\n\n\n\n\n\n\nNote\n\n\n\nSuppose we want to determine the statistical power to detect a medium-sized effect assuming five observations per group and at significance level of \\(\\alpha=.01\\) and \\(\\alpha=.10\\). Recall that there are three groups:\n\nCode# Load the \"pwr\" package\nlibrary(pwr) # Provides pwr.anova.test\n\npwr.anova.test(k = 3 , \n               n = 5 , \n               f = .25 , \n               sig.level = .01 ) \n#> \n#>      Balanced one-way analysis of variance power calculation \n#> \n#>               k = 3\n#>               n = 5\n#>               f = 0.25\n#>       sig.level = 0.01\n#>           power = 0.02811243\n#> \n#> NOTE: n is number in each group\n\n\nThe statistical power of this test is low. To increase the power, one may increase the effect size (the larger it is, the easier it is to detect), increase \\(\\alpha\\) (make it easier to reject \\(H_0\\) and find a significant effect), and/or increase the number of observations per group."
  },
  {
    "objectID": "lr.html",
    "href": "lr.html",
    "title": "14  Simple linear regression",
    "section": "",
    "text": "AI art generated by keywords “linear regression”\nSection 10.2 addresses how to investigate the relationship between two numerical variables by looking at a scatterplot. Simple linear regression (SLR) is a statistical method used to model the linear relationship between two numerical variables, where one variable is the response variable and the other is the explanatory variable. SLR is a special case of linear regression, which models the relationship between two or more variables by fitting a linear equation to the observed data. This section focuses on SLR, which involves only one explanatory variable.\nThe SLR model assumes that the value of \\(y\\) is linearly related to \\(x\\) via the line \\(\\beta_0 + \\beta_1 x\\). However, the observed \\(y\\) values deviate randomly from this line, as quantified by an error term. On average, the value of \\(y\\) for a given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\). The random error term signifies that other factors, such as measurement errors, unmeasured variables, or inherent variability in the data, may influence the response variable and are not captured or explained by the estimated SLR line\nNote that the line shown in the above scatterplot is the estimated SLR line denoted by \\(\\hat{y} = b_0 + b_1 x\\).\nDue to the random error in the relationship between the explanatory and response variables, the estimated line may not pass through every data point. The estimates of \\(b_0\\) and \\(b_1\\) are obtained using the method of least squares, which minimizes the sum of squared differences between the observed values of \\(y\\) and predicted values \\(\\hat{y}\\).\nNow that some basic of SLR have been covered, this section illustrates common key aspects of SLR, including:\nFor inferential methods to provide meaningful results, several assumptions are made about the data on which the method is intended to be used."
  },
  {
    "objectID": "lr.html#fitting-the-slr-model-and-model-diagnostics.",
    "href": "lr.html#fitting-the-slr-model-and-model-diagnostics.",
    "title": "14  Simple linear regression",
    "section": "\n14.1 Fitting the SLR model and model diagnostics.",
    "text": "14.1 Fitting the SLR model and model diagnostics.\nThe function lm() is used to fit the SLR model to the data.\n\n\n\n\n\n\nR functions\n\n\n\n### lm( y ~ x , data )\n# y: replace y with the name of the resposne (measured) \n#  variable of interest\n# x: replace x with the name of the explantory variable\n# data: set equal to a dataframe that is in tidy form\n#\n\n\nNote that the R functions used in these modules often have identical arguments, which will no longer be described in this module. The lm() function provides more than just the values of \\(b_0\\) and \\(b_1\\), and the illustrations below show how to use it to create an lm object that contains useful information. To summarize the SLR model fit, extract fitted values, or extract residuals from the lm object, we use the summary(), predict(), and residuals() functions on the lm object, respectively. The function xyplot() can be used to plot the data along with the estimated SLR line by specifying an argument called type:\n\n\n\n\n\n\nR functions\n\n\n\n### summary( lm object )\n# lm object: An object saving the result of usuing \n#      the lm function.\n#\n### predict( lm object )\n#\n### residuals( lm object )\n#\n### xyplot( y ~ x, data, type=c(\"p\", \"r\") )\n#\n\n\nThe summary() function will provide a lot of information, including the values of \\(b_0\\) and \\(b_1\\).\nTo perform model diagnostics (process of of assess the validity of the assumptions of data) with will use xyplot() in a different way. The R package tactile expands the functionality of xyplot() so that it creates a set of residual plot by specifying only an lm object. Specifically, for a SLR model fit, it will provide the following residual plots:\n\nResiduals vs. predicted values: This residual plot shows the relationship between the residuals and the predicted values. The plot helps to assess assumptions 1, 2, and 4. A plot that shows no clear pattern or trend, no outlying observations, and a spread that is fairly constant suggest that these assumptions are reasonable. If there is a pattern or structure in the residual plot, this suggests that there may be dependence between the observations. Similarly, if spread of residuals change as a function of predicted values, it suggests non-constant variance.\nNormal QQ-plot of the residuals: The plot helps to assess assumption 3. If the residuals are normally distributed, the points on the plot will roughly follow a straight line.\nStandardized residuals vs. predicted values. This residual plot is generally referred to as a scale-location plot. This plots the square root of the standardized residual versus the predicted values. Note that standardization has these transformed residuals to have a mean of zero and a standard deviation of one. This allows for comparisons between models with different explanatory variables or dependent variables on different scales. It is analyzed and interpreted in the same manner as the residuals vs. predicted values plot.\nResiduals vs. leverage plot. This plot shows the leverage (the degree to which an observation affects the estimated slope of the fitted regression line) against the standardized residuals. Although it helps identify potential influential outliers (such outliers have high leverage and correspond to a large residual), a plot that shows no clear pattern or trend and and a spread that is fairly constant suggest that will suggest assumptions 1 and 4 are reasonable. If the spread of standardized residuals changes as a function of leverage, it indicates non-constant variance. Similarly, a trend or pattern as a function of leverage would indicate a violation of assumption 1.\n\n\n\n\n\n\n\nNote\n\n\n\nWe fit the SLR model to the data described in the case study provided in Chapter 3 to model the linear relationship between Pm10 and temperature. The data provided in the case study consists of several variables (PM10, AQI, …, county, and City.Name) with PM10 and temp being the variables of interest.\nTo begin, we import the data, and then one would explore the data using numerical and graphical data summaries. Numerical and graphical data exploration for bivariate data are covered in other modules but we start by assessing the linearity of the relationship.\n\n\nR code\nVideo\n\n\n\n\nCode# Import data \nPM10df <- read.csv(\"datasets/DailyPM10.2021.csv\")\n\n# obtain the names of the variables\nnames( PM10df )\n#> [1] \"PM10\"      \"AQI\"       \"temp\"      \"Latitude\"  \"Longitude\" \"windspeed\"\n#> [7] \"state\"     \"county\"    \"City.Name\"\n\n\nlibrary(lattice) # provides the xyplot() function\nxyplot( PM10 ~ temp , \n    data= PM10df , \n    xlab= \"Temperature\" , \n    ylab= \"Particulate matter 10 micrometers or less (PM10)\" ,\n    main= \"PM10 vs Temperature\" ,\n    col=\"black\" ) # default color is \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot reflect a positive linear trend with some outlying PM10 measurements. To address the presence of outliers when conducting inference, we will consider some approaches in the last section. For now, we will proceed with fitting the SLR model in R for the purpose of illustration. This will involve using the lm() to fit the model and xyplot() to show the estimated SLR line:\n\n\nR code\nVideo\n\n\n\n\nCode# The function will require the following information:\n# y: replace with 'PM10'\n# x: replace with 'temp'\n# data: set equal to a 'PM10df'\nlm( PM10 ~ temp,\n  data= PM10df )\n#> \n#> Call:\n#> lm(formula = PM10 ~ temp, data = PM10df)\n#> \n#> Coefficients:\n#> (Intercept)         temp  \n#>    -12.7702       0.8053\n\n# xyplot now includes a new argument called type. \n# Set type=c(\"p\", \"r\"). This tells xyplot() to plot both the \n# points and the estimated line. A few additional\n# aesthetic arguments are provided below as well. \nxyplot( PM10 ~ temp,\n  data= PM10df,\n  type=c(\"p\", \"r\"), # ensures both data points and est. reg. line is plotted\n  col.line = \"red\", # make the line red \n  lwd= 2 ) # make the line wider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output provides \\(b_0=-12.77\\) and \\(b_1=0.8053\\). With \\(b_1=0.8053\\), it is estimated that PM10 levels (the response), on average, changes by \\(.8053\\) when temperature (the explanatory) is increased by 1 degree. The lm() function provides much more than just these values. To extract other information, the result from lm() must be stored. Below the resulting lm() fit is stored and then the summary() is applied to this object. The functions residuals() and predict() are also illustrated below.\n\n\nR code\nVideo\n\n\n\n\nCode# Store lm() fit in an object, call it PM10fit\nPM10fit <- lm( PM10 ~ temp,\n         data= PM10df )\n\n# PM10fit is called an lm object\nsummary( PM10fit )\n#> \n#> Call:\n#> lm(formula = PM10 ~ temp, data = PM10df)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -47.88 -16.94  -7.25   8.58 401.17 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -12.77025    2.67450  -4.775 1.89e-06 ***\n#> temp          0.80532    0.03899  20.655  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 30.77 on 2722 degrees of freedom\n#>   (391 observations deleted due to missingness)\n#> Multiple R-squared:  0.1355, Adjusted R-squared:  0.1352 \n#> F-statistic: 426.6 on 1 and 2722 DF,  p-value: < 2.2e-16\n\n# extract the residuals from PMfit\n# and store in an object called PM10res\nPM10res <- residuals( PM10fit )\n\n# extract the predicted values from PMfit\n# and store in an object called PM10predict\nPM10predict <- predict( PM10fit )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the summary() provides the following table:\n\n\n\nEstimate\nStd. Error\nt-value\np-value\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(T_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(T_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nThe terms \\(SE_{b_0}\\) and \\(SE_{b_1}\\) are the standard errors for \\(b_0\\) and \\(b_1\\). The table also provides the coefficient of determination or \\(R^2\\) (Multiple R-squared: 0.1355). The value of \\(R^2\\) is the sample correlation, \\(r\\), squared. In terms of its interpretation, \\(R^2\\), measures how well the estimated SLR line fits the data. A higher value of \\(R^2\\) indicates a better fit, meaning that more of the variance in the response variable can be explained by the explanatory variable. However, a high or low value of does not necessarily imply a significant linear relationship. Here, temperature explains \\(13.55\\)% of the total variation observed in PM10 measurements. In other words, the SLR model accounts for only a small fraction of the variability in PM10 measurements, and there may be other variables that are not included in the model that also affect PM10 levels. The rest of the information provided by summary() will be addressed as needed throughout the rest of these sections.\nThe next sections cover different inferential methods for a SLR, and they require for the assumptions of the data to be reasonable. Next, we perform model diagnostics to assess the model assumptions with the help of xyplot() after loading the tactile R package:\n\n\nR code\nVideo\n\n\n\n\nCode\n# This package provides diagnostic \n# plots using xyplot()\nlibrary( tactile ) \n\n# To create a set of pre-specified \n# residual plots, provide the lm object\n# to xyplot()\nxyplot( PM10fit )\n\n\n\n\n\nFigure 14.2: Diagnostic plots\n\n\n\n\n\n\n\n\nSee previous video.\n\n\n\nThe scatterplot showed a linear trend in the data, so assumption 5 (linearity) is reasonable. The residual plots provided by xyplot() (excluding the normal QQ-plot) will have a black (not necessarily linear) trend line that is lowess smoother3. Each plot is analyzed and interpreted below:\n\n\nThe upper and lower left plots display the residuals against the predicted value and the squared rooted standardized residuals against the predicted values, respectively. The plot shows the following characteristics:\n\nNo clear strong patterns or trends are evident in either plot so assumption 1 may be considered reasonable.\nThere are a few potential outliers and the variability of the residuals is not constant across the predicted values, so assumptions 2 and 4 are questionable.\n\n\nThe upper right plot is a normal QQ-plot of the residuals4 and it suggests that residuals are not approximately normality distributed but the sample size is large, so assumption 3 is met\nThe lower right plot shows the standardized residuals against the leverage. The presence of outliers, which are also visible in other residual plots, can be seen in this plot. Cook’s distance5 of 1/2 and 1 will appear as orange contour lines if any values values have a Cook’s distance exceeding these numbers, but no leverage values exceed a Cook’s distance of 1. The plot also shows that the variability in residuals decreases as leverage increases, suggesting that the non-constant assumption of the SLR model is violated.\n\nOverall, not all data assumptions are met for the SLR model. In such cases, we say the model did not pass the model diagnostics. Possible remedial measures will be discussed in the last section. For now, we will proceed with this model for the purpose of illustrating inferential methods, as if it were appropriate for the data."
  },
  {
    "objectID": "lr.html#testing-the-significance-of-the-linear-relationship-using-t-based-methods.",
    "href": "lr.html#testing-the-significance-of-the-linear-relationship-using-t-based-methods.",
    "title": "14  Simple linear regression",
    "section": "\n14.2 Testing the significance of the linear relationship using t-based methods.",
    "text": "14.2 Testing the significance of the linear relationship using t-based methods.\nThe focus in this section is on is on inference for \\(\\beta_1\\) (the true slope), where \\(\\beta_1\\) represents the change in the average value of \\(y\\) when \\(x\\) is increased by 1-unit. The t-test for the slope of a regression line is a widely used hypothesis test to assess the significance of a linear relationship between two variables. Specifically, test is be used to determine if the true slope \\(\\beta_1\\) is significantly different from zero, based on the observed value of \\(b_1\\). It is important to note that the observed value of \\(b_1\\) can vary. If a study were replicated, the value of \\(b_1\\) would likely be different. The use of inferential methods allows us to account for this variability and make statistically valid conclusions about the true slope \\(\\beta_1\\). A t-based confidence interval can be constructed for \\(\\beta_1\\) and it provides a range of plausible values for the true slope at a specified significance level. These methods are generally referred to as t-based methods for the slope.\nThe t-test for the slope of a regression line requires that certain assumptions (1, 2, 3, 4, and 56) about the data are reasonable. The test statistic for this t-test is given by:\n\\[T=\\frac{b_1-\\beta_1}{SE_{b_1}} \\]\nHere, \\(SE_{b_1}\\) represents the standard error of \\(b_1\\)7. Our primary interest is in determining if there is a significant linear relationship (association) between the explanatory variable and response variable, which translate to \\(\\beta_1\\neq 0\\). Consequently, the null and alternative hypotheses are:\n\\[H_0: \\beta_1=0 \\qquad H_a: \\beta_1 \\neq 0\\]\nNote that when two variables are linearly related or associated, it means that they are correlated, and when two variables are correlated and linearly related, it implies that they have a linear relationship or association. Under \\(H_0\\), the null distribution of the test statistic \\(T\\) follows a t-distribution with \\(n-2\\) degrees of freedom, where \\(n\\) is the sample size. This distribution is used to compute the p-value. The function lm() computes the test statistic and p-value for the t-test for a linear association in a SLR, and the results are provided by the summary() output. Recall that the summary() function provides a table with relevant information, including the test statistic and p-value for the t-test, as well as other statistics like the coefficient estimates and standard errors:\n\n\n\nEstimate\nStd. Error\nt-value\np-value\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(T_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(T_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nHere, \\(T_{b_1}\\) and p-value\\(_{b_1}\\) refer to the test statistic for the slope and two-sided p-value\\(_{b_1}\\), respectively for testing if \\(\\beta_1\\neq 0\\). The function lm() can also be utilized to obtain a confidence interval (CI) for the true slope \\(\\beta_1\\). To obtain a CI, apply the confint() function to the resulting lm object.\n\n\n\n\n\n\nR functions\n\n\n\n### confint( lm object , parm , level)\n# lm object: An object saving the result of usuing \n#      the lm function.\n# parm: Set equal to 2 \n# level: Set to desired confidence level.\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe apply an t-test to the data described in the case study from Chapter 3 to determine if there is a significant linear association between PM10 and temperature levels at \\(\\alpha=.05\\). Recall that not all the required assumptions are met, but we perform the test for the purpose of illustration. Let \\(\\beta_1\\) denote the true slope in the linear association between PM10 and temperature. Since the aim is to determine if there is a significant linear relationship or association, the hypothesis is\n\\[H_0: \\beta_1=0 \\qquad H_a: \\beta_1 \\neq 0\\]\nThe following code uses lm() and summary() to obtain compute the test statistic and the corresponding p-value for a t-test:\n\n\nR code\nVideo\n\n\n\n\nCode# PM10fit was created in the previous section\n# but the code is given here as reminder. \n# Below, the result from lm() is \n# stored in an object, called PM10fit\nPM10fit <- lm( PM10 ~ temp,\n         data= PM10df )\n\n# summarize the lm object, PM10fit. \nsummary( PM10fit )\n#> \n#> Call:\n#> lm(formula = PM10 ~ temp, data = PM10df)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -47.88 -16.94  -7.25   8.58 401.17 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -12.77025    2.67450  -4.775 1.89e-06 ***\n#> temp          0.80532    0.03899  20.655  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 30.77 on 2722 degrees of freedom\n#>   (391 observations deleted due to missingness)\n#> Multiple R-squared:  0.1355, Adjusted R-squared:  0.1352 \n#> F-statistic: 426.6 on 1 and 2722 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on output from summary( PM10fit ), the observed value of test statistic is \\(T=20.655\\). The corresponding p-value is approximately 0, indicating that the data provides very strong evidence against the null hypothesis. Since the p-value\\(\\leq \\alpha=.05\\), we have sufficient evidence to reject the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\) at significance level of \\(\\alpha=.05\\). This result suggests that changes in temperature are linearly associated with changes in PM10 levels, and that the relationship between these variables is not due to chance alone.\nThe function confint() is used to obtain a 95% CI for \\(\\beta1\\):\n\n\nR code\nVideo\n\n\n\n\nCode \n# summarize the lm object, PM10fit. \nconfint( PM10fit ,\n     parm= 2 , # Setting parm =1 will provide CI for beta0\n     level= .95 )\n#>          2.5 %    97.5 %\n#> temp 0.7288668 0.8817716\n\n\n\n\nSee previous video.\n\n\n\nThe output provides a 95% CI for \\(\\beta_1\\) with bounds \\((0.729, 0.882)\\). Thus, we can be 95% confident that on average, the PM10 levels will change, on average, anywhere between 0.729 and 0.882 units per 1-degree change in temperature. Since the interval does not contain 0, it further supports the conclusion that changes in PM10 levels are linearly associated with changes in temperature.\nThus, we are 95% confidence that on average, the response will change anywhere between .729 to .882 per unit increase in the explanatory variable. Since the interval does not contain 0, it is clear that changes in PM10 levels are linearly associated with changes in temperature. Note that this conclusion only applies to the monitoring sites where PM10 levels were measured, as these measurements were not taken at random locations. Thus, the observed relationship between PM10 levels and temperature is not generalizable to all locations or situations beyond the scope of the monitoring sites in this study."
  },
  {
    "objectID": "lr.html#testing-the-significance-of-the-linear-relationship-using-a-randomization-test",
    "href": "lr.html#testing-the-significance-of-the-linear-relationship-using-a-randomization-test",
    "title": "14  Simple linear regression",
    "section": "\n14.3 Testing the significance of the linear relationship using a randomization test",
    "text": "14.3 Testing the significance of the linear relationship using a randomization test\nRecall that \\(\\beta_1\\) represents the change in the average value of \\(y\\) when \\(x\\) is increased by 1-unit. If \\(\\beta_1=0\\), then there is no linear relationship or association between the response and explanatory variable. If \\(\\beta_1\\neq 0\\), then there is a linear associated or a correlation between the variables. If the variables are in fact linearly related (as illustrated in a scatterplot), it would not make sense to pair, for example, the 3rd measurement of the explanatory variable (circled in red) with a different measurement of the response variable. This is because the linear relationship between the variables implies that each value of the explanatory variable corresponds to a specific value of the response variable according to the linear pattern. Mixing up these pairings would distort the true linear relationship between the variables.\n\n\nR code\nVideo\n\n\n\n\n\n\n\n\nFigure 14.3: Example of a linear relationship with the estimated or best fit SLR line. The red circle denote the third observation in the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, if instead \\(\\beta_1=0\\) (indicating no linear association or correlation), then it wouldn’t matter how we matched up the 3rd measurement of the explanatory variable with some other measurement of the response variable. This is because, with no linear relationship between the variables, mixing up these pairings would not distort any relationship between them, as there is no linear relationship to be affected.\nThe aim is still to see if there is a significant linear relationship or association:\n\\[H_0: \\beta_1=0 \\qquad H_a: \\beta_1 \\neq 0\\]\nWith a randomization test, we assume that the null hypothesis (\\(H_0: \\beta_1 = 0\\)) is true, implying there is no linear relationship between the variables. If this is the case, we could just randomly shuffle the observed values of the response variable and it would not change any relationship between the response and the explanatory. Since the randomization (in this context it is the randomly shuffling the order of observations) of observed values of the response is permitted under \\(H_0\\), the size of observed value of \\(b_1\\) for this shuffled data is due to chance alone.\nAnother way to view it is that chance enters the analysis only through the random shuffling of the response values when the null hypothesis is assumed to be true and nothing else. The figure below shows the results of one such randomization. The resulting size of \\(b_1\\) under this randomization represents one observed value of \\(b_1\\) due to chance when the null hypothesis is assumed to be true.\n\n\n\n\n\nFigure 14.4: The resulting scatterplot after randomly shuffling the observe values of the response variable, which produced a slope estimate of -0.3754\n\n\n\n\n\nIf assumptions 3 or 4 do not hold, a randomization test is a good alternative to the t-test. To conduct this randomization test, we use the same test statistic as the t-test, namely\n\\[T=\\frac{b_1-\\beta_1}{SE_{b_1}} \\]\nTo obtain the null distribution of \\(T\\), the randomization process is repeated many times, each time computing the value of \\(T\\). We then obtain the null distribution of \\(T\\) by randomly shuffling (randomization) the observed values of the response variable many times, each time computing the value of the test statistic for the randomized data. The R function slr.randtest() computes the test statistic and p-value for the randomization test.\n\n\n\n\n\n\nR functions\n\n\n\n### slr.randtest( y ~ x, data, nshuffles, direction )\n# nshuffles: Set equal to the desired number of randomizations. \n# x: replace x with the name of the explantory variable\n# direction: the sign in the alternative, \"two.sided\", \"greater\" , or \"less\"\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe apply the randomization test to the data described in the case study from Chapter 3 to determine if there is a significant linear association between PM10 and temperature levels at \\(\\alpha=.05\\). Recall that after fitting the SLR to the data, it was found that no influential outliers were present. Thus, the necessary assumptions for this test (assumptions 1, 2, and 5) are reasonable for the data. The aim of the randomization test is to determine if a significant linear association exists:\n\\[H_0: \\beta_1=0 \\qquad H_a: \\beta_1 \\neq 0\\]\nThe following code uses slr.randtest to obtain compute the test statistic and the corresponding p-value for the randomization test:\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's R's memory.\nsource(\"rfuns/slr.randtest.R\")\n\n# The function will require the following information:\n# y: replace with 'PM10'\n# x: replace with 'temp'\n# data: set equal to a 'PM10df'\n# nshuffles: Set to 30000 randomization \n# direction: Set to \"two.sided\"\nslr.randtest( PM10 ~ temp ,\n       data= PM10df , \n       nshuffles=30000 , \n       direction = \"two.sided\" )\n# Randomization test for a linear association in SLR \n#\n#> data: PM10 ~ temp \n#> Obs. test statistic= 20.65466    \n#> Obs. slope estimate= 0.8053192    \n#> R-squared = 0.135493 \n#> p-value = 0 \n#> \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on output, the observed value of test statistic is \\(T=20.655\\) with a corresponding p-value is approximately 0, indicating that the data provides very strong evidence against the null hypothesis. Considering a significance level of \\(\\alpha = 0.05\\), since the p-value \\(\\leq \\alpha = 0.05\\), we have sufficient evidence to reject the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\). The randomization test suggests that changes in temperature are linearly associated with changes in PM10 levels, and that the relationship between these variables is not due to chance alone."
  },
  {
    "objectID": "lr.html#sec-remedmeasure",
    "href": "lr.html#sec-remedmeasure",
    "title": "14  Simple linear regression",
    "section": "\n14.4 Remedial measures",
    "text": "14.4 Remedial measures\nIf the model fails diagnostics, one possible remedial measure is to transform the response variable. This can be particularly helpful if only assumption 4 (equal variance or spread) is not met and/or if the small sampled data are skewed. A transformation is a rescaling of the data. Below are some standard transformations for the response variable that may help the data (on a transformed scale) better meet the assumptions8:\n\n\n\n\n\n\n\nTransformation\nFormula\nDescription\n\n\n\nSquare root\n\\(y' = \\sqrt{y}\\)\nSquare root of the response\n\n\nNatural log\n\\(y' = \\ln(y)\\)\nNatural log of the response\n\n\nSquare\n\\(y' = y^2\\)\nSquare of the response\n\n\nInverse\n\\(y' = \\frac{1}{y}\\)\nInverse of the response\n\n\n\nThe function mutate() from the dplyr package will be used to add transform variables to a dataframe.\n\n\n\n\n\n\nR functions\n\n\n\n### mutate( dataframe ,'new variable' = 'function of variable in data frame', ...)\n# dataframe: Replace with the name of the dataframe\n# 'new variable': Replace with the name of the new variable\n# 'function of variable in data frame': Replace with a function \n# of a variable pesent in the dataframe. \n# \n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that the residual plots suggested the variability of the residuals is not constant across the predicted values, which makes assumption 4 (equal variance or spread) questionable. To address this issue, we will apply a transformation to the PM10 levels using `mutate():\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function so that it's R's memory.\nlibrary(dplyr) # provides mutate()\n\n# Below we create a modified version of the PM10df,\n# called PM10mod, that contains the transformed response\nPM10dfmod <- mutate( PM10df,\n           sqrty = sqrt( PM10 ) , # square root transformed\n           invy = 1/PM10 ,    # inverse transformed\n           ysq =  PM10^2 )    # squared transformed\n# natural log and inverse considered due to 0's. \n\n# The dataframe contains the transformed variables\nnames( PM10dfmod )\n#>  [1] \"PM10\"      \"AQI\"       \"temp\"      \"Latitude\"  \"Longitude\" \"windspeed\"\n#>  [7] \"state\"     \"county\"    \"City.Name\" \"sqrty\"     \"invy\"      \"ysq\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, plot the transformed variables against the explanatory:\n\n\nR code\nVideo\n\n\n\n\nCodexyplot( PM10 ~ temp, \n    data= PM10dfmod, \n    xlab= \"Temperature\", \n    ylab= \"Original scale\") \n\nxyplot( sqrty ~ temp , \n    data= PM10dfmod , \n    xlab= \"Temperature\" , \n    ylab= \"Square root scale\" )\n\nxyplot( invy ~ temp , \n    data= PM10dfmod , \n    xlab= \"Temperature\" , \n    ylab= \"Inverse scale\" )\n\n\nxyplot( ysq ~ temp , \n    data= PM10dfmod , \n    xlab= \"Temperature\" , \n    ylab= \"Squared scale\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee previous video.\n\n\n\n\n\nThe square root transformation appears to reduce the impact of outliers and make the linear trend more evident. Next, the SLR model would be fitted using lm() with the formula sqrty ~ temp and data = PM10dfmod argument. After fitting the model, the residuals would be assessed to determine if the transformed data are appropriate for the model."
  },
  {
    "objectID": "lr.html#sample-size-estimation-and-power-analysis-for-slr",
    "href": "lr.html#sample-size-estimation-and-power-analysis-for-slr",
    "title": "14  Simple linear regression",
    "section": "\n14.5 Sample size estimation and power analysis for SLR",
    "text": "14.5 Sample size estimation and power analysis for SLR\nIdeally, estimating the sample size for a study is one of the first things that researchers do prior to collecting data. Knowing the sample size required to detect a desired effect at the beginning of a project allows one to manage their data collection efforts. Further, this allows for one to determine how much statistical power the test will have to detect an effect if it exist.\nThe R function pwr.f2.test() from the pwr R package provides statistical power calculations for a SLR model when the sample size, significant level (\\(\\alpha\\)), and effect size are provided. It is assumed the appropriate assumptions about the data for the t-test are met.\n\n\n\n\n\n\nR functions\n\n\n\n### pwr.f2.test( u, v, f2, sig.level , \n###           power)\n# u: Set equal 2.\n# v: Set equal to n-2, where n is the \n#   sample size.\n# f2: Set equal to the desired effect size\n# sig.level: Set equal to the desired alpha value (significance level)\n# power: Set equal to the desired power (a number between 0 and 1). \n#\n\n\n\n\n\\(f2\\) is a called Cohen’s \\(f2\\) and is typically interpreted as follows:\n\nSmall effect size: \\(f2 = 0.02\\)\nMedium effect size: \\(f2 = 0.15\\)\nLarge effect size: \\(f2 = 0.35\\) or higher.\n\nThese are suggested guidelines and may vary slightly depending on the specific field of research or context of the study.\n\n\n\n\n\n\nNote\n\n\n\nSuppose we want to determine the statistical power to detect a medium-sized effect assuming a sample size of 30 observations at significance level of \\(\\alpha=.001\\).\n\nCode\nlibrary(pwr) # provides power.f2.test\n\npwr.f2.test(u = 2 , \n      v = 28 , # 30 - 2 \n      f2 = .02 , \n      sig.level = .001 ) \n#> \n#>      Multiple regression power calculation \n#> \n#>               u = 2\n#>               v = 28\n#>              f2 = 0.02\n#>       sig.level = 0.001\n#>           power = 0.003225818\n\n\nThe statistical power of this test is rather low, although not surprising given the sample size, \\(\\alpha\\) specified, and size of the effect. To increase the power, on may increase the effect size (the larger it is, the easier it is to detect), increase \\(\\alpha\\) (make it easier to reject \\(H_0\\) and find a significant effect), and/or increase the number of observations per group.\nIf instead you sought to collect a sample that would provided a power of \\(.90\\) for detecting a small effect at \\(\\alpha=.001\\), we use the power argument but not specifying \\(v\\):\n\nCodepwr.f2.test(u = 2 , \n      power = .90 ,  \n      f2 = .02 , \n      sig.level = .001 ) \n#> \n#>      Multiple regression power calculation \n#> \n#>               u = 2\n#>               v = 1194.778\n#>              f2 = 0.02\n#>       sig.level = 0.001\n#>           power = 0.9\n\n\nThus, to ensure a power of at least .90, a sample of size 1195 should be collected."
  },
  {
    "objectID": "logisticreg.html",
    "href": "logisticreg.html",
    "title": "15  Simple logistic regression",
    "section": "",
    "text": "AI art generated from the text “simple logistic regression”\nSimple (binary) logistic regression (SLogiR) allows us to assess the relationship between a binary response variable (i.e., a categorical variable with two possible outcomes) and a numerical or categorical explanatory variable. Unlike simple linear regression, which examines the relationship between two numerical variables, simple logistic regression (SLogiR) is used when the response variable is binary. By modeling the probability of the response variable taking on a specific outcome, SLogiR can help identify the impact of the explanatory variable on the likelihood of observing one of the two possible outcomes.\nThis model captures the relationship between the explanatory variable \\(x\\) and the odds of success (i.e., the ratio of the probability that \\(y=1\\) to the probability of \\(y=0\\)), which is then transformed to the logit scale for modeling purposes. The logit scale provides a way to interpret the \\(\\beta_1\\). For example, \\(logit(0.5)=0\\) and \\(logit(0.6)=0.4\\) Thus, an increase of \\(0.4\\) on the logit scale corresponds to a change from \\(0.50\\) (or 50%) to \\(0.60\\) (or 60%) in the probability scale. In other words, a one-unit increase in \\(x\\) is associated with a \\(\\beta_1\\) increase in the logit of the odds of \\(y=1\\)\nAnother way to interpret \\(\\beta_1\\) is in terms of odds. The term \\(p/(1-p)\\) is the odds that \\(y=1\\) relative to \\(y=0\\). The above equation shows that the log odds, \\(log(p/(1-p))\\) are linearly related to \\(x\\). Suppose the log odds are computed at a value of \\(x\\), which we denote by \\(log(odds_1)\\). If we increase \\(x\\) by one unit and compute the log odds at the new value of \\(x\\), denoted by \\(log(odds_2)\\), the difference in the log odds becomes\n\\[log(odds_2)-log(odds_1)=log(odds_2/odds_1)=\\beta_1.\\]\nThis means that \\(\\frac{odds_2}{odds_1}=e^{\\beta_1}\\). In short, for each one-unit increase in \\(x\\), the odds of the binary response variable change multiplicatively by a factor of \\(e^{\\beta_1}\\). For example, if \\(\\frac{odds_2}{odds_1}=e^{\\beta_1}=1.20\\), the odds of the response variable taking the value of 1 increase by 20%. Conversely, if \\(\\frac{odds_2}{odds_1}=e^{\\beta_1}=0.75\\), the odds of the response variable taking the value of 1 decrease by 25%.\nEstimates of \\(\\beta_0\\) and \\(\\beta_1\\) are obtained via maximum likelihood estimation and are denoted by \\(b_0\\) and \\(b_1\\).\nThese estimates are used to obtain predicted values of the response variable provided by the SLogiR model for a given value of the explanatory variable. In other words, it represents the model’s best estimate of the response variable based a value of the of the explanatory variables. With a SLogiR, the predicted values can be given as an estimated probability of the response variable being 1 for a given value of \\(x\\). They can also be given on the log-odds scale.\nA residual is the difference between an observed value and its corresponding predicted value provided by the SLogiR model. In the context of a SLogiR model, residuals are typically more complex compared to a standard linear regression model because of the nature of the binary outcome and the use of the logistic function. Some common types of residuals used in logistic regression include deviance residuals, Pearson residuals, and their standardized versions1. Each of these residuals is calculated differently and serves specific purposes in model diagnostics and assessment.\nNow that some basic concepts of SLogiR have been covered, this section illustrates common key aspects of SLogiR, including:\nFor inferential methods in SLogiR to provide meaningful results, several assumptions are made about the data on which the method is intended to be used."
  },
  {
    "objectID": "logisticreg.html#fitting-the-slogir-model-and-model-diagnostics.",
    "href": "logisticreg.html#fitting-the-slogir-model-and-model-diagnostics.",
    "title": "15  Simple logistic regression",
    "section": "\n15.1 Fitting the SLogiR model and model diagnostics.",
    "text": "15.1 Fitting the SLogiR model and model diagnostics.\nThe function glm() is used to fit the SLogiR model to the data.\n\n\n\n\n\n\nR functions\n\n\n\n### glm( y ~ x , data , family )\n# y: Replace y with the name of the binary response \n#  variable of interest\n# x: Replace x with the name of the explanatory variable\n# data: Set equal to the name of the dataframe.\n# family: set equal to \"binomial\"\n#\n\n\nNote that the R functions used in these modules often have identical arguments, which will no longer be described in this module. The glm() function provides more than just the values of \\(b_0\\) and \\(b_1\\), and the illustrations below show how to use it to create an glm object that contains useful information. To summarize the SLR model fit, extract the predicted probabilities that \\(y=1\\) for a given value of \\(x\\), or extract the Pearson residuals from the glm object, we use the summary(), predict(), and residuals() functions on the glm object, respectively. The function xyplot() (after loading the tactile package) will also be used.\n\n\n\n\n\n\nR functions\n\n\n\n### summary( lm object )\n# lm object: An object saving the result of  \n#      an lm function.\n#\n### predict( lm object, type )\n# type: Set equal to \"link\" (prediction log odds scale) or \n#    \"response\" (predicted probabilites)\n#\n### residuals( lm object )\n# type: Set equal to \"deviance\" for deviance residuals,\n#    \"pearson\" for pearson residuals, \n#    \"partial\" for partial residuals\n#\n### xyplot( lm object )\n#\n\n\nThe summary() function will provide a lot of information, including the values of \\(b_0\\) and \\(b_1\\). To perform model diagnostics (process of assessing the validity of the assumptions about the data) the functions predict() and xplot() are used to create the following plots\n\nExplanatory variable vs. predicted log odds values: This residual plot shows the relationship between the explanatory variable and the predicted values on the log odds scale. The plot helps to assess assumption 4. A plot that shows a linear trend suggests that this assumption is reasonable. A clear non-linear trend implies that that assumption 4 is questionable.\nStandardized Pearson residuals vs. leverage plot: This plot shows the leverage (the degree to which an observation affects the estimated slope) against the standardized Pearson residuals. Potential influential outliers will have a high leverage and large residual in magnitude.\n\n\n\n\n\n\n\nNote\n\n\n\nAQI stands for Air Quality Index, which is a measure used to report the level of air pollution in a specific area at a given time. It takes into account the concentrations of several air pollutants (such as ground-level ozone, particulate matter, and others) and is generally reported on a scale from 0 to 500. The index is divided into different categories, each corresponding to a range of values, and each category has a color code to reflect the level of air pollution in a given area.\n\n\nFigure 15.1: Air Quality Index (AQI) levels of health concern. Source: www.airnow.gov\n\n\nAQI values and temperature can be related or associated in air pollution studies, but the nature of their relationship can vary depending on a number of factors. For example, high temperatures may increase the formation and concentration of certain air pollutants, such as ground-level ozone or particulate matter. However, AQI values are affected by a range of other factors, such as wind speed, urban vs rural area, humidity, an area’s high fire risk, and the types and levels of pollutants in the air.\nThe case study Chapter 3 provides daily measurements of several variables, including PM10, AQI, …, City.Name and others. For the purposes of this analysis, that data now includes a categorical variable called type_of_day. The categorical variable has two possible values (“Healthy air quality” or “Unhealthy air quality”), and a binary version of the variable, type_of_day01, was created with outcomes of 1 for “Healthy air quality” and 0 for “Unhealthy air quality”. A day is classified as having “Unhealthy air quality” if its AQI exceeds 50. The goal of this study is to investigate the relationship between the type of day and temperature. To begin the analysis, the data are imported and explored using numerical and graphical data summaries.\n\n\nR code\nVideo\n\n\n\n\nCode# Import data \nAQIdf <- read.csv(\"datasets/DailyAQI.2021.csv\")\n \n\n\n# Print the variable names in the dataset\nnames( AQIdf )\n#>  [1] \"PM10\"          \"AQI\"           \"temp\"          \"Latitude\"     \n#>  [5] \"Longitude\"     \"windspeed\"     \"state\"         \"county\"       \n#>  [9] \"City.Name\"     \"type_of_day\"   \"type_of_day01\"\n\n# Load the dplyr package\nlibrary(dplyr) # Provides the mutate() function\n\n# Transform the type_of_day into a factor variable  \n# using the mutate() function. This has R treat\n# `type_of_day` as a categorical variable.\nAQIdf <- mutate(AQIdf, \n         type_of_day = as.factor( type_of_day ), \n         type_of_day01 = as.factor( type_of_day01 ) )\n\n# Print the summary statistics of the dataset\nsummary( AQIdf )\n#>       PM10            AQI              temp           Latitude    \n#>  Min.   :  0.0   Min.   :  0.00   Min.   : 35.13   Min.   :35.05  \n#>  1st Qu.: 19.0   1st Qu.: 18.00   1st Qu.: 54.19   1st Qu.:35.44  \n#>  Median : 32.0   Median : 30.00   Median : 64.79   Median :35.73  \n#>  Mean   : 38.9   Mean   : 33.34   Mean   : 66.91   Mean   :36.02  \n#>  3rd Qu.: 50.0   3rd Qu.: 46.00   3rd Qu.: 80.05   3rd Qu.:36.79  \n#>  Max.   :437.0   Max.   :316.00   Max.   :102.00   Max.   :36.99  \n#>                                   NA's   :391                     \n#>    Longitude        windspeed         state              county         \n#>  Min.   :-119.8   Min.   : 0.750   Length:3115        Length:3115       \n#>  1st Qu.:-119.7   1st Qu.: 2.090   Class :character   Class :character  \n#>  Median :-119.1   Median : 3.337   Mode  :character   Mode  :character  \n#>  Mean   :-118.9   Mean   : 4.147                                        \n#>  3rd Qu.:-118.2   3rd Qu.: 4.917                                        \n#>  Max.   :-117.7   Max.   :22.462                                        \n#>                                                                         \n#>   City.Name                        type_of_day   type_of_day01\n#>  Length:3115        Healthy air quality  :2476   0: 639       \n#>  Class :character   Unhealthy air quality: 639   1:2476       \n#>  Mode  :character                                             \n#>                                                               \n#>                                                               \n#>                                                               \n#> \n\n\n# Load the lattice package\nlibrary(lattice) # Provides the bwplot() function\n\n# Create a boxplot of temperature by type of day\nbwplot(~ temp | type_of_day , \n    data= AQIdf , \n    xlab= \"Temperature\")\n\n\n\n\n\n\nCode\n# Load the mosaic package, which allows formula expression in mean() and sd()\nlibrary( mosaic ) \n\n# Compute the mean temperature by type of day\nmean(~ temp | type_of_day , \n    data= AQIdf, \n    na.rm=TRUE )\n#>   Healthy air quality Unhealthy air quality \n#>              64.38551              75.64739\n\n# Compute the standard deviation of temperature by type of day\nsd(~ temp | type_of_day , \n   data= AQIdf, \n   na.rm=TRUE )\n#>   Healthy air quality Unhealthy air quality \n#>              14.95971              12.15426\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplots and summary statistics show that temperatures tend to be lower on healthy air days compared to unhealthy air days. The mean temperatures for healthy air days are also lower than those for unhealthy air days. Additionally, temperatures exhibit more variability on healthy air days compared to unhealthy air days. This may indicate more extreme temperatures on healthy air days or suggest that other factors contribute to the variability, such as wind speed or humidity.\nThe SLogiR model is fitted using glm() but note that the response variable must be binary (such as type_of_day01):\n\n\nR code\nVideo\n\n\n\n\nCode# The function will require the following information:\n# y: Replace with 'type_of_day01', the binary response variable.\n# x: Replace with 'temp', the explanatory variable.\n# data: Set equal to a 'kernAQIdf', the name of the dataframe.\n# family: Set equal to \"binomial\".\n\n# Fit the SLogiR model using glm() function\nglm( type_of_day01 ~ temp ,\n  data = AQIdf , \n  family = \"binomial\" )\n#> \n#> Call:  glm(formula = type_of_day01 ~ temp, family = \"binomial\", data = AQIdf)\n#> \n#> Coefficients:\n#> (Intercept)         temp  \n#>     4.89571     -0.05214  \n#> \n#> Degrees of Freedom: 2723 Total (i.e. Null);  2722 Residual\n#>   (391 observations deleted due to missingness)\n#> Null Deviance:       2897 \n#> Residual Deviance: 2629  AIC: 2633\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output provides \\(b_0= 4.90\\) and \\(b_1=-0.05\\). Note that \\(e^{b_1}=e^{-0.05}=0.95\\), which suggests that the odds of having a healthy air day are estimated to decrease by 5% for each degree increase in temperature. This also suggests that a 5 degree increase in temperature means that the odds of having a healthy air day are estimated to decrease by 25% The glm() function provides much more than just these values. To extract other information, the result from glm() must be stored. Below the resulting glm() fit is stored and then the summary() is applied to this object. The functions residuals() and predict() are also illustrated below.\n\n\nR code\nVideo\n\n\n\n\nCode# Store glm() fit in an object, call it aqfit\naqfit <- glm( type_of_day01 ~ temp ,\n           data = AQIdf , \n           family = \"binomial\" )\n\n# aqfit is called an glm object.\n# Display a summary of the SLogiR model fit using the summary() function on the glm object, aqfit.\nsummary( aqfit )\n#> \n#> Call:\n#> glm(formula = type_of_day01 ~ temp, family = \"binomial\", data = AQIdf)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2405   0.3715   0.5044   0.7166   1.3612  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  4.895708   0.251133   19.49   <2e-16 ***\n#> temp        -0.052137   0.003377  -15.44   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 2897.5  on 2723  degrees of freedom\n#> Residual deviance: 2629.3  on 2722  degrees of freedom\n#>   (391 observations deleted due to missingness)\n#> AIC: 2633.3\n#> \n#> Number of Fisher Scoring iterations: 4\n\n# Extract the residuals from the glm object, aqfit, and store them in aqres\n# The type argument specifies the type of residuals to extract,\n# which in this case are Pearson residuals\naqres <- residuals( aqfit, \n          type = \"pearson\" )\n\n# Extract the predicted values from the glm object, aqfit, and store \n# them in aqpredictLO. The type argument specifies the scale of the predictions, which in this case is the log odds scale\n# Store the result in an object called aqpredictLO\naqpredictLO <- predict( aqfit,\n             type =\"link\")\n\n# Extract the predicted probabilities of y=1 from the glm object, aqfit, \n# and store them in an object  called aqpredictProbs.\n# The type argument specifies the scale of the predictions, which \n# in this case is the probability scale.\naqpredictProbs <- predict( aqfit,\n             type =\"response\")\n\n\n\n\nSee previous video.\n\n\n\nNote that the summary() provides the following table:\n\n\n\nEstimate\nStd. Error\nt-value\np-value -\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(Z_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(Z_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nThe terms \\(SE_{b_0}\\) and \\(SE_{b_1}\\) are the standard errors for \\(b_0\\) and \\(b_1\\). The next sections cover inference for the SLogiR, and it requires that the assumptions about the data be reasonable.\nNext, we perform model diagnostics to assess the model assumptions with the help of the stored fitted values, residuals, and xyplot(). Here, xyplot() will only require the glm object after loading the tactile R package:\n\n\nR code\nVideo\n\n\n\n\nCode# Create a scatter plot of the predicted log odds \n# against temperature. \nxyplot(aqpredictLO ~ temp,\n        data = AQIdf , \n        xlab = \"Temperature\" ,\n        ylab= \"Predicted log odds\" )\n\n# Load the tactile package, which allows the creation\n# of residual plots using the glm object\nlibrary( tactile ) \n\n\n# Plot Pearson residuals vs. leverage.\n# Note that xyplot() can produce various\n# residual plots, but only certain plots\n# are relevant for the SLogiR model.\n\n# Create a scatter plot of the Pearson residuals (from aqfit) vs. leverage.\nxyplot( aqfit, \n   which= 5 ) # this tells R to plot residuals vs. leverage.\n\n\n\n\n\nFigure 15.2: Diagnostic plots\n\n\n\n\n\n\nFigure 15.3: Diagnostic plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach plot is analyzed and interpreted below:\n\nThe plot on the left displays the predicted log odds against the explanatory variable and shows a linear trend. This suggests that Assumption 4 (linearity of log-odds) is reasonable.\nThe plot on the right shows the standardized Pearson residuals against the leverage. The presence of some potential outliers (standardized Pearson residuals greater than 3 in magnitude) is visible. Cook’s distance4 contour lines of 1/2 and 1 are plotted, but no leverage values exceed a Cook’s distance of 1. This suggests that Assumption 2 (no influential outliers) is reasonable.\n\nAssumptions 1 (independence of observations) and 3 (large sample size) are reasonable since the sample size is large, and it seems reasonable to assume that a given observation is not influenced by or related to the rest of the observations.\nOverall, all data assumptions are met for the SLogiR model."
  },
  {
    "objectID": "logisticreg.html#inference-on-the-explanatory-variable-use-wald-based-methods",
    "href": "logisticreg.html#inference-on-the-explanatory-variable-use-wald-based-methods",
    "title": "15  Simple logistic regression",
    "section": "\n15.2 Inference on the Explanatory Variable use Wald-based methods",
    "text": "15.2 Inference on the Explanatory Variable use Wald-based methods\nThe focus in this section is on inference for \\(\\beta_1\\) (the true slope) in the context of SLogiR. The Wald test for the slope of a regression line is a widely used hypothesis test to assess the significance of the explanatory variable’s effect on the binary response variable. Specifically, the test is used to determine if the true slope \\(\\beta_1\\) is significantly different from zero, based on the observed value of \\(b_1\\).\nIt is important to note that the observed value of \\(b_1\\) can vary. If a study were replicated, the value of \\(b_1\\) would likely be different. The use of inferential methods allows us to account for this variability and make statistically valid conclusions about the true slope \\(\\beta_1\\). A Wald-based confidence interval can be constructed for \\(\\beta_1\\) and it provides a range of plausible values for the true slope at a specified significance level. These methods are generally referred to as Wald-based methods for the slope.\nThe Wald test requires that certain assumptions (1, 2, 3, and 4) about the data are reasonable. The test statistic for this Wald test may be defined as:\n\\[Z= \\frac{b_1}{SE_{b_1}}\\]\nHere, \\(SE_{b_1}\\) represents the standard error of \\(b_1\\)5. Our primary interest is in determining if \\(\\beta_1\\) is significantly different from zero, which translate to \\(\\beta_1\\neq 0\\). Consequently, the null and alternative hypotheses are:\n\\[H_0: \\beta_1=0 \\qquad H_a: \\beta_1 \\neq 0\\]\nUnder \\(H_0\\), the null distribution of the test statistic \\(Z\\) follows a standard normal distribution6. This distribution is used to compute the p-value. The function glm() computes the test statistic and p-value for the Wald test, and the results are provided by the summary() output. Recall that the summary() function provides a table with relevant information, including a test statistic and p-value for the Wald test, as well as other statistics like the coefficient estimates and standard errors:\n\n\n\nEstimate\nStd. Error\nz-value\np-value\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(Z_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(Z_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nHere, \\(Z_{b_1}\\) and p-value\\(_{b_1}\\) refer to the test statistic for the Wald test and the two-sided p-value\\(_{b_1}\\), respectively for testing if \\(\\beta_1\\neq 0\\). The function glm() can also be utilized to obtain a confidence interval (CI) for the true slope \\(\\beta_1\\). To obtain a CI, apply the confint() function to the resulting glm object.\n\n\n\n\n\n\nR functions\n\n\n\n### confint( glm object , parm , level)\n# lm object: An object saving the result of usuing \n#      the lm function.\n# parm:  Set equal to 2 \n# level: Set to desired confidence level.\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe apply a Wald test to the data involving AQI levels and temperature. The aim is to investigate the effect of temperature on the probability of having an ‘Unhealthy air quality’ day compared to a ‘Healthy air quality’ day at \\(\\alpha=.001\\). Let \\(\\beta_1\\) represent the effect of temperature on the odds of having an ‘Unhealthy air quality’ day. Since the aim is to determine if the effect is significant, the hypotheses are:\n\\[H_0: \\beta_1=0 \\qquad H_a: \\beta_1 \\neq 0\\]\nThe following code uses glm() and summary() to obtain the test statistic and the corresponding p-value for a Wald test:\n\n\nR code\nVideo\n\n\n\n\nCode# The glm object, aqfit, was previously created by \n# fitting a SLogiR model to the data.\n# Summarizes the model fit using the summary() function on \n# the glm object, aqfit.\nsummary( aqfit )\n#> \n#> Call:\n#> glm(formula = type_of_day01 ~ temp, family = \"binomial\", data = AQIdf)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2405   0.3715   0.5044   0.7166   1.3612  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  4.895708   0.251133   19.49   <2e-16 ***\n#> temp        -0.052137   0.003377  -15.44   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 2897.5  on 2723  degrees of freedom\n#> Residual deviance: 2629.3  on 2722  degrees of freedom\n#>   (391 observations deleted due to missingness)\n#> AIC: 2633.3\n#> \n#> Number of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on output from summary( aqfit ), the observed value of test statistic is \\(Z=-15.44\\). The corresponding p-value is approximately 0, indicating that the data provides very strong evidence against the null hypothesis. When considering a significance level of \\(\\alpha=.001\\), since the p-value\\(\\leq \\alpha=.001\\), we have sufficient evidence to reject the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\). This means that there is a significant effect of temperature on the odds of having an “Unhealthy air quality” day compared to a “Healthy air quality” day. In other words, temperature levels have a significant impact on the probability of observing an unhealthy or healthy air quality day. This also suggests that temperature is an important factor to consider when analyzing or predicting air quality conditions, and that the relationship between these variables is not due to chance alone.\nThe function confint() is used to obtain a 99% CI for \\(\\beta1\\):\n\n\nR code\nVideo\n\n\n\n\nCode \n# Summarize the lm object, PM10fit. ct\n\n# Compute a 99% CI for beta1.\nconfint( aqfit ,\n         parm= 2 , # Setting parm =1 will provide CI for beta0\n         level= .99 )\n#>       0.5 %      99.5 % \n#> -0.06095298 -0.04354691\n\n\n\n\nSee previous video.\n\n\n\nThe output provides a 99% CI for \\(\\beta_1\\) with bounds \\((-0.061, -0.044)\\). Thus, we can be 99% confident that the true effect of temperature on the log-odds of having an “Healthy air quality” day compared to a “Unhealthy air quality” day lies between -0.061 and -0.044. Since the interval does not contain 0, it further supports the conclusion that changes in temperature levels affect the probability of observing unhealthy or healthy air quality day.\nNote, if the bounds on the CI are exponentiated, it provides a CI for the odds. Thus, we can be 99% confident that the interval for \\(e^{\\beta_1}\\) is given by \\((e^{-0.061}, e^{-0.044})\\), which is approximately \\((0.941, 0.957)\\). In other words, we are 99% confident that with each unit increase in temperature, the odds of having a “Healthy air quality” day compared to an “Unhealthy air quality” day decrease anywhere from 4.3% to 5.9%"
  },
  {
    "objectID": "logisticreg.html#sample-size-estimation-and-power-analysis-for-slogir",
    "href": "logisticreg.html#sample-size-estimation-and-power-analysis-for-slogir",
    "title": "15  Simple logistic regression",
    "section": "\n15.3 Sample size estimation and power analysis for SLogiR",
    "text": "15.3 Sample size estimation and power analysis for SLogiR\nIdeally, researchers should estimate the sample size for a study before collecting data. Knowing the sample size required to detect a desired effect at the beginning of a project allows one to manage their data collection efforts. Further, this allows researchers to determine the statistical power the test will have to detect an effect if it exists. The R functions powerLogisticCon() and SSizeLogisticCon() from the powerMediation R package provide sample size and statistical power calculations for a SLogiR model, respectively, when the explanatory variable is assumed to be numerical. Both functions assume the appropriate assumptions about the data for the Wald test are met.\n\n\n\n\n\n\nR functions\n\n\n\n### powerLogisticCon( n , p1, OR, alpha = 0.05 )\n# n: Set equal to the desired sample size.\n# p1: Set equal to the expected probability of observing a 1\n#   for an outcome. Typically it is set to .5.\n# OR: Set equal to the expected odds ratio. log(OR) is\n#   the change in log odds for an increase of one unit \n#   in X. Thus, beta1=log(OR) is the effect size to be tested.\n# alpha: Set equal to the desired alpha value (significance level).\n#\n### SSizeLogisticCon(power, p1, OR, alpha = 0.05 )\n# power: Set equal to the desired power (a number between 0 and 1). \n# p1: Set equal to the expected probability of observing a 1\n#   for an outcome. Typically it is set to .5.\n# OR: Set equal to the expected odds ratio. log(OR) is\n#   the change in log odds for an increase of one unit \n#   in X. Thus, beta1=log(OR) is the effect size to be tested.\n# alpha: Set equal to the desired alpha value (significance level).\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nBelow we use powerLogisticCon() to determine the power when the sample size is 150, probability of observing a 1 is .5, and expected odds ratio of 1.5 at significance level of \\(\\alpha=.001\\).\n\nCode# Load the powerMediation package \nlibrary(powerMediation) # Provides SSizeLogisticCon and powerLogisticCon\n\n# Calculate the statistical power of a SLogiR model \npowerLogisticCon( n = 150 , \n         p1 = .5 , \n         OR = 1.5 ,\n         alpha = .001 )\n#> [1] 0.209669\n\n\nThe statistical power of this test is low. To increase the power, one may increase the effect size (the larger it is, the easier it is to detect), increase \\(\\alpha\\) (make it easier to reject \\(H_0\\) and find a significant effect), and/or increase the sample size.\nBelow we use SSizeLogisticCon() to determine the sample size needed to obtain a power of .90 when the probability of observing a 1 is .5, and the expected odds ratio is 1.5 at a significance level of \\(\\alpha=.001\\).\n\nCode# Compute the required sample size for a SLogiR model\nSSizeLogisticCon( p1 = .5 , \n         OR = 1.5 ,\n         alpha = .001 ,\n         power = .90 )\n#> [1] 509\n\n\nTo obtain a power of at least .90 under the given conditions, a sample size of at least 509 is needed."
  },
  {
    "objectID": "a1-startingR.html",
    "href": "a1-startingR.html",
    "title": "Appendix A — Installing or accessing R and RStudio",
    "section": "",
    "text": "This resource uses R for data analysis. As a programming language, R, like any programming language or statistical software, is not perfect, but R does have the distinction of being a programming language that was created and developed for the purpose of making the analysis of data easier. RStudio may be thought of as a graphical user interface (GUI) for R. Think of R as the engine for a car, whereas RStudio is the dashboard, wheel, etc. that allows one to control the engine. Although RStudio is much more than a GUI, it will only be used as in interface for R in the modules. That is, R will not be used directly, rather R will always used through RStudio.\nR and RStudio were selected for various reasons, including:"
  },
  {
    "objectID": "a1-startingR.html#how-to-install-r",
    "href": "a1-startingR.html#how-to-install-r",
    "title": "Appendix A — Installing or accessing R and RStudio",
    "section": "A.1 How to install R",
    "text": "A.1 How to install R\nGo to The Comprehensive R Archive Network page. Select the appropriate link and follow the instructions below based on your operating system.\n\nWindows and Mac\nTo install R, follow these steps:\n\nGo to The Comprehensive R Archive Network page.\nSelect the appropriate link for your operating system.\nSelect the “base” link, and then select the link at the top of the page that opens. This link should read “Download R … for [Operating System],” where … is the current version number (currently 4.2.0 as of July 2022).\nDownload the installer program, which installs the most up-to-date version of R for your operating system.\nRun the installer program and go through the installation steps that appear, using the default selections.\n\n\n\nLinux\nR comes preinstalled on some Linux distributions, but you’ll want the newest version of R if yours is out of date. Files for building R from source on either Debian, Fedora, Redhat, SUSE, or Ubuntu distributions may be found by selecting the link “Download R for Linux.”. Select the appropriate distribution to obtain instructions and the source files for installation."
  },
  {
    "objectID": "a1-startingR.html#how-to-install-rstudio",
    "href": "a1-startingR.html#how-to-install-rstudio",
    "title": "Appendix A — Installing or accessing R and RStudio",
    "section": "A.2 How to install RStudio",
    "text": "A.2 How to install RStudio\nTo install RStudio, follow these steps:\n\nVisit the RStudio download page.\nScroll down towards the bottom of the page under “Installers for Supported Platforms.”\nClick the link that corresponds to your operating system.\nDownload the RStudio installer.\nRun the RStudio installer with the default settings\n\n\nR and RStudio for Chrome OS\nFor Chromebook users, there is currently no R/RStudio app available for installation unless you are proficient in Linux. However, R and RStudio are accessible through the CSUB virtual lab (https://its.csub.edu/VCL)."
  },
  {
    "objectID": "a2-firstR.html",
    "href": "a2-firstR.html",
    "title": "Appendix B — A first session in RStudio",
    "section": "",
    "text": "Anytime you open RStudio, we say we are in an RStudio session. After opening RStudio for the first time, you will see the RStudio interface, which consists of three panes. Next, open the script editor by selecting File > New File > R Script to make the script editor appear. RStudio should look something like what you see below but perhaps with different colors:"
  },
  {
    "objectID": "a2-firstR.html#the-layout-of-rstudio",
    "href": "a2-firstR.html#the-layout-of-rstudio",
    "title": "Appendix B — A first session in RStudio",
    "section": "\nB.1 The layout of RStudio",
    "text": "B.1 The layout of RStudio\nA brief description of each pane follows:\n\nThe upper left pane is the R Script file. This is where will type in commands to be executed in the script editor. Writing and saving a series of R commands in the script file makes it easy to reuse or modify code at a later time.\nThe lower left pane is called the console pane. Every time you launch RStudio, it will have the same text at the top of the console telling you the version of R that you’re running. Below that information is the Rprompt (the symbol >). As its name suggests, this prompt is really a request, a request for a command. It is here where RStudio will tell R what to do. This is the most important pane because this is where R actually does stuff.\n\nUpper right pane contains your environment/history pane.\n\nIn the environment  tab you can see which data and values R has in its memory. R’s memory is called an environment.\nThe history tab shows what has been typed before.\nDon’t worry about the rest of the tabs.\n\n\n\nBottom right pane is the files/plots/packages/help pane.\n\nIn the Files tab you can browse and select files to open.\nThe Plots tab will show any plots that you create.\nThe rest of the tabs we will revisit as needed."
  },
  {
    "objectID": "a2-firstR.html#getting-familiar-with-r",
    "href": "a2-firstR.html#getting-familiar-with-r",
    "title": "Appendix B — A first session in RStudio",
    "section": "\nB.2 Getting familiar with R",
    "text": "B.2 Getting familiar with R\nTo get familiar with R, it will first be used as a calculator. To get started, enter the 2 + 2 in the script editor. Once typed, highlight it and select the “Run” button (see image below)\n\n\nFigure B.1: Using the “Run” button\n\n\n\n\n\n\n\n\nObjects, assignments, and evaluation\n\n\n\nExecuting the command 2 + 2 only executes/runs the command in R. That is, RStudio was used to to tell R to add two plus two, and R responded with 4 in the R console. It did not save the result, it simply evaluated 2 + 2.\nIf you want to save the result of 2 + 2 so that you can use it later, then we have to tell R to store the result by assigning the result to an object you can access later. Think of an R object as anything that holds/stores information (some data structure)\nTo assign the result of 2 + 2 to an object called x, type x <- 2 + 2 on a new line in the editor. Also, assign the result of 3+5 to an object called X. Then highlight these line and select “Run”.\n\n\n\n\n\n\nNote that either = or <- may be used for assignments.\n\n\n\n\nFigure B.2: Assigning the evaluation of 2 + 2 to an object called x\n\n\nNote the following:\n\nThe R code x <- 2 + 2 translate to “assign the result of 2 plus 2 to an object called x”.\nYou can also see the new object x in the environment on the upper right pane.\nR is case-sensitive! x and X are distinct.\nGet into the habit of saving things in R objects so that they can be accessed at later time. As long as your RStudio session remains open, RStudio keeps any R objects that you defined in it’s memory.\nOnce you end your session, RStudio loses what it has stored in its memory. Next time an RStudio session is started, no R objects will appear in its memory. Is this a problem? Later you will see that this is not a problem!\nAt its most basic level, R can be viewed as a fancy calculator. While we won’t be using RStudio do algebra problems, it is a good way to get familiar with RStudio.\nUpon selecting “Run”, the result of the above appears in the console pane, preceded by the command you executed, and prefixed by the number 1 in square brackets [1]. The [1] indicates that this is the first (and in this case only) result from the command.\nMany commands will return multiple values. Try the following one by one, where each is typed on a new line and then selecting “Run” after typing the command in the editor:\n\n\nCode25*10\n    \n5/2\n\n2 + 2\n\n\n\nAfter running these commands, you should note the R prompt: >.\n\nThe > prompt means that R is content and ready for a new command or input.\nPlease note that spacing is not an issue with R. For example, 5 / 2 is the same as 5/2. However, using spaces makes the code easier to read and catch mistakes."
  },
  {
    "objectID": "a2-firstR.html#comments",
    "href": "a2-firstR.html#comments",
    "title": "Appendix B — A first session in RStudio",
    "section": "\nB.3 Comments",
    "text": "B.3 Comments\nEverything that is typed after a # sign is assumed to be a comment and is ignored by R. Adding comments to R scripts is useful because it will help one recall what the commands or lines of code does. In the editor, type the following and select “Run”:\n\nCode# Comments are ignored by R!\ny <- 1 + 3    # this command tells R to compute 1 plus 3 and assign it an R object called  y\n\n\nNote that y is now in RStudio’s memory, and its value can be accessed by typing y on a new line and selecting “Run”. Now, type the following in your editor on a new line and then select “Run”:\n\nCode#z <- 3 - 7  # this code  is ignored because of # ```\n\n\nNote that this code ignored because of #. Get into the habit of using comments!"
  },
  {
    "objectID": "a2-firstR.html#creating-and-saving-your-rstudio-script",
    "href": "a2-firstR.html#creating-and-saving-your-rstudio-script",
    "title": "Appendix B — A first session in RStudio",
    "section": "\nB.4 Creating and saving your RStudio script",
    "text": "B.4 Creating and saving your RStudio script\nIt’s always good practice to save your RStudio script. Edit the script that we have started to look like this:\n\nCode### Name: Your-name-here\n### File description: Some misc R code  \n\n\n2 + 2\n\nx <- 2 + 2\n\n25*10\n\n5/2\n\n2 + 2\n\n\nNext, save the RStudio script/file by selecting File > Save as. Save the text file as an .R file. Any RStudio script should be saved as a .R file."
  },
  {
    "objectID": "a2-firstR.html#installing-required-packages",
    "href": "a2-firstR.html#installing-required-packages",
    "title": "Appendix B — A first session in RStudio",
    "section": "\nB.5 Installing required packages",
    "text": "B.5 Installing required packages\nThere are many packages (also called libraries) that come installed with R or that can be installed to expand the ability of R. There are a few additional packages that need to be installed. The following packages are specifically designed to make R more accessible and easier for data analysis:\n\ntidyverse: This packages provide a collection of R packages designed for data science. Functions front his packages will be utilized for aspects of importing data and data management.\nmosaic: This packages provides a number of functions that makes some basic data analysis tasks easier.\n\nTo install these packages, in the R console type the following command:\n\nCodeinstall.packages( \"tidyverse\" )  # no spaces within quotation marks!\n\n\nHit enter. Activity will start occurring in the R console. This may take a few minutes to complete. During the installation process, if R requests that + an CRAN mirror be selected for downloading, any USA mirror will do. + the personal library on your computer be used, answer yes. This ensures that admin privileges are not required for installation. + packages be updated, please agree to the updates.\nWhen it is done installing, it should say something similar to “The downloaded binary packages are in…” right above the R prompt >. This mean the packages were installed without error. Now repeat the sames step for the package mosaic. Again, this may take a few minutes to complete.\nAlternatively, you can select Tools from the RStudio menu, followed by Install packages. Enter the name of the R package under “Packages (separate…)” and click “Install”. Activity will then start occurring in the R console as when using install.packages().\n\n\n\n\n\n\nLoading packages\n\n\n\nR packages will only have to installed once on the computer that is being used. However, once the package is installed, you will have to load it into the workspace (R’s memory) before it can be used every time you start a new RStudio session. If you are using a different computer, you will have to install these packages again.\nTo load a package, run the command library(package-name). For example. to load the mosaic packages, run the following in R:\n\nCodelibrary( mosaic )  # no quotation marks!"
  },
  {
    "objectID": "a2-firstR.html#some-useful-tips",
    "href": "a2-firstR.html#some-useful-tips",
    "title": "Appendix B — A first session in RStudio",
    "section": "\nB.6 Some useful tips",
    "text": "B.6 Some useful tips\nSome things to remember when using RStudio:\n\nAlways use an R script (a .R file) to save your work.\nR is case-sensitive!\nDoes the code in the R console keep going and going and…? Hit the Esc key once or twice!\nIf you see a + prompt in the R console, it means the console is waiting for more input.\nComment your code using #. It’s amazing what you can forget quickly.\nYou are expected to look back at the appendix or other segments in this resource if you forget something along the way, as you would for any other software that you are starting to learn.\nSome sections will contain examples or templates of how to do certain analysis. You job will primarily will be to edit the templates to reflect the data that you are working with.\nGetting an error message? An online search for the error message will be useful."
  },
  {
    "objectID": "a3-basicsR.html",
    "href": "a3-basicsR.html",
    "title": "Appendix C — Some basic tasks in R",
    "section": "",
    "text": "These section covers importing data into R and some basic R functions."
  },
  {
    "objectID": "a3-basicsR.html#importing-data-from-your-computer",
    "href": "a3-basicsR.html#importing-data-from-your-computer",
    "title": "Appendix C — Some basic tasks in R",
    "section": "Importing data from your computer",
    "text": "Importing data from your computer\nDownload the .csv file below onto your computer. Make sure to note where the file is saved. Depending on the browser settings, after clicking on the link below, the file will either open in the browser and it will be downloaded.\nhttp://www.csub.edu/~emontoya2/datasets/sampledata.csv\nAfter saving this .csv file, one can take a “point and click” approach to importing data. Like most software, RStudio has a toolbar where you can access many commands. To import data using the toolbar menu, select File>Import Dataset> From Text (base). Then select your .csv file. If your data are stored in an excel spreadsheet (.xlsx) in tidy form, then select “From Excel”.\nAlternatlively, if you know the file path for the spreadsheet, you can run following\n\nCode# by putting in \"file.choose()\", this tells R that you want to browse for and select the csv file\nImportFromDrive = read.csv( file.choose() )\n\n\n\n\n\nThis automatically opens a window that allows you to browse for your .csv file. Select your file and click on Open. You should see the following in the console\nThe data set ImportFromDrive should appear in the Environment tab in the upper right panel where it displays the number of observations and variables. One may also replace file.choose() with the location of the file in quotation marks."
  },
  {
    "objectID": "a3-basicsR.html#sec-UsingRfuns",
    "href": "a3-basicsR.html#sec-UsingRfuns",
    "title": "Appendix C — Some basic tasks in R",
    "section": "\nC.2 Using R functions",
    "text": "C.2 Using R functions\nR functions are either ready use when you start a new RStudio session or an R package must be loaded to use a function from that package. The syntax of an R function consist of the function name followed by parentheses that contain argument(s). The R functions used in this resource will generally consists of the name of the data frame that is in tidy form and one or more of the following arguments:\n\nNames of response variable as spelled in the data frame\nNames of explanatory variable(s) as spelled in the data frame\nadditional optional arguments\n\nR has many built-in function or function that become available once a library is loaded. For example, the mean( ) function computes the mean of sample of data."
  },
  {
    "objectID": "a3-basicsR.html#summarizing-data-with-r",
    "href": "a3-basicsR.html#summarizing-data-with-r",
    "title": "Appendix C — Some basic tasks in R",
    "section": "\nC.3 Summarizing data with R",
    "text": "C.3 Summarizing data with R\nExploring data using graphical and numerical summary statistics will be discussed in ?sec-summaries. For now, a brief introduction to graphing and computing summary statistics is provided. When wanting to summarize a single variable, functions to create graphical and numerical summaries follows the general form task( ~ x , data) where x is the variable you want to graph or compute a statistic of. Depending on the summary, we may have to specify some additional things. If the variable x is a variable in the data frame, then data would be set equal to the name of the data frame. The names used for x and the name of the data frame must be the names that R has given the data set and variable. task would be replaced with the name of the function. For bivariate, the form becomes task( y ~ x , data) becomes where y represent the dependent variable and x is the explanatory variable. Most R tasks covered in the modules will follow this general form. The expressions y ~ x or ~ x are called formulas in R.\nRecall that we imported the MLB dataset and we stored it in a data frame called mlbDataImport. Here, the task is to create a histogram of salary. salary represents the salary (in $1000s) of a given baseball player in 2010. Again, the basic formula is task( ~ x , data) where task would be replaced with the name of the function to create the desired graph. RStudio refers to histograms as histogram. The variable of interest is salary and it would replace x. The argument data is set equal to mlbDataImport.\n\nCodelibrary( lattice ) # load the lattice package. This package provides the histogram function\nhistogram( ~ salary  , \n           data=  mlbDataImport )\n\n\n\n\n\n\n\nTo compute the mean salary of these baseball players we replace task with mean. As with the histogram, x is replace with the name of the variable salary, and data is set equal to mlbDataImport:\n\nCodelibrary( mosaic ) # load the mosaic package. This provides the  mean function \nmean( ~ salary  , \n      data=  mlbDataImport )\n#> [1] 3281.828\n\n\nSuppose you wanted to compute the average salary for each position. To so, we can tell RStudio to compute the mean by levels/groups of a grouping variable. A grouping variable groups data into categories or groups. Grouping variables are categorical variables. When summaries are computed based on the levels or groups of the grouping variables, they are called side-by-side or comparative summaries.\nTo summarize the data conditional on a value of categorical variable, the general formula becomes task( ~ x | gfactor , data) or task( y ~ x | gfactor , data), where gfactor represent the categorical variable, typically called the grouping variable. Here, the task is to compute the mean salary for each potion, so the grouping variable is position:\n\nCodemean( ~ salary | position , data=  mlbDataImport )\n#>           Catcher Designated Hitter     First Baseman         Infielder \n#>          1937.220          5235.714          5826.521           770.575 \n#>        Outfielder           Pitcher    Second Baseman         Shortstop \n#>          3753.926          2999.197          3022.723          2844.146 \n#>     Third Baseman \n#>          4641.286\n\n\nNote that the names of the variables (e.g., salary, position, etc.) are spelled exactly as they appear in the data frame.\n\n\n\n\n\n\nFunction arguments\n\n\n\nFunctions in R will have a number of arguments. An argument is piece of information that is provided to the function in order to execute the requested task, such a compute a summary statistic or create a graph. From hereinafter, functions used in the modules will describe the function in the following ways:\n\n\n\n\n\n\nR functions\n\n\n\n### task( arg1 , arg2,  arg3, ...)\n# arg1: Description of what information is needed for argument 1\n# arg2: Description of what information is needed for argument 2\n# arg2: Description of what information is needed for argument 2\n# and so on\n\n\n\nSome arguments will be required, others are optional depending on the function. More often than not, the functions will require at least the following arguments to run: A formula (such as y ~ x or ~ x) and the name of the data frame (data=‘name of data frame’). The phrase task will be replaced with the name of the function, such as mean or histogram.\n\n\nYou can use the code provided in this resource as a template to adapt to your dataframe and the names of the variables within your dataframe (discussed in the next section).\nAll functions used in the module will only be described when they are first used. Any subsequent uses of a given function will not provided a description but the reader may search for the name of the function in the search bar provided in the upper left of the module."
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "16  PCA",
    "section": "",
    "text": "AI art generated from the text “Principal component analysis”\nThis section introduces an advanced exploratory data analysis method called Principal component analysis (PCA). PCA summarizes the information1 in data that is composed of multiple variables. Below is a scatterplot and correlation matrix of a dataset that consists of five variables. The scatterplot shows the pairwise relationships between the variables and they are useful for visualizing relationships, clusters, and outliers. The correlation matrix provides a numerical measure of the strength and direction of these relationships. This section only covers PCA on standardized data.\nGraphical and numerical summaries show positive and negative correlations between variables, and the strength of the linear relationship between them depends on which pair of variables is considered. With nine variables, there are 10 possible 3D scatterplots to examine, making it challenging to explore relationships beyond pairwise comparisons. That is, when dealing with multiple variables, examining all possible pairwise relationships can be challenging, particularly in large datasets. Visualizing multidimensional relationships also becomes difficult as the number of variables increases, limiting the usefulness of graphical and numerical summaries. Therefore, more advanced exploratory data analysis methods, such as PCA may be needed to obtain meaningful insights from high-dimensional datasets.\nPCA is a powerful and widely used technique for extracting dominant modes of variation from a dataset. The data must consist of only numerical variables. The goal of PCA is to identify directions (or principal components or modes of variation) along which the variation in the data are maximal. The dominant modes of variation in PCA correspond to the principal components that explain most of the variation in the dataset. Thus, a few principal components (PCs) may capture almost as much of the variation as the original variables. Such principal components capture the most important patterns or structures in the data. In some cases, the dominant modes of variation may be associated with specific features or facets in the data, allowing for a better understanding of the underlying variation processes that generates the data.\nTo better ensure that PCA provides meaningful results, several assumptions about about the data are recommended."
  },
  {
    "objectID": "pca.html#the-method-of-pca",
    "href": "pca.html#the-method-of-pca",
    "title": "16  PCA",
    "section": "\n16.1 The method of PCA",
    "text": "16.1 The method of PCA\nIt is difficult to describe the method of PCA without some prior knowledge of matrix algebra and linear operators, but a basic overview of the technique is provided. In the simplest terms, PCA is a statistical technique that creates linear combinations of the original data to summarize variation and reduce the dimensionality of the data. We start with some essential notation and definitions for the PCA:\n\n\n\n\n\n\nNotation and definitions\n\n\n\n\nWe assume there are \\(p\\) variables and \\(n\\) observations so that \\(x_{ij}\\) denotes the i\\(^{th}\\) observed value under the j\\(^{th}\\) variable for \\(i=1, ..., n\\) and \\(j=1, 2, .., p\\). For example, \\(x_{34}\\) denotes the 3rd observed value under the 4th variable.\nThe i\\(^{th}\\) principal component (PC) is linear combination (weighted sum) of the variables, \\(e_{i1} x_{1}+ e_{i2} x_{2} + e_{i3} x_{3} + \\cdots + e_{ip} x_{p}\\), and in total there will be \\(p\\) PCs. \\(e_{i1}, e_{i2}, ..., e_{ip}\\) are the weights assigned to each variable in the linear combination for the \\(i^{th}\\) principal component and are generally referred as loadings.\nThe first PC is the linear combination of variables that provides the maximum variance (among all possible linear combinations considered) so that it accounts for as much variation in the data as possible. That is, the values of \\(e_{11}, e_{12}, \\ldots, e_{1p}\\) are chosen to maximize the variance4.\nThe second PC is also a linear combination of the variables, but with the constraint that it is uncorrelated with the first principal component, and that it accounts for as much of the remaining variation in the data as possible5.\nThe third PC is also a linear combination of variables but that accounts for as much of the remaining variation as possible (not accounted for by the first two PCs), with this PC is uncorrelated with between the rest of the PCs.\n\nThe process of computing the PCs continues until all \\(p\\) principal components have been computed, with each component capturing a unique source of variation in the data that is uncorrelated with the other components. The first few principal components typically capture the majority of the variation in the data and retain as much information as possible from the original variables.\n\n\nIf the above description of PCA is too technical, think of PCA as providing a view of the data from a different vantage point, where this vantage point6 focuses on maximizing the variability in the data that can highlight certain patterns or relationships. This is akin to say a real estate agent trying to sell a property. Some pictures of the property may better highlight the most desirable or best features of the property. Of course, certain pictures will better highlight certain features more than others. In PCA, the data are viewed from a different vantage point in such a way to best reveal certain patterns or relationships not visible in the original data. This is just an attempt at an analogy but hopefully helps illustrate the idea of using PCA to create a different perspective on the data.\nThe equations or formulas for finding principal components will not be discussed here since the focus is on using software to conduct PCA. Specifically, we use R to obtain important elements of PCA, including\n\nStandard deviation for each PC: It measures variability across a single component component (i.e. spread of data within a given PC).\nProportion of Variance for each PC: The amount of variance that a given PC accounts for. This is provided by taking squared standard deviations (variance) of the given PC divided by sum of the variances.\nCumulative Proportion for each PC: The cumulative variation represented by PCs up until a given component.\nLoadings for each PC: If PCA computations are performed on standardized data, the loadings may be thought of as correlations in that they reflect the strength and direction of the relationship between each variable and each PC. These correlations help provide the coefficients of the linear combinations of the original variables that create the PCs. If performed on non-standardized data, they represent the covariances rather than the correlations.\n\nIf the proportion of variation explained by the first, say \\(k\\), PCs is large, then not much information is lost by considering only the first \\(k\\) PCs instead of the whole dataset. However, determining the appropriate value of \\(k\\) will be addressed later."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Module: Data basics and drawing statistical conclusions",
    "section": "",
    "text": "The chapters in this module cover the following topics:\n\nProperties of data\nStudy design\nScope of inference\nInference permitted based on the study design\n\nModule chapters:\n\nData summaries\nDrawing statistical conclusions"
  },
  {
    "objectID": "data.html#module-learning-objectives",
    "href": "data.html#module-learning-objectives",
    "title": "Module: Data basics and drawing statistical conclusions",
    "section": "Module learning objectives",
    "text": "Module learning objectives\n\nIdentify variables as discrete numerical, continuous numerical, or categorical.\nDistinguish between discrete and continuous numerical data.\nIdentify tidy data.\nIdentify the explanatory variable and the response variable in a pair of variables.\nDistinguish between an observational study and a randomized experiment.\nIdentify a confounding variable.\nList principles of experimental design.\nDistinguish the difference between random selection and random assignment.\nEvaluate whether conclusions drawn from a study are appropriate, given a description of the statistical study."
  },
  {
    "objectID": "pca.html#using-r-for-pca",
    "href": "pca.html#using-r-for-pca",
    "title": "16  PCA",
    "section": "\n16.2 Using R for PCA",
    "text": "16.2 Using R for PCA\nThe function gpairs() (from the gpairs package) creates a scatterplot matrix for visualizing pairwise relationships between variables. The function cor() computes a basic correlation matrix, which show all pairwise correlations between variables, and the function corrplot.mixed() (from the corrplot package) creates a better visual representation of the correlation matrix.\n\n\n\n\n\n\nR functions\n\n\n\n### gpairs( dataframe,  diag.pars = list(fontsize = 9))\n# dataframe:  Replace with the name of the tidy dataframe. \n#             All variables must be numerical.\n# diag.pars:  9 is the default font size for variables names. \n#             Decrease the number for smaller font sizes.\n#\n### cor( dataframe )\n# dataframe:  Replace with the name of the tidy dataframe. \n#             All variables must be numerical.\n#\n#\n### corrplot.mixed( `cor object` )\n# cor object:  Replace with an object\n#              created from the cor() output\n#\n\n\nThe function principal() from the psych package helps compute the required elements of PCA. By default, this function will standardized the data and conduct PCA computations on the standardized data.\n\n\n\n\n\n\nR functions\n\n\n\n### principal( dataframe, center , scale. )\n# dataframe: Replace with the name of the tidy dataframe. \n#            All variables must be numerical.\n# nfactors: Set equal to the number of variables in the data/\n# rotate:  Set equal to \"none\". \n# cor: By default, it is set to \"cor\" so that computations\n#      are done on the standarized data. Set equal to \"cov\"\n#      to conduct PCA computation on the original centered data.\n\n\nAlthough performing PCA on standardized data is not required, standardizing the data is recommended for PCA, especially when the variables in the dataset have different units or scales. Standardization transforms each variable to have a mean of 0 and a standard deviation of 1, and it prevents variables with larger scales from dominating the computed PCs.\n\n\n\n\n\n\nIllustration of PCA\n\n\n\nThe case study provided in Chapter 6 provides various plant physiological measurements for 29 different plant species. This data was used by by Pratt et al. (2021) to investigate trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field. The objective here is to use PCA to identify dominant modes of variation among these measurements that are related to xylem structure, function, and properties, as well as underlying factors or dimensions that may be driving the variability in the data.\n\n\nArctostaphylos glauca (Big Berry Manzanita). It is one of shrub species considered in the study. Source:calscape.org\n\n\nBelow is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water (\\(\\triangle \\text{Relative water content}/\\triangle \\text{MPa}\\)).\n\nThese variables provide insight into how xylem functions. The goal here is to apply PCA to determine if key patterns in these data can be identified, as well as underlying factors or dimensions that may be driving the variation in the data.\nTo begin the analysis, the data are imported and explored using numerical and graphical data summaries.\n\nCode\nlibrary( openxlsx ) # provides read.xlsx() \n\n# Import data  \nxylemDF <- read.xlsx(\"datasets/PlantWaterPhysiology.xlsx\")\n\n# obtain the  names of the variables\nnames( xylemDF )\n#>  [1] \"Species\"       \"Family\"        \"P75\"           \"Ks\"           \n#>  [5] \"starch\"        \"xylem_density\" \"fiber_%\"       \"vessel_%\"     \n#>  [9] \"par_%\"         \"Pmin\"          \"water_storage\"\n\n# For PCA, only numerical variables can be \n# present in the data.  Either create a copy\n# of the sheet with non-numerical variables\n# removed and reimport the data, or\n# remove the variables in R as shown below.\n\nlibrary(dplyr) # provides the select() function\n### select( dataframe ,'variable to remove', 'variable to remove' , ...)\n#  dataframe: Replace with the name of the tidy dataframe\n# 'variable to variable': Replace with the name of the variable remove or \n#                         the column number (negative version)\n# \n\n# create a new dataframe that only consists\n# of numerical variables. \n# Note: columns 1 and 2 denote the columns \n# we need to remove.\nxylemDFnum <- select(xylemDF, \n                     -1 ,  # remove column 1 \n                     -2 ) # remove column 2\n\n\n# summarize the variables of interest \nsummary( xylemDFnum )\n#>       P75                Ks           starch       xylem_density   \n#>  Min.   :-15.360   Min.   :0.29   Min.   : 2.210   Min.   :0.4600  \n#>  1st Qu.: -7.760   1st Qu.:0.72   1st Qu.: 3.240   1st Qu.:0.5800  \n#>  Median : -3.920   Median :1.02   Median : 4.000   Median :0.6400  \n#>  Mean   : -5.399   Mean   :1.48   Mean   : 5.069   Mean   :0.6417  \n#>  3rd Qu.: -2.500   3rd Qu.:2.17   3rd Qu.: 5.980   3rd Qu.:0.7100  \n#>  Max.   : -1.380   Max.   :3.79   Max.   :12.900   Max.   :0.7900  \n#>     fiber_%         vessel_%         par_%            Pmin       \n#>  Min.   :42.22   Min.   : 7.78   Min.   : 7.11   Min.   :-8.250  \n#>  1st Qu.:60.28   1st Qu.:14.66   1st Qu.:14.95   1st Qu.:-5.710  \n#>  Median :64.91   Median :16.87   Median :17.30   Median :-3.900  \n#>  Mean   :64.25   Mean   :17.77   Mean   :17.46   Mean   :-4.267  \n#>  3rd Qu.:67.62   3rd Qu.:18.78   3rd Qu.:20.43   3rd Qu.:-2.830  \n#>  Max.   :84.30   Max.   :35.78   Max.   :31.28   Max.   :-1.670  \n#>  water_storage  \n#>  Min.   : 5.42  \n#>  1st Qu.: 9.08  \n#>  Median :13.28  \n#>  Mean   :12.64  \n#>  3rd Qu.:14.54  \n#>  Max.   :24.61\n\n \nlibrary( gpairs ) # provides gpairs()\n\n# Create scatterplot matrix with decreased font size\ngpairs( data.frame( xylemDFnum ) ,\n         diag.pars = list(fontsize = 6) )\n\n\n\n\n\n\nCode\n# Some potential outliers seen in the matrix. Examine them more closely \n# using a boxplot and do the same for other potential cases\n\nlibrary(lattice) # provides bwplot()\nbwplot(  ~ water_storage , \n           data= xylemDFnum ) \n\n\n\n\n\n\nCode\n# compute standard correlation matrix \ncor( xylemDFnum )\n#>                      P75         Ks      starch xylem_density    fiber_%\n#> P75            1.0000000  0.6486291  0.50004669   -0.36963666 -0.4072020\n#> Ks             0.6486291  1.0000000  0.33916342   -0.26232492 -0.4217329\n#> starch         0.5000467  0.3391634  1.00000000   -0.22135129 -0.5299457\n#> xylem_density -0.3696367 -0.2623249 -0.22135129    1.00000000  0.3273450\n#> fiber_%       -0.4072020 -0.4217329 -0.52994575    0.32734503  1.0000000\n#> vessel_%       0.3151310  0.3337739  0.39992711   -0.53097026 -0.7522285\n#> par_%          0.2842809  0.1413345  0.33729586   -0.07180259 -0.7026932\n#> Pmin           0.8182375  0.5039328  0.56823143   -0.47661507 -0.4143263\n#> water_storage  0.2441534  0.2623565  0.09817602   -0.61754995 -0.2019742\n#>                 vessel_%       par_%       Pmin water_storage\n#> P75            0.3151310  0.28428093  0.8182375    0.24415343\n#> Ks             0.3337739  0.14133448  0.5039328    0.26235648\n#> starch         0.3999271  0.33729586  0.5682314    0.09817602\n#> xylem_density -0.5309703 -0.07180259 -0.4766151   -0.61754995\n#> fiber_%       -0.7522285 -0.70269315 -0.4143263   -0.20197421\n#> vessel_%       1.0000000  0.18413958  0.3928702    0.40577689\n#> par_%          0.1841396  1.00000000  0.1935726   -0.06949848\n#> Pmin           0.3928702  0.19357260  1.0000000    0.40564362\n#> water_storage  0.4057769 -0.06949848  0.4056436    1.00000000\n\n# store the correlation matrix\nM <- cor( xylemDFnum )\n\nlibrary( corrplot ) # provides corrplot.mixed()\n\ncorrplot.mixed( M )\n\n\n\n\n\n\n\nThe scatterplot and correlation matrix show that the variables have both positive and negative correlations. Many of the variables show clear linear associations, although the strength of the relationship varies depending on which variables are compared. The scatterplot matrix shows some potential outliers and such cases may be further explored using boxplots. In such cases, a transformation such as the natural log transformation may help reduce the influence of the outliers. Despite the presence of some potential outliers, we proceed with PCA without applying any transformations for the purpose of illustrating the method on this data.\nA good first step in PCA, it to compute the standard deviation for each PC and determine how many PCs should be retained. To compute the standard deviation for each PC, we conduct PCA computations on the data using prcomp().\n\nCodelibrary( psych ) # provides principal()\n# The function will require the following information:\n# dataframe: Replace with 'xylemDFnum'\n# nfactors: Set equal to 9\n# rotate: Set equal to \"none\"\n# cor: By default this is \"cor\", so it does not \n#      need to be provided.\n\n# save result of prcomp()\nxylemPCA <- principal( xylemDFnum, \n                       nfactor= 9,\n                       rotate= \"none\") # 9 variables\n\nxylemPCA\n#> Principal Components Analysis\n#> Call: principal(r = xylemDFnum, nfactors = 9, rotate = \"none\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9 h2       u2\n#> P75            0.78  0.01  0.51 -0.03 -0.14 -0.17  0.12  0.24 -0.03  1  1.1e-16\n#> Ks             0.66 -0.04  0.39 -0.56  0.17  0.11 -0.21 -0.07  0.03  1  2.2e-16\n#> starch         0.67  0.31  0.16  0.49  0.27  0.24 -0.22  0.04  0.01  1  0.0e+00\n#> xylem_density -0.63  0.52  0.29 -0.11  0.21  0.28  0.34  0.02  0.02  1  4.4e-16\n#> fiber_%       -0.78 -0.43  0.39  0.14 -0.07 -0.01 -0.08  0.06  0.14  1  8.9e-16\n#> vessel_%       0.72 -0.08 -0.49 -0.05  0.41 -0.15  0.16  0.06  0.09  1  0.0e+00\n#> par_%          0.44  0.70 -0.24 -0.07 -0.48  0.06 -0.04  0.01  0.09  1 -6.7e-16\n#> Pmin           0.81 -0.14  0.38  0.23 -0.11 -0.07  0.23 -0.23  0.02  1  6.7e-16\n#> water_storage  0.50 -0.67 -0.24 -0.03 -0.21  0.42  0.11  0.06  0.00  1  8.9e-16\n#>               com\n#> P75           2.2\n#> Ks            3.2\n#> starch        3.5\n#> xylem_density 3.9\n#> fiber_%       2.3\n#> vessel_%      2.8\n#> par_%         2.9\n#> Pmin          2.1\n#> water_storage 3.3\n#> \n#>                        PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8  PC9\n#> SS loadings           4.14 1.52 1.18 0.65 0.63 0.39 0.32 0.13 0.04\n#> Proportion Var        0.46 0.17 0.13 0.07 0.07 0.04 0.04 0.01 0.00\n#> Cumulative Var        0.46 0.63 0.76 0.83 0.90 0.95 0.98 1.00 1.00\n#> Proportion Explained  0.46 0.17 0.13 0.07 0.07 0.04 0.04 0.01 0.00\n#> Cumulative Proportion 0.46 0.63 0.76 0.83 0.90 0.95 0.98 1.00 1.00\n#> \n#> Mean item complexity =  2.9\n#> Test of the hypothesis that 9 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0 \n#>  with the empirical chi square  0  with prob <  NA \n#> \n#> Fit based upon off diagonal values = 1\n\n\nThe printing the output of xylemPCA (a principal object) provides the following:\n\nThe squared standard deviation (variance) for each PC (denoted as SS loadings in the output).\nThe loadings for each PC. Recall that these can be interpreted as the correlations between the variables and the components. They are denoted as Standardized loadings (pattern matrix) based upon correlation matrix.\nThe proportion of variance for each PC (denoted as Proportion Var).\nThe cumulative proportion of variance at a given PC (denoted as Cumulative Var)7.\n\nNote that the first PC explains 46% of the variation, the first two PCs together explain about 63% of the variation, and the first three PCs explain about 76% of the variation, and so on. The next step in PCA is to determine an acceptably large percentage of variance that is explained by the PCs.\n\n\nThere are different methods to determine how many PCs to consider for retention, including:\n\nScree plot: This plot displays the squared standard deviations (variance) explained by each principal component on the y-axis with the PCs shown on the x-axis. The variances are provided in descending order, so the amount of variance explained by each PC decreases along the x-axis. The goal is to identify an “elbow” point in the line and selects all components just before the line flattens out.\nKaiser’s rule or Kaiser-Guttman criterion: This rule retains the PC with squared standard deviations greater than or equal to 18.\n\nSimply note the size fo the variance of each PC to apply Kaiser’s rule. To construct a screeplot, we use the function fviz_eig.psych which is a modified version of function called fviz_eig from the factoextra package. fviz_eig.psych was created to work with object created from principal(), where as fviz_eig does not work with such objects.\n\n\n\n\n\n\nR functions\n\n\n\n### fviz_eig.psych( `principal object` , choice , addlabels, ncp , xlab )\n# principal object:  Replace with the name of a  \n#                    principal object.\n# choice: Determine the scale of the height of the bars. \n#         Set equal to either \"variance\" or \"eigenvalue\". \n#         Default is \"variance\". \"eigenvalue\" will provide\n#         the square standard deviations of each PC. \"variance\"\n#         will provide the amount of variance explained by each PC\n# addlabels: If set to equal to TRUE, labels are added at the top of \n#            bars or points showing the information retained by each \n#            dimension. Default is FALSE.\n# ncp: Set equal the number of PCs to display. Default is 10.  \n# xlab: Label for x-axis. Default label is \"dimensions\"\n\n\n\n\n\n\n\n\nIllustration of PCA contined\n\n\n\nHere we determine the number PCs to retain by applying Kaiser’s rule and using a scree plot. Recall that the principal object xylemPCA holds all the relevant PCA computations.\nThe y-axis can either display the squared standard deviations (variances) of the PCs or the percentage of variance explained but the shape does not change. Setting choice= \"variance\" and addlabels= TRUE allows one to apply Kaiser’s rule using the scree plot.\n\nCode# The function will require the following information:\n# dataframe: Replace with 'xylemDFnum'\n# center: By default this is true, so it is not required\n# scale.: Set equal to TRUE\n\n# Recall the principal object is `xylemPCA`\n\nsource(\"rfuns/fviz_eig.psych.R\") # provides fviz_eig.psych()\n\n# The plot displays the prop of variance \n# attributed to each PC.\n# to apply Kaiser's rule.\nfviz_eig.psych(xylemPCA, \n         choice= \"variance\" , \n         addlabels= TRUE, \n         xlab= \"Principal components\")\n\n\n\n\n\n\nCode \n\n# The plot displays the squared\n# standard deviations of each PC. \n# Can apply Kaiser's rule using this plot.\n# Note: One can also examine the squared \n# standard deviations of each PC by printing \n# xylemPCA and noting the values for \"SS loadings\".\nfviz_eig.psych(xylemPCA, \n         choice= \"eigenvalue\" , \n         addlabels= TRUE, \n         xlab= \"Principal components\")\n\n\n\n\n\n\n\nNote that the scree plot shows that the squared standard deviations of the first three PCs are greater than 1. Further note that the first three PC consist of the components just before the line flattens out. In short, both the scree plot and Kaiser’s rule suggest retaining the first 3 PCs. While Kaiser’s rule is easy to apply, PCA must be done on the correlation matrix if applying this rule. The scree plot method does not rely on standardization, but identifying the elbow point may be subjective, and others may arrive at different conclusions based on their interpretation of the scree plot.\nNext, we focus on interpreting of the PCs.\n\n\nOnce the number of PCs to retain is determined, the PCs should be examined. Since PCA was performed on the standardized data, one approach to interpreting each component is to focus on the size of the loadings of a given PC, as they represent correlations between the variables and the given PC. Here, we consider a loading at 0.7 or greater in magnitude as important.\nWe can examine the loadings by printing the principal object or use the function loadings() on principal object\n\n\n\n\n\n\nR functions\n\n\n\n### loadings( `principal object` )\n# principal object:  Replace with the name of a  \n#                    principal object.\n\n\n\n\n\n\n\n\n\nIllustration of PCA continued\n\n\n\nRecall that the principal object xylemPCA holds all the relevant PCA computations.\n\nCode# Recall the principal object is `xylemPCA`.\n# Save the result to an object\nPCAloadings <- loadings( xylemPCA )\n\n# display the loadings\nPCAloadings # loadings smaller than .1 are not displayed\n#> \n#> Loadings:\n#>               PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8    PC9   \n#> P75            0.783         0.511        -0.139 -0.175  0.122  0.243       \n#> Ks             0.661         0.394 -0.560  0.174  0.114 -0.209              \n#> starch         0.675  0.309  0.158  0.491  0.274  0.242 -0.218              \n#> xylem_density -0.630  0.521  0.287 -0.114  0.209  0.282  0.335              \n#> fiber_%       -0.781 -0.425  0.393  0.141                              0.143\n#> vessel_%       0.721        -0.492         0.408 -0.154  0.164              \n#> par_%          0.444  0.704 -0.242        -0.482                            \n#> Pmin           0.813 -0.142  0.376  0.230 -0.109         0.229 -0.234       \n#> water_storage  0.500 -0.672 -0.244        -0.210  0.422  0.112              \n#> \n#>                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9\n#> SS loadings    4.14 1.523 1.180 0.650 0.627 0.392 0.318 0.132 0.039\n#> Proportion Var 0.46 0.169 0.131 0.072 0.070 0.044 0.035 0.015 0.004\n#> Cumulative Var 0.46 0.629 0.760 0.832 0.902 0.946 0.981 0.996 1.000\n\n\nHere, the loadings among the first three PC are nearly all large or close to being large for the first PC. Such loadings means that the variables are highly correlated with the PCs, which can make it challenging to interpret the unique contribution of each variable to each PC. This makes it difficult to identify patterns or attributes in data that may have interpretations or implications in the context of the data. In such instances, one can apply a rotation on the PCs.\n\n\nIt is not uncommon for the interpretation of loadings to be challenging, particularly when the retained principal components exhibit patterns of large or small loadings, making it difficult to interpret in the context of the data. If most, if not all, of the loadings are large, this indicates that nearly all variables are highly correlated with the principal components, which can make it difficult to identify the unique contributions of individual variables to each component. On the other hand, if most, if not all, of the loadings are small, this may indicate that the data is too noisy. If all the loadings are small, one may consider lowering the threshold for what is considered important. Applying a rotation on the retained PCs can help deal with instances in which the loadings are all large or small.\nRotation transforms the retained PCs into a new set of linear combinations (i.e., leading to changes in the values of \\(e_{i1}, e_{i2}, ..., e_{ip}\\)) that may be easier to interpret. By rotating the retained PCs, a new set of loadings can be obtained that may have a simpler structure and be easier to interpret compared to the original loadings. Varimax rotation9 is one commonly used rotation method in PCA. Here, we apply varimax rotation on the retained PCs. The varimax rotated PCs will still account for the same amount of information as the unrotated retained PCs.\nTo provide a non-technical explanation of varimax rotating the PCs, recall the real estate analogy used earlier. One deciding on a set of best pictures that better highlight the most desirable or best features of a property is akin to deciding how many PCs to retain. However, some of the pictures that were selected may have similar or overlapping features, making it hard to tell them apart. Here, varimax rotation would be like the real estate agent taking the same pictures but using a different angle (“rotating them”) so that each pictures better highlights distinct features and there is minimal overlap between the features in each picture. This new set of rotated pictures provides hopefully provides clearer and better view of the property, making it easier to identify its unique features and characteristics. As before, this is just an attempt at an analogy but hopefully helps illustrate the idea of using varimax rotation on the PCs.\nThe function principal() dy defaults applies a varimax rotation to the retained PC by setting nfactors equal to then number of retained PCs and having rotate= \"varimax\" (default rotate value).\n\n\n\n\n\n\nIllustration of PCA continued – applying varimax rotation\n\n\n\nRecall Kaiser’s rule suggested keeping the first three PCs.\n\nCode# Note that nfactor is set to 3\n# since we retained 3 PCs. The argument\n# rotate is not needed since by default\n# is it \"varimax\"\nxylemPCAvmr <- principal( xylemDFnum, \n                       nfactor= 3) # The number of retained PC\n\nxylemPCAvmr\n#> Principal Components Analysis\n#> Call: principal(r = xylemDFnum, nfactors = 3)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                 RC1   RC3   RC2   h2    u2 com\n#> P75            0.91  0.16  0.14 0.87 0.126 1.1\n#> Ks             0.74  0.12  0.17 0.59 0.406 1.2\n#> starch         0.58  0.49  0.05 0.58 0.424 2.0\n#> xylem_density -0.23 -0.09 -0.83 0.75 0.249 1.2\n#> fiber_%       -0.25 -0.89 -0.29 0.95 0.054 1.4\n#> vessel_%       0.14  0.55  0.67 0.77 0.232 2.0\n#> par_%          0.13  0.84 -0.15 0.75 0.249 1.1\n#> Pmin           0.83  0.13  0.33 0.82 0.178 1.4\n#> water_storage  0.18 -0.11  0.85 0.76 0.239 1.1\n#> \n#>                        RC1  RC3  RC2\n#> SS loadings           2.60 2.13 2.12\n#> Proportion Var        0.29 0.24 0.24\n#> Cumulative Var        0.29 0.53 0.76\n#> Proportion Explained  0.38 0.31 0.31\n#> Cumulative Proportion 0.38 0.69 1.00\n#> \n#> Mean item complexity =  1.4\n#> Test of the hypothesis that 3 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.08 \n#>  with the empirical chi square  13.3  with prob <  0.35 \n#> \n#> Fit based upon off diagonal values = 0.96\n\n# xylemPCAvmr is a principal object\nvmrPCAloadings <- loadings( xylemPCAvmr )\n\n# display the loadings\nvmrPCAloadings # loadings smaller than .1 are not displayed\n#> \n#> Loadings:\n#>               RC1    RC3    RC2   \n#> P75            0.911  0.162  0.137\n#> Ks             0.742  0.121  0.168\n#> starch         0.578  0.490       \n#> xylem_density -0.233        -0.830\n#> fiber_%       -0.253 -0.892 -0.294\n#> vessel_%       0.144  0.550  0.667\n#> par_%          0.125  0.845 -0.148\n#> Pmin           0.835  0.134  0.327\n#> water_storage  0.176 -0.105  0.848\n#> \n#>                  RC1   RC3   RC2\n#> SS loadings    2.596 2.129 2.117\n#> Proportion Var 0.288 0.237 0.235\n#> Cumulative Var 0.288 0.525 0.760\n\n\nHere, the loadings among the first three PC are nearly large (>.7) for the first PC. Large loadings means that the variables are highly correlated with the PCs, which can make it challenging to interpret the unique contribution of each variable to each PC. This makes it difficult to identify patterns or attributes in data that may have interpretations or implications in the context of the data. In such instances, one can apply a rotation on the PCs.\n\n\nHere, the loadings among the first three PC are mostly all small, making it difficult to interpret the results. Recall that we consider a loading above 0.7 in magnitude as important.\nSince PCA was performed on the standarized data, one approach to interpreting the each component, we must compute the correlations between the original data and each principal component.\n** It measures variability across a single component component (i.e. spread of data within a given PC). These are also known as the square root of the eigenvalues of the correlation matrix.\n\nProportion of Variance for each PC: The amount of variance that a given PC accounts for. This is provided by taking squared standard devations (variance) of the given PC divided by sum of the variances.\nCumulative Proportion for each PC: The cumulative variation represented by PCs up until a given component.\nLoadings for each PC: They are the correlations between the variables and the components. These correlations are the coefficients of the linear combinations of the original variables that create the PCs. These are also known as the eigenvectors of the correlation matrix.\nScores: These are data observations projected onto the PCs. Scores are obtained by multiplying the loadings by the corresponding values of the original variables and then summing the results.\n\n\\(b_0= 4.90\\) and \\(b_1=-0.05\\). Note that \\(e^{b_1}=e^{-0.05}=0.95\\), which suggest that odds of having a healthy air day are estimated to decrease by 5% for each degree in temperature. This also suggests that a 5 degree increase in temperature means that the odds of having a healthy air day are estimated to decrease by 22.1% The glm() function provides much more than just these values. To extract other information, the result from glm() must be stored. Below the resulting glm() fit is stored and then the summary() is applied to this object. The functions residuals() and predict() are also illustrated below as well.\n\n\n\nNote that the summary() provides the following table:\n\n\n\nEstimate\nStd. Error\nt-value\np-value -\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(Z_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(Z_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nThe terms \\(SE_{b_0}\\) and \\(SE_{b_1}\\) are the standard errors for \\(b_0\\) and \\(b_1\\). The next sections cover inference for the SLogiR, and it requires for the assumptions of the data to be reasonable. Next, we perform model diagnostics to assess the model assumptions with the help of the stored fitted values, residuals, and the use xyplot() on the glm object after loading the tactile R package:\n\n\n\nEach plot is analyzed and interpreted below:\n\nThe plot on the left displays the predicted log odds against the explanatory variable and shows a linear trend. This suggests that Assumption 4 (linearity of log-odds) is reasonable.\nThe plot on the right shows the standardized Pearson residuals against the leverage. The presence of some potential outliers (standardized Pearson residuals greater than 3 in magnitude) is visible. Cook’s distance10 contour lines of 1/2 and 1 are plotted, but no leverage values exceed a Cook’s distance of 1. This suggests that Assumption 2 (no influential outliers) is reasonable.\n\nAssumptions 1 (independence of observations) and 3 (large sample size) are reasonable since the sample size is large, and it seems reasonable to assume that a given observation is not influenced by or related to the rest of the observations.\nOverall, all data assumptions are met for the SLogiR model.\n:::\nmaximizing the variance of each component NOW DESCRIBE FUNCTION FOR PCA…\nes, in the context of PCA, the loadings can be interpreted as the correlations between the original variables and the principal components. The loadings represent the coefficients of the linear combinations of the original variables that create the principal components.\nEach loading value indicates the contribution of the corresponding original variable to the principal component. A higher absolute value of a loading indicates that the original variable has a stronger influence on the principal component, while a lower absolute value suggests a weaker influence.\nWhen the PCA is performed on standardized data (i.e., data with a mean of 0 and a standard deviation of 1), the loadings directly represent the correlations between the original variables and the principal components. In this case, the loadings can be used to assess the relationships between the original variables and the PCs, with higher absolute values indicating stronger relationships.\nIn summary, the loadings in PCA can be seen as the correlations between the original variables and the principal components when working with standardized data, and they also represent the coefficients of the linear combinations of the original variables that create the PCs.\nThe information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal.\nIn other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.\nSay we have a set of observations that differ from each other on a number of dimensions, for example, we have a number of whiskey brands (observations) that are rated on a number of attributes such as body, sweetness, fruitiness, etc (dimensions). If some of those dimensions are strongly correlated then it should be possible to describe the observations by a smaller (than original) number of dimensions without losing too much information. For example, sweetness and fruitness could be highly correlated and could therefore be replaced by one variable. Such dimensionality reduction is the goal of principal component analysis.\nRequiring factor loadings to be larger than 0.7 would be a more stringent criterion than the typical threshold of 0.4 or 0.5. This means that only variables with very strong relationships with the underlying principal component would be retained in the analysis, and variables with weaker relationships would be dropped.\nSetting a high threshold for factor loadings can be useful in some cases, such as when the research question is very specific and requires a high degree of confidence in the identified relationships between variables and principal components. However, it can also result in a loss of information and a reduction in the number of variables in the analysis, which may limit the ability to identify more subtle patterns or structures in the data.\nIn general, the choice of threshold for factor loadings depends on the research question, the nature of the data, and the goals of the analysis. It is important to carefully consider the implications of setting a high threshold for factor loadings and to balance the need for specificity and confidence with the desire to retain as much information as possible in the analysis. .\nhttps://rpubs.com/samopolo/746605 <– good rules\nhttps://bookdown.org/egarpor/SSS2-UC3M/pca-examps.html <-biplot explained a bitul\n\n\n\n\nPratt, RB, AL Jacobsen, MI Percolla, ME De Guzman, CA Traugh, and MF Tobin. 2021. “Trade-Offs Among Transport, Support, and Storage in Xylem from Shrubs in a Semiarid Chaparral Environment Tested with Structural Equation Modeling.” Proceedings of the National Academy of Sciences 118 (33)."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "7  Basics of data",
    "section": "",
    "text": "AI art generated from the text “data”"
  },
  {
    "objectID": "basics.html#variables-and-observations",
    "href": "basics.html#variables-and-observations",
    "title": "7  Basics of data",
    "section": "\n7.1 Variables and observations",
    "text": "7.1 Variables and observations\nThe following are some important data basics:\n\nA variable is any characteristic whose value may change from one individual (or object) to another.\nData result from making observations on a single variable (called univariate data) or simultaneously on two or more variables (called multivariate data).\n\n\n\n\n\n\n\nNote\n\n\n\nRecall the data given in Chapter 1. Each row in the data table above represents a single early-stage Delta smelt larvae in the sample. The formal name for a row is a case or observational unit. The columns represent characteristics of interest for each early-stage Delta smelt larvae, where each column is referred to as a variable. Here, the only characteristics that are of concern for each larvae are the level of exposure of light intensity (Light), turbidity level exposure (Turbidity), and the percentage of larvae that survived. For example, the first row represents a early-stage Delta smelt larvae that was exposed to a light intensity of 4.5 \\(\\mu mol/m^2/s\\) and a turbidity level of 2 NTUs, where only 56.8% of larvae survived under these conditions."
  },
  {
    "objectID": "basics.html#types-of-variables",
    "href": "basics.html#types-of-variables",
    "title": "7  Basics of data",
    "section": "\n7.2 Types of variables",
    "text": "7.2 Types of variables\nVariables in a statistical study can be classified as either categorical or numerical.\n\nA variable is categorical if the individual observations are categorical responses, such as labels, people’s attributes, opinions, etc.\nA variable is numerical (or quantitative) if its corresponding observations are a numerical values that are measured or counted.\n\nFactor variables generally refers to the software’s treatment of categorical variables. The levels of a factor variable refers to the value that a categorical variable takes on. For example, the variable “first-generation college student” may take on levels 0 and 1, with 1 representing a student who is first-generation college student born before 1999 and 0 representing a student whose is not a first-generation college student. Factor variable will be used interchangeably with categorical variable. If the levels of a factor variables have a natural ordering, it is called an ordinal variable, while a factor variable without this type of special ordering is called a nominal variable. For readability, factor variables will be treated as a nominal factor variables in this resource.\n\n\n\n\n\n\nNote\n\n\n\nThe case study data given in Chapter 1 frame consist of three variables labeled as Light, Turbidity, and Survival in the data set.\n\nSurvival is a continuous numerical variable. These observations can take a wide range of values. A numerical variable is continuous if its possible values form an entire interval on the number line.\nLight and Turbidity are both factor variables, with each having levels low, medium, and high.\n\n\n\n\n\nFigure 7.1: Breakdown of variables into their respective types\n\n\n\n\n\n\n\n\nR and factor variables\n\n\n\nWhen data are imported (described in Chapter C) that contains factor variables, R may not automatically recognize them as such1. If the levels of the factor variables are represented by numbers, R by default, classifies these variables as numeric. If the levels are instead letters or words, R will treat them as character variables by default.\nTo convert a variable to a factor variable, you can use the mutate() function in the dplyr package.\n\n\n\n\n\n\n\n\nR functions\n\n\n\n### mutate( data ,\n###        'new variable' = 'function of variable in data frame', ...)\n# data: Replace with the name of the dataframe being used.\n# 'new variable' = 'function of variable in data frame': Replace \n#                 'new variable' with name of desired variable \n#                  and set equal to a function of \n#                  a current variable in the  dataframe. \n#                  This will add the new variable to the \n#                  dataframe.  The 'new variable' name may \n#                  also be replaced with the name of a current \n#                  variable as this will override/replace the \n#                  current variable.\n#...: additional new variables can be added to the dataframe. \n#\n### summary( data )\n# Note: This function provides a summary of each numerical variable \n# in the data frame, including the five-number summary (minimum, \n# lower quartile, median, upper quartile, maximum) and the number \n# of non-missing observations. For factor variables, the function \n# provides the count of each level of the factor variable.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe case study data given in Chapter 1 consist of two factor variables. The code below instructs R that Light and Turbidity are factor variables by using the function mutate(). Combined with mutate(), the function as.factor() converts a variable into a factor variable\n\n\nR code\nVideo\n\n\n\n\nCode# Import the dataset 'dssurv' from a csv file\ndssurv <- read.csv(\"datasets/dssurv.csv\")\n \n# Print a summary of the dataset, including the \n# five-number summary for numeric variables and counts \n# of observations for categorical variables.\nsummary( dssurv )  \n#>     Light            Turbidity            Survival    \n#>  Length:20          Length:20          Min.   :45.02  \n#>  Class :character   Class :character   1st Qu.:55.79  \n#>  Mode  :character   Mode  :character   Median :59.56  \n#>                                        Mean   :62.22  \n#>                                        3rd Qu.:67.24  \n#>                                        Max.   :89.09\n\n# Load the 'dplyr' package which provides the 'mutate()' function\nlibrary( dplyr ) \n\n# Replace `Light` and `Turbidity` with the factor\n# versions. The code below tells R to convert  \n# `Light` and `Turbidity` into factor variables.\ndssurv <- mutate( dssurv, \n                  Light= as.factor( Light ) ,\n                  Turbidity= as.factor( Turbidity ) )\n\n# Print a new summary of the updated dataset.\nsummary( dssurv )  \n#>   Light   Turbidity    Survival    \n#>  high:8   high:8    Min.   :45.02  \n#>  low :8   low :8    1st Qu.:55.79  \n#>  med :4   med :4    Median :59.56  \n#>                     Mean   :62.22  \n#>                     3rd Qu.:67.24  \n#>                     Max.   :89.09"
  },
  {
    "objectID": "basics.html#sec-csv",
    "href": "basics.html#sec-csv",
    "title": "7  Basics of data",
    "section": "\n7.3 .csv files",
    "text": "7.3 .csv files\nTo provide a consistent approach to using R, it is assumed that any research data will be saved in the tidy format using a comma separated values (.csv) file. A .csv may be viewed with a spread sheet program such as Microsoft Excel. It is assumed readers have used an excel spreadsheet. If not, please view the tutorial Excel Tutorial for Beginners .\nTo store data in a .csv file, organize your data in an Excel worksheet, such that the first row (Row 1) contains the column (variable) names and each subsequent row contains all the necessary information for each data point. Next, save the worksheet as a .csv file type.\n\n\nSample .csv file in tidy format"
  },
  {
    "objectID": "basics.html#tidy-data",
    "href": "basics.html#tidy-data",
    "title": "7  Basics of data",
    "section": "\n7.4 Tidy data",
    "text": "7.4 Tidy data\nNote the format of the data presented in Chapter 1:\n\nEach variable forms a column.\nEach observation forms a row.\nEach value must have its own cell.\n\nThen data are stored in this format, it is called tidy data (Wickham 2014).\n\n\nFigure 7.2: Tidy data format. source: Wickham and Grolemund (2016)\n\n\nThe R code below imports two versions of the data (a tidy and non-tidy version). The R function read.csv() is described in Chapter C. The functionhead() (tail()) will print out the first (last) few rows of a data frame.\n\n\n\n\n\n\nR functions\n\n\n\n### head( data , n )\n# data: Replace with the name of the dataframe\n# n: Set equal to a number.  The first 'n' rows will be printed\n#\n### tail( data , n )\n# Note: It will print the last 'n' rows.\n#\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code below imports the data (tidy and non-tidy format) for the case study given in Chapter 4.\n\n\nR code\nVideo\n\n\n\n\nCode# Import tidy data from a csv file and store it in a dataframe called \"bmidf\"\nbmidf <- read.csv( \"datasets/BMIcsdata.csv\" )\n\n# Import non-tidy data from a csv file and store it in a dataframe called \"bmidfnt\"\nbmidfnt <- read.csv( \"datasets/BMIcsdataNT.csv\" )\n\n# View the first 5 rows of the \"bmidf\" dataframe.\nhead( bmidf , \n     n= 5 )  \n#>       Country   Sex Region Year  BMI\n#> 1 Afghanistan   Men  Rural 1985 19.7\n#> 2 Afghanistan   Men  Urban 1985 22.4\n#> 3 Afghanistan   Men  Rural 2017 22.5\n#> 4 Afghanistan   Men  Urban 2017 23.6\n#> 5 Afghanistan Women  Rural 1985 20.1\n\n# View the last 5 rows of the \"bmidf\" dataframe.\nhead( bmidfnt , \n     n= 5 ) \n#>       Country   Sex Region yr.1985 yr.2017\n#> 1 Afghanistan   Men  Rural    19.7    22.5\n#> 2 Afghanistan   Men  Urban    22.4    23.6\n#> 3 Afghanistan Women  Rural    20.1    23.6\n#> 4 Afghanistan Women  Urban    23.2    26.3\n#> 5     Albania   Men  Rural    25.0    26.9\n\n\nThe tidy format make a clear distinction between a variable, an observation and a value. The format of the second data set may be useful for quickly observing some year to year changes. However, this format violates the tidy form because information regarding year appears both in the rows and columns. Tidy data helps to think about a data set in terms of variables and their observed values."
  },
  {
    "objectID": "basics.html#codebooks",
    "href": "basics.html#codebooks",
    "title": "7  Basics of data",
    "section": "\n7.5 Codebooks",
    "text": "7.5 Codebooks\nAn important task in a data based research project is to document the structure and content of project dataset. This can be done in the form of a codebook, a file that lists at least the names of the variables, variable descriptions, and data source. Sometimes it may also provide the variables’ type (e.g., continuous, categorical) and levels of factor variables (e.g., 1 = disagree,…, 3 = agree). Writing a codebook is an important step in the managing the data aspect of your research project. The codebook will serve as an important reference for the researcher and any collaborators, and there are multiple approaches for creating a codebook, with Excel being a popular choice. The next section contains a sample codebook for the case study data.\n\n\n\n\n\n\nNote\n\n\n\nHere, a codebook is provided for the data given in Chapter 1.\nCodebook\n\n\n\nThe Delta smelt case study data frame has the following dimensions:\n\n\nFeature\nResult\n\n\n\nNumber of observations\n20\n\n\nNumber of variables\n3\n\n\n\nCodebook summary table\n\n\n\n\n\n\n\n\n\n\nLabel\nVariable\nClass\n# Unique values\nMissing\nDescription\n\n\n\nlow dosage (4.5 micromols per square meter per second), medium dosage (6.75 micromols per square meter per second), high dosage (9 micromols per square meter per second)\n[Light]\nfactor\n3\n0.00 %\n\n\n\nlow dosage (2 NTUs), medium dosage (5.5 NTUs), high dosage (9 NTUs)\n[Turbidity]\nfactor\n3\n0.00 %\n\n\n\nPercentage of early-stage Delta smelt larvae (0-40 days post hatch) that survived\n[Survival]\nnumeric\n20\n0.00 %\n\n\n\nVariable list\nLight\nlow dosage (4.5 micromols per square meter per second), medium dosage (6.75 micromols per square meter per second), high dosage (9 micromols per square meter per second)\n\n\nFeature\nResult\n\n\n\nVariable type\nfactor\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3\n\n\nMode\n“low”\n\n\nReference category\nlow\n\n\n\n\n\n\n\n\n\n\n\n\nObserved factor levels: “high”, “low”, “med”.\nTurbidity\nlow dosage (2 NTUs), medium dosage (5.5 NTUs), high dosage (9 NTUs)\n\n\nFeature\nResult\n\n\n\nVariable type\nfactor\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3\n\n\nMode\n“low”\n\n\nReference category\nlow\n\n\n\n\n\n\n\n\n\n\n\n\nObserved factor levels: “high”, “low”, “med”.\nSurvival\nPercentage of early-stage Delta smelt larvae (0-40 days post hatch) that survived\n\n\nFeature\nResult\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n20\n\n\nMedian\n59.56\n\n\n1st and 3rd quartiles\n55.79; 67.24\n\n\nMin. and max.\n45.02; 89.09\n\n\n\n\n\n\n\n\n\n\n\nReport generation information:\n\nCreated by: Eduardo L. Montoya (email:emontoya2@csub.edu).\nSource: The data are a subset of the data provided by Dr. Tien-Chieh Hung. The data are a subset of the data analyzed in Tigan et al. (2020).\nCodebook created using dataMaid v1.4.1 [Pkg: 2021-10-08 from CRAN (R 4.1.3)]\nR version 4.1.2 (2021-11-01).\nPlatform: x86_64-w64-mingw32/x64 (64-bit)(Windows 10 x64 (build 22000)).\n\n\n\n\n\n\n\nTigan, Galen, William Mulvaney, Luke Ellison, Andrew Schultz, and Tien-Chieh Hung. 2020. “Effects of Light and Turbidity on Feeding, Growth, and Survival of Larval Delta Smelt (Hypomesus Transpacificus, Actinopterygii, Osmeridae).” Hydrobiologia 847 (13): 2883–94. https://doi.org/10.1007/s10750-020-04280-4.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "basics.html#sample-codebook-for-case-study-data",
    "href": "basics.html#sample-codebook-for-case-study-data",
    "title": "7  Basics of data",
    "section": "\n7.6 Sample codebook for case study data",
    "text": "7.6 Sample codebook for case study data\n\n\n\nThe Delta smelt case study data frame has the following dimensions:\n\n\nFeature\nResult\n\n\n\nNumber of observations\n20\n\n\nNumber of variables\n3\n\n\n\nCodebook summary table\n\n\n\n\n\n\n\n\n\n\nLabel\nVariable\nClass\n# Unique values\nMissing\nDescription\n\n\n\nlow dosage (4.5 micromols per square meter per second), medium dosage (6.75 micromols per square meter per second), high dosage (9 micromols per square meter per second)\n[Light]\nfactor\n3\n0.00 %\n\n\n\nlow dosage (2 NTUs), medium dosage (5.5 NTUs), high dosage (9 NTUs)\n[Turbidity]\nfactor\n3\n0.00 %\n\n\n\nPercentage of early-stage Delta smelt larvae (0-40 days post hatch) that survived\n[Survival]\nnumeric\n20\n0.00 %\n\n\n\nVariable list\nLight\nlow dosage (4.5 micromols per square meter per second), medium dosage (6.75 micromols per square meter per second), high dosage (9 micromols per square meter per second)\n\n\nFeature\nResult\n\n\n\nVariable type\nfactor\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3\n\n\nMode\n“low”\n\n\nReference category\nlow\n\n\n\n\n\n\n\n\n\n\n\n\nObserved factor levels: “high”, “low”, “med”.\nTurbidity\nlow dosage (2 NTUs), medium dosage (5.5 NTUs), high dosage (9 NTUs)\n\n\nFeature\nResult\n\n\n\nVariable type\nfactor\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3\n\n\nMode\n“low”\n\n\nReference category\nlow\n\n\n\n\n\n\n\n\n\n\n\n\nObserved factor levels: “high”, “low”, “med”.\nSurvival\nPercentage of early-stage Delta smelt larvae (0-40 days post hatch) that survived\n\n\nFeature\nResult\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n20\n\n\nMedian\n59.56\n\n\n1st and 3rd quartiles\n55.79; 67.24\n\n\nMin. and max.\n45.02; 89.09\n\n\n\n\n\n\n\n\n\n\n\nReport generation information:\n\nCreated by: Eduardo L. Montoya (email:emontoya2@csub.edu).\nSource: The data is a subset of the data provided by Dr. Tien-Chieh Hung. The data is a subset of the data analyzed in\nCodebook created using dataMaid v1.4.1 [Pkg: 2021-10-08 from CRAN (R 4.1.3)]\nR version 4.1.2 (2021-11-01).\nPlatform: x86_64-w64-mingw32/x64 (64-bit)(Windows 10 x64 (build 22000)).\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "datacs6.html#sec-csshrub",
    "href": "datacs6.html#sec-csshrub",
    "title": "6  Case study 6",
    "section": "\n6.1 A Case Study: Exploring Xylem Characteristics in Shrub Species",
    "text": "6.1 A Case Study: Exploring Xylem Characteristics in Shrub Species\nIntroduction\nIn plants, the function of the xylem tissue is to transport water and minerals, and this forms part of a network that ensures that the plant can transport essential resources and maintain its growth and development. A study by Pratt et al. (2021) investigated the trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field. The study, which used structural equation modeling, showed the importance of understanding that the roles of different cell types in the xylem are important for understanding the functional trade-offs that govern xylem traits, and emphasized the critical role of minimum hydrostatic pressures in plant growth and development.\nFor their study, various plant physiological measurements for 29 different plant species were collected. Below is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water \\(\\Delta \\text{Relative water content}/\\Delta \\text{MPa}\\).\n\nThese variables provide insight on how the xylem functions.\nHere, we apply principal component analysis (PCA) to determine if we can identify key patterns in these data, which describe various aspects related to the structure, function, and properties of xylem, as well as underlying factors or dimensions that may be driving the variation in the data. The data is shown in the table below.\n\n\n\n\n\n\n\n\nSummary of findings\nNumerical and graphical summaries of the plant physiological variables are provided below:\n\n\n\n\n\n(a) Scatterplot matrix showing the pairwise relationships between the plant physiological variables in the dataset\n\n\n\n\n\n\n\n\n(b) correlation matrix table that shows the correlation coefficients between each pair of plant physiological variables in the dataset.\n\n\n\n\nFigure 6.1: Scatterplot matrix and correlation matrix of the plant physiological variables.\n\n\nThe graphical and numerical summaries show that the variables have both positive and negative correlations, with many showing clear linear associations, although the strength of a given linear relationship depends on which pair of variables is considered. There are 84 possible 3D scatterplots that could be examined when dealing with the nine variables, so it is difficult to examine beyond pairwise relationships. That is, graphical or numerical summaries provide limited insight when attempting to explore the interdynamics of the variables when the data set is very large.\nTo better understand the characteristics of xylem tissue in shrub species, it is important to identify the underlying factors or dimensions that drive the variation in the data. By identifying these factors, we can more easily interpret the data set, potentially revealing underlying patterns, and gain insights into how these shrubs respond to water availability in a semi-arid chaparral environment. Principal component analysis (PCA) is a useful tool for achieving this goal. In this case study, PCA is applied to the plant physiological measurements to simplify the interpretation of the variables and reveal the underlying patterns that drive the variation in the data\nAccording to Kaiser’s rule, three principal components capture most of the variation contained in these nine variables. In fact, about 76% of the variation in the plant physiological variables is explained by the first three principal components, and this is an acceptably large percentage.  \n\n\n\nTo simplify the interpretation of the principal components, varimax rotation is applied. Overall, the first principal component (PC) is viewed as an indicator of a shrub’s adaptability to drought conditions with respect to water efficiency, while the second PC is viewed as an indicator of a shrub’s adaptability to drought conditions with respect to water storage. The third PC, on the other hand, is an indicator of a shrub’s adaptability to drought conditions with respect to water transport. These findings provide a view into how these shrubs respond to water availability in a semi-arid chaparral environment.\nIt is important to note that PCA is an exploratory method rather than an inferential one. While it can reveal patterns and relationships in the data, it does not provide statistically significant conclusions or results. Overall, PCA is a powerful method for exploring complex datasets and revealing underlying patterns, but it should not be used as the sole basis for making conclusions.\n\n\n\n\nPratt, RB, AL Jacobsen, MI Percolla, ME De Guzman, CA Traugh, and MF Tobin. 2021. “Trade-Offs Among Transport, Support, and Storage in Xylem from Shrubs in a Semiarid Chaparral Environment Tested with Structural Equation Modeling.” Proceedings of the National Academy of Sciences 118 (33)."
  },
  {
    "objectID": "pca.html#r-functions",
    "href": "pca.html#r-functions",
    "title": "16  PCA",
    "section": "\n16.3 R functions",
    "text": "16.3 R functions\n### prcomp( y ~ x , data , family )\n# y: replace y with the name of the the binary resposne \n#    variable of interest\n# x: replace x with the name of the explantory variable\n# data: set equal to a dataframe that is in tidy form\n# family: set equal to \"binomial\"\n#\n\n\n\n\n\n\nNote\n\n\n\nThe case study Chapter 6 provides various plant physiological measurements for 29 different plant species. This data was used by by Pratt et al. (2021) to investigat trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field.\n\n\nFigure 16.4: Arctostaphylos glauca (Big Berry Manzanita). It is one of shrub species considered in the study. Source:calscape.org\n\n\nBelow is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water \\(\\Delta \\text{Relative water content}/\\Delta \\text{MPa}\\).\nThese variables provide insight on how the xylem functions. The goal here is to apply PCA to determine if key patterns in these data can be identified, as well as underlying factors or dimensions that may be driving the variation in the data.\n\nTo begin the analysis, the data is imported and explored using numerical and graphical data summaries.\n\nCode\nlibrary( openxlsx ) # provides read.xlsx() \n\n# Import data `\nxylemDF <- read.xlsx(\"datasets/PlantWaterPhysiology.xlsx\")\n\n# obtain the  names of the variables\nnames( xylemDF )\n#>  [1] \"Species\"       \"Family\"        \"P75\"           \"Ks\"           \n#>  [5] \"starch\"        \"xylem_density\" \"fiber_%\"       \"vessel_%\"     \n#>  [9] \"par_%\"         \"Pmin\"          \"water_storage\"\n\n# For PCA, only numerical variables can be \n# present in the data.  Either create a copy\n# of the sheet with non-numerical variables\n# removed and reimport the data, or\n# remove the variables in R as shown below.\n\nlibrary(dplyr) # provides the select() function\n### select( dataframe ,'variable to remove', 'variable to remove' , ...)\n#  dataframe: Replace with the name of the tidy dataframe\n# 'variable to variable': Replace with the name of the variable remove or \n#                         the column number (negative version)\n# \n\n# create a new dataframe that only consists\n# of numerical variables. \n# Note: columns 1 and 2 denote the columns \n# we need to remove.\nxylemDFnum <- select(xylemDF, \n                     -1 ,  # remove column 1 \n                     -2 ) # remove column 2\n\n\n# summarize the variables of interest \nsummary( xylemDFnum )\n#>       P75                Ks           starch       xylem_density   \n#>  Min.   :-15.360   Min.   :0.29   Min.   : 2.210   Min.   :0.4600  \n#>  1st Qu.: -7.760   1st Qu.:0.72   1st Qu.: 3.240   1st Qu.:0.5800  \n#>  Median : -3.920   Median :1.02   Median : 4.000   Median :0.6400  \n#>  Mean   : -5.399   Mean   :1.48   Mean   : 5.069   Mean   :0.6417  \n#>  3rd Qu.: -2.500   3rd Qu.:2.17   3rd Qu.: 5.980   3rd Qu.:0.7100  \n#>  Max.   : -1.380   Max.   :3.79   Max.   :12.900   Max.   :0.7900  \n#>     fiber_%         vessel_%         par_%            Pmin       \n#>  Min.   :42.22   Min.   : 7.78   Min.   : 7.11   Min.   :-8.250  \n#>  1st Qu.:60.28   1st Qu.:14.66   1st Qu.:14.95   1st Qu.:-5.710  \n#>  Median :64.91   Median :16.87   Median :17.30   Median :-3.900  \n#>  Mean   :64.25   Mean   :17.77   Mean   :17.46   Mean   :-4.267  \n#>  3rd Qu.:67.62   3rd Qu.:18.78   3rd Qu.:20.43   3rd Qu.:-2.830  \n#>  Max.   :84.30   Max.   :35.78   Max.   :31.28   Max.   :-1.670  \n#>  water_storage  \n#>  Min.   : 5.42  \n#>  1st Qu.: 9.08  \n#>  Median :13.28  \n#>  Mean   :12.64  \n#>  3rd Qu.:14.54  \n#>  Max.   :24.61\n\n\n\nlibrary( gpairs ) # provides gpairs \n\n#Create scatterplot matrix with decreased font size\ngpairs( data.frame( xylemDFnum ) ,\n        ,  diag.pars = list(fontsize = 6) )\n\n\n\n\n\n\nCode\n# compute correlation matrix \ncor( xylemDFnum )\n#>                      P75         Ks      starch xylem_density    fiber_%\n#> P75            1.0000000  0.6486291  0.50004669   -0.36963666 -0.4072020\n#> Ks             0.6486291  1.0000000  0.33916342   -0.26232492 -0.4217329\n#> starch         0.5000467  0.3391634  1.00000000   -0.22135129 -0.5299457\n#> xylem_density -0.3696367 -0.2623249 -0.22135129    1.00000000  0.3273450\n#> fiber_%       -0.4072020 -0.4217329 -0.52994575    0.32734503  1.0000000\n#> vessel_%       0.3151310  0.3337739  0.39992711   -0.53097026 -0.7522285\n#> par_%          0.2842809  0.1413345  0.33729586   -0.07180259 -0.7026932\n#> Pmin           0.8182375  0.5039328  0.56823143   -0.47661507 -0.4143263\n#> water_storage  0.2441534  0.2623565  0.09817602   -0.61754995 -0.2019742\n#>                 vessel_%       par_%       Pmin water_storage\n#> P75            0.3151310  0.28428093  0.8182375    0.24415343\n#> Ks             0.3337739  0.14133448  0.5039328    0.26235648\n#> starch         0.3999271  0.33729586  0.5682314    0.09817602\n#> xylem_density -0.5309703 -0.07180259 -0.4766151   -0.61754995\n#> fiber_%       -0.7522285 -0.70269315 -0.4143263   -0.20197421\n#> vessel_%       1.0000000  0.18413958  0.3928702    0.40577689\n#> par_%          0.1841396  1.00000000  0.1935726   -0.06949848\n#> Pmin           0.3928702  0.19357260  1.0000000    0.40564362\n#> water_storage  0.4057769 -0.06949848  0.4056436    1.00000000\n\n# store the correlation matrix\nM <- cor( xylemDFnum )\n\nlibrary( corrplot ) # provides corrplot.mixed()\n\ncorrplot.mixed(M )\n\n\n\n\n\n\n\nThe boxplots and summary statistics show that temperatures tend to be lower on healthy air days compared to unhealthy air days. The mean temperatures for healthy air days are also lower than those for unhealthy air days. Additionally, temperatures exhibit more variability on healthy air days compared to unhealthy air days. This may indicate more extreme temperatures on healthy air days or suggest that there are other factors contributing to the variability such as wind speed or humidity.\nThe SLogiR model is fitted using glm() but note that the response variable must be binary (such as type_of_day01):\n\nCode# The function will require the following information:\n# y: replace with 'type_of_day01'\n# x: replace with 'temp'\n# data: set equal to a 'kernAQIdf'\n# family: set equal to \"binomial\"\n\n##glm( type_of_day01 ~ temp ,\n  #  data = AQIdf , \n   # family = \"binomial\" )\n\n\nThe output provides \\(b_0= 4.90\\) and \\(b_1=-0.05\\). Note that \\(e^{b_1}=e^{-0.05}=0.95\\), which suggest that odds of having a healthy air day are estimated to decrease by 5% for each degree in temperature. This also suggests that a 5 degree increase in temperature means that the odds of having a healthy air day are estimated to decrease by 22.1% The glm() function provides much more than just these values. To extract other information, the result from glm() must be stored. Below the resulting glm() fit is stored and then the summary() is applied to this object. The functions residuals() and predict() are also illustrated below as well.\n\n\n\nNote that the summary() provides the following table:\n\n\n\nEstimate\nStd. Error\nt-value\np-value -\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(Z_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(Z_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nThe terms \\(SE_{b_0}\\) and \\(SE_{b_1}\\) are the standard errors for \\(b_0\\) and \\(b_1\\). The next sections cover inference for the SLogiR, and it requires for the assumptions of the data to be reasonable. Next, we perform model diagnostics to assess the model assumptions with the help of the stored fitted values, residuals, and the use xyplot() on the glm object after loading the tactile R package:\n\n\n\nEach plot is analyzed and interpreted below:\n\nThe plot on the left displays the predicted log odds against the explanatory variable and shows a linear trend. This suggests that Assumption 4 (linearity of log-odds) is reasonable.\nThe plot on the right shows the standardized Pearson residuals against the leverage. The presence of some potential outliers (standardized Pearson residuals greater than 3 in magnitude) is visible. Cook’s distance7 contour lines of 1/2 and 1 are plotted, but no leverage values exceed a Cook’s distance of 1. This suggests that Assumption 2 (no influential outliers) is reasonable.\n\nAssumptions 1 (independence of observations) and 3 (large sample size) are reasonable since the sample size is large, and it seems reasonable to assume that a given observation is not influenced by or related to the rest of the observations.\nOverall, all data assumptions are met for the SLogiR model.\n\n\nmaximizing the variance of each component NOW DESCRIBE FUNCTION FOR PCA…\nes, in the context of PCA, the loadings can be interpreted as the correlations between the original variables and the principal components. The loadings represent the coefficients of the linear combinations of the original variables that create the principal components.\nEach loading value indicates the contribution of the corresponding original variable to the principal component. A higher absolute value of a loading indicates that the original variable has a stronger influence on the principal component, while a lower absolute value suggests a weaker influence.\nWhen the PCA is performed on standardized data (i.e., data with a mean of 0 and a standard deviation of 1), the loadings directly represent the correlations between the original variables and the principal components. In this case, the loadings can be used to assess the relationships between the original variables and the PCs, with higher absolute values indicating stronger relationships.\nIn summary, the loadings in PCA can be seen as the correlations between the original variables and the principal components when working with standardized data, and they also represent the coefficients of the linear combinations of the original variables that create the PCs.\nThe information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal.\nIn other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.\nSay we have a set of observations that differ from each other on a number of dimensions, for example, we have a number of whiskey brands (observations) that are rated on a number of attributes such as body, sweetness, fruitiness, etc (dimensions). If some of those dimensions are strongly correlated then it should be possible to describe the observations by a smaller (than original) number of dimensions without losing too much information. For example, sweetness and fruitness could be highly correlated and could therefore be replaced by one variable. Such dimensionality reduction is the goal of principal component analysis.\nRequiring factor loadings to be larger than 0.7 would be a more stringent criterion than the typical threshold of 0.4 or 0.5. This means that only variables with very strong relationships with the underlying principal component would be retained in the analysis, and variables with weaker relationships would be dropped.\nSetting a high threshold for factor loadings can be useful in some cases, such as when the research question is very specific and requires a high degree of confidence in the identified relationships between variables and principal components. However, it can also result in a loss of information and a reduction in the number of variables in the analysis, which may limit the ability to identify more subtle patterns or structures in the data.\nIn general, the choice of threshold for factor loadings depends on the research question, the nature of the data, and the goals of the analysis. It is important to carefully consider the implications of setting a high threshold for factor loadings and to balance the need for specificity and confidence with the desire to retain as much information as possible in the analysis. .\nhttps://rpubs.com/samopolo/746605 <– good rules\nhttps://bookdown.org/egarpor/SSS2-UC3M/pca-examps.html <-biplot explained a bitul\n\n\n\n\nPratt, RB, AL Jacobsen, MI Percolla, ME De Guzman, CA Traugh, and MF Tobin. 2021. “Trade-Offs Among Transport, Support, and Storage in Xylem from Shrubs in a Semiarid Chaparral Environment Tested with Structural Equation Modeling.” Proceedings of the National Academy of Sciences 118 (33)."
  },
  {
    "objectID": "pca.html#r-functions-1",
    "href": "pca.html#r-functions-1",
    "title": "16  PCA",
    "section": "\n16.3 R functions",
    "text": "16.3 R functions\n### prcomp( y ~ x , data , family )\n# y: replace y with the name of the the binary resposne \n#    variable of interest\n# x: replace x with the name of the explantory variable\n# data: set equal to a dataframe that is in tidy form\n# family: set equal to \"binomial\"\n#\n\n\n\n\n\n\nNote\n\n\n\nThe case study Chapter 6 provides various plant physiological measurements for 29 different plant species. This data was used by by Pratt et al. (2021) to investigat trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field.\n\n\nArctostaphylos glauca (Big Berry Manzanita). It is one of shrub species considered in the study. Source:calscape.org\n\n\nBelow is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water \\(\\Delta \\text{Relative water content}/\\Delta \\text{MPa}\\).\nThese variables provide insight on how the xylem functions. The goal here is to apply PCA to determine if key patterns in these data can be identified, as well as underlying factors or dimensions that may be driving the variation in the data.\n\nTo begin the analysis, the data is imported and explored using numerical and graphical data summaries.\n\nCode\nlibrary( openxlsx ) # provides read.xlsx() \n\n# Import data `\nxylemDF <- read.xlsx(\"datasets/PlantWaterPhysiology.xlsx\")\n\n# obtain the  names of the variables\nnames( xylemDF )\n#>  [1] \"Species\"       \"Family\"        \"P75\"           \"Ks\"           \n#>  [5] \"starch\"        \"xylem_density\" \"fiber_%\"       \"vessel_%\"     \n#>  [9] \"par_%\"         \"Pmin\"          \"water_storage\"\n\n# For PCA, only numerical variables can be \n# present in the data.  Either create a copy\n# of the sheet with non-numerical variables\n# removed and reimport the data, or\n# remove the variables in R as shown below.\n\nlibrary(dplyr) # provides the select() function\n### select( dataframe ,'variable to remove', 'variable to remove' , ...)\n#  dataframe: Replace with the name of the tidy dataframe\n# 'variable to variable': Replace with the name of the variable remove or \n#                         the column number (negative version)\n# \n\n# create a new dataframe that only consists\n# of numerical variables. \n# Note: columns 1 and 2 denote the columns \n# we need to remove.\nxylemDFnum <- select(xylemDF, \n                     -1 ,  # remove column 1 \n                     -2 ) # remove column 2\n\n\n# summarize the variables of interest \nsummary( xylemDFnum )\n#>       P75                Ks           starch       xylem_density   \n#>  Min.   :-15.360   Min.   :0.29   Min.   : 2.210   Min.   :0.4600  \n#>  1st Qu.: -7.760   1st Qu.:0.72   1st Qu.: 3.240   1st Qu.:0.5800  \n#>  Median : -3.920   Median :1.02   Median : 4.000   Median :0.6400  \n#>  Mean   : -5.399   Mean   :1.48   Mean   : 5.069   Mean   :0.6417  \n#>  3rd Qu.: -2.500   3rd Qu.:2.17   3rd Qu.: 5.980   3rd Qu.:0.7100  \n#>  Max.   : -1.380   Max.   :3.79   Max.   :12.900   Max.   :0.7900  \n#>     fiber_%         vessel_%         par_%            Pmin       \n#>  Min.   :42.22   Min.   : 7.78   Min.   : 7.11   Min.   :-8.250  \n#>  1st Qu.:60.28   1st Qu.:14.66   1st Qu.:14.95   1st Qu.:-5.710  \n#>  Median :64.91   Median :16.87   Median :17.30   Median :-3.900  \n#>  Mean   :64.25   Mean   :17.77   Mean   :17.46   Mean   :-4.267  \n#>  3rd Qu.:67.62   3rd Qu.:18.78   3rd Qu.:20.43   3rd Qu.:-2.830  \n#>  Max.   :84.30   Max.   :35.78   Max.   :31.28   Max.   :-1.670  \n#>  water_storage  \n#>  Min.   : 5.42  \n#>  1st Qu.: 9.08  \n#>  Median :13.28  \n#>  Mean   :12.64  \n#>  3rd Qu.:14.54  \n#>  Max.   :24.61\n\n\n\nlibrary( gpairs ) # provides gpairs \n\n#Create scatterplot matrix with decreased font size\ngpairs( data.frame( xylemDFnum ) ,\n        ,  diag.pars = list(fontsize = 6) )\n\n\n\n\n\n\nCode\n# compute correlation matrix \ncor( xylemDFnum )\n#>                      P75         Ks      starch xylem_density    fiber_%\n#> P75            1.0000000  0.6486291  0.50004669   -0.36963666 -0.4072020\n#> Ks             0.6486291  1.0000000  0.33916342   -0.26232492 -0.4217329\n#> starch         0.5000467  0.3391634  1.00000000   -0.22135129 -0.5299457\n#> xylem_density -0.3696367 -0.2623249 -0.22135129    1.00000000  0.3273450\n#> fiber_%       -0.4072020 -0.4217329 -0.52994575    0.32734503  1.0000000\n#> vessel_%       0.3151310  0.3337739  0.39992711   -0.53097026 -0.7522285\n#> par_%          0.2842809  0.1413345  0.33729586   -0.07180259 -0.7026932\n#> Pmin           0.8182375  0.5039328  0.56823143   -0.47661507 -0.4143263\n#> water_storage  0.2441534  0.2623565  0.09817602   -0.61754995 -0.2019742\n#>                 vessel_%       par_%       Pmin water_storage\n#> P75            0.3151310  0.28428093  0.8182375    0.24415343\n#> Ks             0.3337739  0.14133448  0.5039328    0.26235648\n#> starch         0.3999271  0.33729586  0.5682314    0.09817602\n#> xylem_density -0.5309703 -0.07180259 -0.4766151   -0.61754995\n#> fiber_%       -0.7522285 -0.70269315 -0.4143263   -0.20197421\n#> vessel_%       1.0000000  0.18413958  0.3928702    0.40577689\n#> par_%          0.1841396  1.00000000  0.1935726   -0.06949848\n#> Pmin           0.3928702  0.19357260  1.0000000    0.40564362\n#> water_storage  0.4057769 -0.06949848  0.4056436    1.00000000\n\n# store the correlation matrix\nM <- cor( xylemDFnum )\n\nlibrary( corrplot ) # provides corrplot.mixed()\n\ncorrplot.mixed(M )\n\n\n\n\n\n\n\nThe boxplots and summary statistics show that temperatures tend to be lower on healthy air days compared to unhealthy air days. The mean temperatures for healthy air days are also lower than those for unhealthy air days. Additionally, temperatures exhibit more variability on healthy air days compared to unhealthy air days. This may indicate more extreme temperatures on healthy air days or suggest that there are other factors contributing to the variability such as wind speed or humidity.\nThe SLogiR model is fitted using glm() but note that the response variable must be binary (such as type_of_day01):\n\nCode# The function will require the following information:\n# y: replace with 'type_of_day01'\n# x: replace with 'temp'\n# data: set equal to a 'kernAQIdf'\n# family: set equal to \"binomial\"\n\n##glm( type_of_day01 ~ temp ,\n  #  data = AQIdf , \n   # family = \"binomial\" )\n\n\nThe output provides \\(b_0= 4.90\\) and \\(b_1=-0.05\\). Note that \\(e^{b_1}=e^{-0.05}=0.95\\), which suggest that odds of having a healthy air day are estimated to decrease by 5% for each degree in temperature. This also suggests that a 5 degree increase in temperature means that the odds of having a healthy air day are estimated to decrease by 22.1% The glm() function provides much more than just these values. To extract other information, the result from glm() must be stored. Below the resulting glm() fit is stored and then the summary() is applied to this object. The functions residuals() and predict() are also illustrated below as well.\n\n\n\nNote that the summary() provides the following table:\n\n\n\nEstimate\nStd. Error\nt-value\np-value -\n\n\n\n(Intercept)\n\\(b_0\\)\n\\(SE_{b_0}\\)\n\\(Z_{b_0}\\)\np-value\\(_{b_0}\\)\n\n\n\nexplanatory\n\\(b_1\\)\n\\(SE_{b_1}\\)\n\\(Z_{b_1}\\)\np-value\\(_{b_1}\\)\n\n\n\n\nThe terms \\(SE_{b_0}\\) and \\(SE_{b_1}\\) are the standard errors for \\(b_0\\) and \\(b_1\\). The next sections cover inference for the SLogiR, and it requires for the assumptions of the data to be reasonable. Next, we perform model diagnostics to assess the model assumptions with the help of the stored fitted values, residuals, and the use xyplot() on the glm object after loading the tactile R package:\n\n\n\nEach plot is analyzed and interpreted below:\n\nThe plot on the left displays the predicted log odds against the explanatory variable and shows a linear trend. This suggests that Assumption 4 (linearity of log-odds) is reasonable.\nThe plot on the right shows the standardized Pearson residuals against the leverage. The presence of some potential outliers (standardized Pearson residuals greater than 3 in magnitude) is visible. Cook’s distance7 contour lines of 1/2 and 1 are plotted, but no leverage values exceed a Cook’s distance of 1. This suggests that Assumption 2 (no influential outliers) is reasonable.\n\nAssumptions 1 (independence of observations) and 3 (large sample size) are reasonable since the sample size is large, and it seems reasonable to assume that a given observation is not influenced by or related to the rest of the observations.\nOverall, all data assumptions are met for the SLogiR model.\n\n\nmaximizing the variance of each component NOW DESCRIBE FUNCTION FOR PCA…\nes, in the context of PCA, the loadings can be interpreted as the correlations between the original variables and the principal components. The loadings represent the coefficients of the linear combinations of the original variables that create the principal components.\nEach loading value indicates the contribution of the corresponding original variable to the principal component. A higher absolute value of a loading indicates that the original variable has a stronger influence on the principal component, while a lower absolute value suggests a weaker influence.\nWhen the PCA is performed on standardized data (i.e., data with a mean of 0 and a standard deviation of 1), the loadings directly represent the correlations between the original variables and the principal components. In this case, the loadings can be used to assess the relationships between the original variables and the PCs, with higher absolute values indicating stronger relationships.\nIn summary, the loadings in PCA can be seen as the correlations between the original variables and the principal components when working with standardized data, and they also represent the coefficients of the linear combinations of the original variables that create the PCs.\nThe information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal.\nIn other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.\nSay we have a set of observations that differ from each other on a number of dimensions, for example, we have a number of whiskey brands (observations) that are rated on a number of attributes such as body, sweetness, fruitiness, etc (dimensions). If some of those dimensions are strongly correlated then it should be possible to describe the observations by a smaller (than original) number of dimensions without losing too much information. For example, sweetness and fruitness could be highly correlated and could therefore be replaced by one variable. Such dimensionality reduction is the goal of principal component analysis.\nRequiring factor loadings to be larger than 0.7 would be a more stringent criterion than the typical threshold of 0.4 or 0.5. This means that only variables with very strong relationships with the underlying principal component would be retained in the analysis, and variables with weaker relationships would be dropped.\nSetting a high threshold for factor loadings can be useful in some cases, such as when the research question is very specific and requires a high degree of confidence in the identified relationships between variables and principal components. However, it can also result in a loss of information and a reduction in the number of variables in the analysis, which may limit the ability to identify more subtle patterns or structures in the data.\nIn general, the choice of threshold for factor loadings depends on the research question, the nature of the data, and the goals of the analysis. It is important to carefully consider the implications of setting a high threshold for factor loadings and to balance the need for specificity and confidence with the desire to retain as much information as possible in the analysis. .\nhttps://rpubs.com/samopolo/746605 <– good rules\nhttps://bookdown.org/egarpor/SSS2-UC3M/pca-examps.html <-biplot explained a bitul\n\n\n\n\nPratt, RB, AL Jacobsen, MI Percolla, ME De Guzman, CA Traugh, and MF Tobin. 2021. “Trade-Offs Among Transport, Support, and Storage in Xylem from Shrubs in a Semiarid Chaparral Environment Tested with Structural Equation Modeling.” Proceedings of the National Academy of Sciences 118 (33)."
  },
  {
    "objectID": "pca.html#pca-comuptation-using-r",
    "href": "pca.html#pca-comuptation-using-r",
    "title": "16  PCA",
    "section": "\n16.2 PCA comuptation using R",
    "text": "16.2 PCA comuptation using R\nThe function gpairs() (from the gpairs package) creates a scatterplot matrix for visualizing pairwise relationships between variables. The function cor() computes a basic correlation matrix, which show all pairwise correlations between variables, and the function corrplot.mixed() (from the corrplot package) creates a better visual representation of the correlation matrix.\n\n\n\n\n\n\nR functions\n\n\n\n### gpairs( dataframe,  diag.pars = list(fontsize = 9))\n# dataframe:  Replace with the name of the tidy dataframe. \n#             All variables must be numerical.\n# diag.pars:  9 is the default font size for variables names. \n#             Decrease the number for smaller font sizes.\n#\n### cor( dataframe )\n# dataframe:  Replace with the name of the tidy dataframe. \n#             All variables must be numerical.\n#\n#\n### corrplot.mixed( `cor object` )\n# cor object:  Replace with an object\n#              created from the cor() output\n#\n\n\nThe function principal() from the psych package helps compute the required elements of PCA. By default, this function will standardized the data and conduct PCA computations on the standardized data.\n\n\n\n\n\n\nR functions\n\n\n\n### principal( dataframe, center , scale. )\n# dataframe: Replace with the name of the tidy dataframe. \n#            All variables must be numerical.\n# nfactors: Set equal to the number of variables in the data/\n# rotate:  Set equal to \"none\". \n# cor: By default, it is set to \"cor\" so that computations\n#      are done on the standarized data. Set equal to \"cov\"\n#      to conduct PCA computation on the original centered data.\n\n\nAlthough performing PCA on standardized data is not required, standardizing the data is recommended for PCA, especially when the variables in the dataset have different units or scales. Standardization transforms each variable to have a mean of 0 and a standard deviation of 1, and it prevents variables with larger scales from dominating the computed PCs.\n\n\n\n\n\n\nIllustration of PCA\n\n\n\nThe case study provided in Chapter 6 provides various plant physiological measurements for 29 different plant species. This data was used by by Pratt et al. (2021) to investigate trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field. The objective here is to use PCA to identify dominant modes of variation among these measurements that are related to xylem structure, function, and properties, as well as underlying factors or dimensions that may be driving the variability in the data.\n\n\nArctostaphylos glauca (Big Berry Manzanita). It is one of shrub species considered in the study. Source:calscape.org\n\n\nBelow is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water (\\(\\triangle \\text{Relative water content}/\\triangle \\text{MPa}\\)).\n\nThese variables provide insight into how xylem functions. The goal here is to apply PCA to determine if key patterns in these data can be identified, as well as underlying factors or dimensions that may be driving the variation in the data.\nTo begin the analysis, the data are imported and explored using numerical and graphical data summaries.\n\nCode\nlibrary( openxlsx ) # provides read.xlsx() \n\n# Import data  \nxylemDF <- read.xlsx(\"datasets/PlantWaterPhysiology.xlsx\")\n\n# obtain the  names of the variables\nnames( xylemDF )\n#>  [1] \"Species\"       \"Family\"        \"P75\"           \"Ks\"           \n#>  [5] \"starch\"        \"xylem_density\" \"fiber_%\"       \"vessel_%\"     \n#>  [9] \"par_%\"         \"Pmin\"          \"water_storage\"\n\n# For PCA, only numerical variables can be \n# present in the data.  Either create a copy\n# of the sheet with non-numerical variables\n# removed and reimport the data, or\n# remove the variables in R as shown below.\n\nlibrary(dplyr) # provides the select() function\n### select( dataframe ,'variable to remove', 'variable to remove' , ...)\n#  dataframe: Replace with the name of the tidy dataframe\n# 'variable to variable': Replace with the name of the variable remove or \n#                         the column number (negative version)\n# \n\n# create a new dataframe that only consists\n# of numerical variables. \n# Note: columns 1 and 2 denote the columns \n# we need to remove.\nxylemDFnum <- select(xylemDF, \n                     -1 ,  # remove column 1 \n                     -2 ) # remove column 2\n\n\n# summarize the variables of interest \nsummary( xylemDFnum )\n#>       P75                Ks           starch       xylem_density   \n#>  Min.   :-15.360   Min.   :0.29   Min.   : 2.210   Min.   :0.4600  \n#>  1st Qu.: -7.760   1st Qu.:0.72   1st Qu.: 3.240   1st Qu.:0.5800  \n#>  Median : -3.920   Median :1.02   Median : 4.000   Median :0.6400  \n#>  Mean   : -5.399   Mean   :1.48   Mean   : 5.069   Mean   :0.6417  \n#>  3rd Qu.: -2.500   3rd Qu.:2.17   3rd Qu.: 5.980   3rd Qu.:0.7100  \n#>  Max.   : -1.380   Max.   :3.79   Max.   :12.900   Max.   :0.7900  \n#>     fiber_%         vessel_%         par_%            Pmin       \n#>  Min.   :42.22   Min.   : 7.78   Min.   : 7.11   Min.   :-8.250  \n#>  1st Qu.:60.28   1st Qu.:14.66   1st Qu.:14.95   1st Qu.:-5.710  \n#>  Median :64.91   Median :16.87   Median :17.30   Median :-3.900  \n#>  Mean   :64.25   Mean   :17.77   Mean   :17.46   Mean   :-4.267  \n#>  3rd Qu.:67.62   3rd Qu.:18.78   3rd Qu.:20.43   3rd Qu.:-2.830  \n#>  Max.   :84.30   Max.   :35.78   Max.   :31.28   Max.   :-1.670  \n#>  water_storage  \n#>  Min.   : 5.42  \n#>  1st Qu.: 9.08  \n#>  Median :13.28  \n#>  Mean   :12.64  \n#>  3rd Qu.:14.54  \n#>  Max.   :24.61\n\n \nlibrary( gpairs ) # provides gpairs()\n\n# Create scatterplot matrix with decreased font size\ngpairs( data.frame( xylemDFnum ) ,\n         diag.pars = list(fontsize = 6) )\n\n\n\n\n\n\nCode\n# Some potential outliers seen in the matrix. Examine them more closely \n# using a boxplot and do the same for other potential cases\n\nlibrary(lattice) # provides bwplot()\nbwplot(  ~ water_storage , \n           data= xylemDFnum ) \n\n\n\n\n\n\nCode\n# compute standard correlation matrix \ncor( xylemDFnum )\n#>                      P75         Ks      starch xylem_density    fiber_%\n#> P75            1.0000000  0.6486291  0.50004669   -0.36963666 -0.4072020\n#> Ks             0.6486291  1.0000000  0.33916342   -0.26232492 -0.4217329\n#> starch         0.5000467  0.3391634  1.00000000   -0.22135129 -0.5299457\n#> xylem_density -0.3696367 -0.2623249 -0.22135129    1.00000000  0.3273450\n#> fiber_%       -0.4072020 -0.4217329 -0.52994575    0.32734503  1.0000000\n#> vessel_%       0.3151310  0.3337739  0.39992711   -0.53097026 -0.7522285\n#> par_%          0.2842809  0.1413345  0.33729586   -0.07180259 -0.7026932\n#> Pmin           0.8182375  0.5039328  0.56823143   -0.47661507 -0.4143263\n#> water_storage  0.2441534  0.2623565  0.09817602   -0.61754995 -0.2019742\n#>                 vessel_%       par_%       Pmin water_storage\n#> P75            0.3151310  0.28428093  0.8182375    0.24415343\n#> Ks             0.3337739  0.14133448  0.5039328    0.26235648\n#> starch         0.3999271  0.33729586  0.5682314    0.09817602\n#> xylem_density -0.5309703 -0.07180259 -0.4766151   -0.61754995\n#> fiber_%       -0.7522285 -0.70269315 -0.4143263   -0.20197421\n#> vessel_%       1.0000000  0.18413958  0.3928702    0.40577689\n#> par_%          0.1841396  1.00000000  0.1935726   -0.06949848\n#> Pmin           0.3928702  0.19357260  1.0000000    0.40564362\n#> water_storage  0.4057769 -0.06949848  0.4056436    1.00000000\n\n# store the correlation matrix\nM <- cor( xylemDFnum )\n\nlibrary( corrplot ) # provides corrplot.mixed()\n\ncorrplot.mixed( M )\n\n\n\n\n\n\n\nThe scatterplot and correlation matrix show that the variables have both positive and negative correlations. Many of the variables show clear linear associations, although the strength of the relationship varies depending on which variables are compared. The scatterplot matrix shows some potential outliers and such cases may be further explored using boxplots. In such cases, a transformation such as the natural log transformation may help reduce the influence of the outliers. Despite the presence of some potential outliers, we proceed with PCA without applying any transformations for the purpose of illustrating the method on this data.\nA good first step in PCA, it to compute the standard deviation for each PC and determine how many PCs should be retained. To compute the standard deviation for each PC, we conduct PCA computations on the data using prcomp().\n\nCodelibrary( psych ) # provides principal()\n# The function will require the following information:\n# dataframe: Replace with 'xylemDFnum'\n# nfactors: Set equal to 9\n# rotate: Set equal to \"none\"\n# cor: By default this is \"cor\", so it does not \n#      need to be provided.\n\n# save result of prcomp()\nxylemPCA <- principal( xylemDFnum, \n                       nfactor= 9,\n                       rotate= \"none\") # 9 variables\n\nxylemPCA\n#> Principal Components Analysis\n#> Call: principal(r = xylemDFnum, nfactors = 9, rotate = \"none\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9 h2       u2\n#> P75            0.78  0.01  0.51 -0.03 -0.14 -0.17  0.12  0.24 -0.03  1  1.1e-16\n#> Ks             0.66 -0.04  0.39 -0.56  0.17  0.11 -0.21 -0.07  0.03  1  2.2e-16\n#> starch         0.67  0.31  0.16  0.49  0.27  0.24 -0.22  0.04  0.01  1  0.0e+00\n#> xylem_density -0.63  0.52  0.29 -0.11  0.21  0.28  0.34  0.02  0.02  1  4.4e-16\n#> fiber_%       -0.78 -0.43  0.39  0.14 -0.07 -0.01 -0.08  0.06  0.14  1  8.9e-16\n#> vessel_%       0.72 -0.08 -0.49 -0.05  0.41 -0.15  0.16  0.06  0.09  1  0.0e+00\n#> par_%          0.44  0.70 -0.24 -0.07 -0.48  0.06 -0.04  0.01  0.09  1 -6.7e-16\n#> Pmin           0.81 -0.14  0.38  0.23 -0.11 -0.07  0.23 -0.23  0.02  1  6.7e-16\n#> water_storage  0.50 -0.67 -0.24 -0.03 -0.21  0.42  0.11  0.06  0.00  1  8.9e-16\n#>               com\n#> P75           2.2\n#> Ks            3.2\n#> starch        3.5\n#> xylem_density 3.9\n#> fiber_%       2.3\n#> vessel_%      2.8\n#> par_%         2.9\n#> Pmin          2.1\n#> water_storage 3.3\n#> \n#>                        PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8  PC9\n#> SS loadings           4.14 1.52 1.18 0.65 0.63 0.39 0.32 0.13 0.04\n#> Proportion Var        0.46 0.17 0.13 0.07 0.07 0.04 0.04 0.01 0.00\n#> Cumulative Var        0.46 0.63 0.76 0.83 0.90 0.95 0.98 1.00 1.00\n#> Proportion Explained  0.46 0.17 0.13 0.07 0.07 0.04 0.04 0.01 0.00\n#> Cumulative Proportion 0.46 0.63 0.76 0.83 0.90 0.95 0.98 1.00 1.00\n#> \n#> Mean item complexity =  2.9\n#> Test of the hypothesis that 9 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0 \n#>  with the empirical chi square  0  with prob <  NA \n#> \n#> Fit based upon off diagonal values = 1\n\n\nThe printing the output of xylemPCA (a principal object) provides the following:\n\nThe squared standard deviation (variance) for each PC (denoted as SS loadings in the output).\nThe loadings for each PC. Recall that these can be interpreted as the correlations between the variables and the components. They are denoted as Standardized loadings (pattern matrix) based upon correlation matrix.\nThe proportion of variance for each PC (denoted as Proportion Var).\nThe cumulative proportion of variance at a given PC (denoted as Cumulative Var)7.\n\nNote that the first PC explains 46% of the variation, the first two PCs together explain about 63% of the variation, and the first three PCs explain about 76% of the variation, and so on. The next step in PCA is to determine an acceptably large percentage of variance that is explained by the PCs."
  },
  {
    "objectID": "pca.html#determining-the-number-of-principal-components-to-retain",
    "href": "pca.html#determining-the-number-of-principal-components-to-retain",
    "title": "16  PCA",
    "section": "\n16.3 Determining the Number of Principal Components to retain",
    "text": "16.3 Determining the Number of Principal Components to retain\nThere are different methods to determine how many PCs to consider for retention, including:\n\nScree plot: This plot displays the squared standard deviations (variance) explained by each principal component on the y-axis with the PCs shown on the x-axis. The variances are provided in descending order, so the amount of variance explained by each PC decreases along the x-axis. The goal is to identify an “elbow” point in the line and selects all components just before the line flattens out.\nKaiser’s rule or Kaiser-Guttman criterion: This rule retains the PC with squared standard deviations greater than or equal to 18.\n\nSimply note the size fo the variance of each PC to apply Kaiser’s rule. To construct a screeplot, we use the function fviz_eig.psych which is a modified version of function called fviz_eig from the factoextra package. fviz_eig.psych was created to work with object created from principal(), where as fviz_eig does not work with such objects.\n\n\n\n\n\n\nR functions\n\n\n\n### fviz_eig.psych( `principal object` , choice , addlabels, ncp , xlab )\n# principal object:  Replace with the name of a  \n#                    principal object.\n# choice: Determine the scale of the height of the bars. \n#         Set equal to either \"variance\" or \"eigenvalue\". \n#         Default is \"variance\". \"eigenvalue\" will provide\n#         the square standard deviations of each PC. \"variance\"\n#         will provide the amount of variance explained by each PC\n# addlabels: If set to equal to TRUE, labels are added at the top of \n#            bars or points showing the information retained by each \n#            dimension. Default is FALSE.\n# ncp: Set equal the number of PCs to display. Default is 10.  \n# xlab: Label for x-axis. Default label is \"dimensions\"\n\n\n\n\n\n\n\n\nIllustration of PCA contined\n\n\n\nHere we determine the number PCs to retain by applying Kaiser’s rule and using a scree plot. Recall that the principal object xylemPCA holds all the relevant PCA computations.\nThe y-axis can either display the squared standard deviations (variances) of the PCs or the percentage of variance explained but the shape does not change. Setting choice= \"variance\" and addlabels= TRUE allows one to apply Kaiser’s rule using the scree plot.\n\nCode# The function will require the following information:\n# dataframe: Replace with 'xylemDFnum'\n# center: By default this is true, so it is not required\n# scale.: Set equal to TRUE\n\n# Recall the principal object is `xylemPCA`\n\nsource(\"rfuns/fviz_eig.psych.R\") # provides fviz_eig.psych()\n\n# The plot displays the prop of variance \n# attributed to each PC.\n# to apply Kaiser's rule.\nfviz_eig.psych(xylemPCA, \n         choice= \"variance\" , \n         addlabels= TRUE, \n         xlab= \"Principal components\")\n\n\n\n\n\n\nCode \n\n# The plot displays the squared\n# standard deviations of each PC. \n# Can apply Kaiser's rule using this plot.\n# Note: One can also examine the squared \n# standard deviations of each PC by printing \n# xylemPCA and noting the values for \"SS loadings\".\nfviz_eig.psych(xylemPCA, \n         choice= \"eigenvalue\" , \n         addlabels= TRUE, \n         xlab= \"Principal components\")\n\n\n\n\n\n\n\nNote that the scree plot shows that the squared standard deviations of the first three PCs are greater than 1. Further note that the first three PC consist of the components just before the line flattens out. In short, both the scree plot and Kaiser’s rule suggest retaining the first 3 PCs. While Kaiser’s rule is easy to apply, PCA must be done on the correlation matrix if applying this rule. The scree plot method does not rely on standardization, but identifying the elbow point may be subjective, and others may arrive at different conclusions based on their interpretation of the scree plot.\nNext, we focus on interpreting of the PCs."
  },
  {
    "objectID": "pca.html#towards-interpretating-the-pcs",
    "href": "pca.html#towards-interpretating-the-pcs",
    "title": "16  PCA",
    "section": "\n16.4 Towards interpretating the PCs",
    "text": "16.4 Towards interpretating the PCs\nOnce the number of PCs to retain is determined, the PCs should be examined. Since PCA was performed on the standardized data, one approach to interpreting each component is to focus on the size of the loadings of a given PC, as they represent correlations between the variables and the given PC. Here, we consider a loading at 0.7 or greater in magnitude as important9.\nWe can examine the loadings by printing the principal object or using the function loadings() on principal object\n\n\n\n\n\n\nR functions\n\n\n\n### loadings( `principal object` )\n# principal object:  Replace with the name of a  \n#                    principal object.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that the principal object xylemPCA holds all the relevant PCA computations.\n\nCode# Recall the principal object is `xylemPCA`.\n# Save the result to an object\nPCAloadings <- loadings( xylemPCA )\n\n# display the loadings\nPCAloadings # loadings smaller than .1 are not displayed\n#> \n#> Loadings:\n#>               PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8    PC9   \n#> P75            0.783         0.511        -0.139 -0.175  0.122  0.243       \n#> Ks             0.661         0.394 -0.560  0.174  0.114 -0.209              \n#> starch         0.675  0.309  0.158  0.491  0.274  0.242 -0.218              \n#> xylem_density -0.630  0.521  0.287 -0.114  0.209  0.282  0.335              \n#> fiber_%       -0.781 -0.425  0.393  0.141                              0.143\n#> vessel_%       0.721        -0.492         0.408 -0.154  0.164              \n#> par_%          0.444  0.704 -0.242        -0.482                            \n#> Pmin           0.813 -0.142  0.376  0.230 -0.109         0.229 -0.234       \n#> water_storage  0.500 -0.672 -0.244        -0.210  0.422  0.112              \n#> \n#>                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9\n#> SS loadings    4.14 1.523 1.180 0.650 0.627 0.392 0.318 0.132 0.039\n#> Proportion Var 0.46 0.169 0.131 0.072 0.070 0.044 0.035 0.015 0.004\n#> Cumulative Var 0.46 0.629 0.760 0.832 0.902 0.946 0.981 0.996 1.000\n\n\nHere, the loadings among the first three PC are nearly all large or close to being large for the first PC. Such loadings means that the variables are highly correlated with the PCs, which can make it challenging to interpret the unique contribution of each variable to each PC. This makes it difficult to identify patterns or attributes in data that may have interpretations or implications in the context of the data. In such instances, one can apply a rotation on the PCs.\n\n\nIt is not uncommon for the interpretation of loadings to be challenging, particularly when the retained principal components exhibit patterns of large or small loadings, making it difficult to interpret in the context of the data. If most, if not all, of the loadings are large, this indicates that nearly all variables are highly correlated with the principal components, which can make it difficult to identify the unique contributions of individual variables to each component. On the other hand, if most, if not all, of the loadings are small, this may indicate that the data are too noisy. If all the loadings are small, one may consider lowering the threshold for what is considered important. Applying a rotation on the retained PCs can help deal with instances in which the loadings are all large or small."
  },
  {
    "objectID": "pca.html#rotating-the-pcs",
    "href": "pca.html#rotating-the-pcs",
    "title": "16  PCA",
    "section": "\n16.5 Rotating the PCs",
    "text": "16.5 Rotating the PCs\nRotation transforms the retained PCs into a new set of linear combinations (i.e., leading to changes in the values of \\(e_{i1}, e_{i2}, ..., e_{ip}\\)) that may be easier to interpret. By rotating the retained PCs, a new set of loadings can be obtained that may have a simpler structure and be easier to interpret compared to the original loadings. Varimax rotation10 is one commonly used rotation method in PCA. Here, we apply varimax rotation on the retained PCs. The varimax rotated PCs will still account for the same amount of information as the unrotated retained PCs.\nTo provide a non-technical explanation of varimax rotating the PCs, recall the real estate analogy used earlier. One deciding on a set of best pictures that better highlight the most desirable or best features of a property is akin to deciding how many PCs to retain. However, some of the pictures that were selected may have similar or overlapping features, making it hard to tell them apart. Here, varimax rotation would be like the real estate agent taking the same pictures but using a different angle (“rotating them”) so that each pictures better highlights distinct features and there is minimal overlap between the features in each picture. This new set of rotated pictures provides hopefully provides clearer and better view of the property, making it easier to identify its unique features and characteristics. As before, this is just an attempt at an analogy but hopefully helps illustrate the idea of using varimax rotation on the PCs.\nThe function principal() by defaults applies a varimax rotation to the retained PC by setting nfactors equal to then number of retained PCs and having rotate= \"varimax\" (default rotate value).\n\n\n\n\n\n\nNote\n\n\n\nRecall Kaiser’s rule suggested keeping the first three PCs. Here we use varimax rotation to rotate the retained PCs.\n\n\nR code\nVideo\n\n\n\n\nCode# Note that nfactor is set to 3\n# since we retained 3 PCs. The argument\n# rotate is not needed since by default\n# is it \"varimax\"\nxylemPCAvmr <- principal( xylemDFnum, \n                       nfactor= 3) # The number of retained PC\n\nxylemPCAvmr\n#> Principal Components Analysis\n#> Call: principal(r = xylemDFnum, nfactors = 3)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                 RC1   RC3   RC2   h2    u2 com\n#> P75            0.91  0.16  0.14 0.87 0.126 1.1\n#> Ks             0.74  0.12  0.17 0.59 0.406 1.2\n#> starch         0.58  0.49  0.05 0.58 0.424 2.0\n#> xylem_density -0.23 -0.09 -0.83 0.75 0.249 1.2\n#> fiber_%       -0.25 -0.89 -0.29 0.95 0.054 1.4\n#> vessel_%       0.14  0.55  0.67 0.77 0.232 2.0\n#> par_%          0.13  0.84 -0.15 0.75 0.249 1.1\n#> Pmin           0.83  0.13  0.33 0.82 0.178 1.4\n#> water_storage  0.18 -0.11  0.85 0.76 0.239 1.1\n#> \n#>                        RC1  RC3  RC2\n#> SS loadings           2.60 2.13 2.12\n#> Proportion Var        0.29 0.24 0.24\n#> Cumulative Var        0.29 0.53 0.76\n#> Proportion Explained  0.38 0.31 0.31\n#> Cumulative Proportion 0.38 0.69 1.00\n#> \n#> Mean item complexity =  1.4\n#> Test of the hypothesis that 3 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0.08 \n#>  with the empirical chi square  13.3  with prob <  0.35 \n#> \n#> Fit based upon off diagonal values = 0.96\n\n# xylemPCAvmr is a principal object\nvmrPCAloadings <- loadings( xylemPCAvmr )\n\n# display the loadings\nvmrPCAloadings # loadings smaller than .1 are not displayed\n#> \n#> Loadings:\n#>               RC1    RC3    RC2   \n#> P75            0.911  0.162  0.137\n#> Ks             0.742  0.121  0.168\n#> starch         0.578  0.490       \n#> xylem_density -0.233        -0.830\n#> fiber_%       -0.253 -0.892 -0.294\n#> vessel_%       0.144  0.550  0.667\n#> par_%          0.125  0.845 -0.148\n#> Pmin           0.835  0.134  0.327\n#> water_storage  0.176 -0.105  0.848\n#> \n#>                  RC1   RC3   RC2\n#> SS loadings    2.596 2.129 2.117\n#> Proportion Var 0.288 0.237 0.235\n#> Cumulative Var 0.288 0.525 0.760\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that in the output, the rotated PCs presented in descending order in terms of the size of the squared standard deviations. The terms SS loadings, Proportion Var, and Cumulative Var mean the same as they did before. The rotated PCs are given under Loadings and are labeled RC1, RC3, and RC2 (note that they are presented based on their proportion of variance). Note that the total amount of variation explained by these three PCs remains the same as before."
  },
  {
    "objectID": "pca.html#interpretating-of-the-pcs",
    "href": "pca.html#interpretating-of-the-pcs",
    "title": "16  PCA",
    "section": "\n16.6 Interpretating of the PCs",
    "text": "16.6 Interpretating of the PCs\nBelow each rotated PC is interpreted. Note that the original PCs would be interpreted in the same manner if they were interpretable.\nThe first PC: The first rotated PC shows a strong correlation with Pmin, P75, and Ks. This suggests that these variables vary in a similar way. Recall that we consider a correlation above 0.7 in magnitude (strong correlation) as important. This component explains 28.8% of the total variation in the data. Note, that the Pmin values represent how dehydrated a plant becomes on a seasonal basis (lower values denote greater tolerance of dehydration). Thus, shrub species with a lower Pmin would be more tolerant to drought conditions and can tolerate greater water deficits than those with higher Pmin. Similarly, lower values of P75 indicate that water can be transported more safely during dry conditions, which is an adaptive trait in a semi-arid environment. However, higher levels of Ks can improve water transport efficiency by allowing water to move more easily through the xylem. This can help counter the negative effects of higher minimum hydrostatic pressure and higher P75 values on water efficiency since as Ks increases then Pmin and P75 tend to also increase. Hence, PC 1 may be viewed as an adaptability measure of plants to drought conditions with respect to water efficiency.\nThe second PC: The second rotated PC explains 23.5% of the variation in the data that is independent from the first PC10. This PC shows a strong positive correlation with water storage capacity and vessel percentage, while showing a strong negative correlation with xylem density. If a shrub has high xylem density then it has tissue that is strong and can resist damage from breaking; however, it will tend to have lower water storage capacity (capacitance). Water storage capacity can help provide water when it is limited, which can help maintain the shrub’s structural stability and prevent damage that can occur when dehydration is extreme. Therefore, this PC may be viewed as a measure of the adaptability of shrubs to drought conditions with respect to mechanical strength and water storage.\nThe third PC: This component explains 23.7% of the remaining variation in the data that is not explained by either the first or second component. It exhibits a strong negative correlation with fiber percentage and a strong positive correlation with parenchyma percentage. The volume of space in the vascular tissue is occupied by three cell types: parenchyma, fibers, and vessels. Fibers are associated with mechanical strength of the plant and parenchyma are associated with starch storage so these cellular relationships can affect plant function. This component suggests that species with a higher proportion of fibers tend to have a lower amount of parenchyma and vice versa, with vessels also loading positively on this axis but with a weaker correlation than the 0.7 cutoff. This suggest that species in a water-limited environment compensate for having a lower percentage of fiber by having a higher percentage of parenchyma to maintain plant function. Therefore, this PC can be considered as an indicator of the adaptability of shrubs to drought conditions, specifically in terms of their capacity to balance parenchyma and fiber to maintain essential plant functions."
  },
  {
    "objectID": "pca.html#interpretating-the-pcs",
    "href": "pca.html#interpretating-the-pcs",
    "title": "16  PCA",
    "section": "\n16.6 Interpretating the PCs",
    "text": "16.6 Interpretating the PCs\nBelow, the rotated PCs are interpreted. It is important to note that the original principal components would have been interpreted in the same way if they were easily interpretable.\n\n\n\n\n\n\nNote\n\n\n\nThe first PC: The first rotated PC shows a strong correlation with Pmin, P75, and Ks. This suggests that these variables vary in a similar way. Recall that we consider a correlation above 0.7 in magnitude (strong correlation) as important. This component explains 28.8% of the total variation in the data. Note, that the Pmin values represent how dehydrated a plant becomes on a seasonal basis (lower values denote greater tolerance of dehydration). Thus, shrub species with a lower Pmin would be more tolerant to drought conditions and can tolerate greater water deficits than those with higher Pmin. Similarly, lower values of P75 indicate that water can be transported more safely during dry conditions, which is an adaptive trait in a semi-arid environment. However, higher levels of Ks can improve water transport efficiency by allowing water to move more easily through the xylem. This can help counter the negative effects of higher minimum hydrostatic pressure and higher P75 values on water efficiency since as Ks increases then Pmin and P75 tend to also increase. Hence, PC 1 may be viewed as an adaptability measure of plants to drought conditions with respect to water efficiency.\nThe second PC: The second rotated PC explains 23.5% of the variation in the data that is independent from the first PC11. This PC shows a strong positive correlation with water storage capacity and vessel percentage, while showing a strong negative correlation with xylem density. If a shrub has high xylem density then it has tissue that is strong and can resist damage from breaking; however, it will tend to have lower water storage capacity (capacitance). Water storage capacity can help provide water when it is limited, which can help maintain the shrub’s structural stability and prevent damage that can occur when dehydration is extreme. Therefore, this PC may be viewed as a measure of the adaptability of shrubs to drought conditions with respect to mechanical strength and water storage.\nThe third PC: This component explains 23.7% of the remaining variation in the data that is not explained by either the first or second component. It exhibits a strong negative correlation with fiber percentage and a strong positive correlation with parenchyma percentage. The volume of space in the vascular tissue is occupied by three cell types: parenchyma, fibers, and vessels. Fibers are associated with mechanical strength of the plant and parenchyma are associated with starch storage so these cellular relationships can affect plant function. This component suggests that species with a higher proportion of fibers tend to have a lower amount of parenchyma and vice versa, with vessels also loading positively on this axis but with a weaker correlation than the 0.7 cutoff. This suggest that species in a water-limited environment compensate for having a lower percentage of fiber by having a higher percentage of parenchyma to maintain plant function. Therefore, this PC can be considered as an indicator of the adaptability of shrubs to drought conditions, specifically in terms of their capacity to balance parenchyma and fiber to maintain essential plant functions.\n\n\n\n\n\n\nPratt, RB, AL Jacobsen, MI Percolla, ME De Guzman, CA Traugh, and MF Tobin. 2021. “Trade-Offs Among Transport, Support, and Storage in Xylem from Shrubs in a Semiarid Chaparral Environment Tested with Structural Equation Modeling.” Proceedings of the National Academy of Sciences 118 (33)."
  },
  {
    "objectID": "pca.html#determining-the-number-of-pcs-to-retain",
    "href": "pca.html#determining-the-number-of-pcs-to-retain",
    "title": "16  PCA",
    "section": "\n16.3 Determining the Number of PCs to retain",
    "text": "16.3 Determining the Number of PCs to retain\nThere are different methods to determine how many PCs to consider for retention, including:\n\nScree plot: This plot displays the squared standard deviations (variance) explained by each principal component on the y-axis with the PCs shown on the x-axis. The variances are provided in descending order, so the amount of variance explained by each PC decreases along the x-axis. The goal is to identify an “elbow” point in the line and selects all components just before the line flattens out.\nKaiser’s rule or Kaiser-Guttman criterion: This rule retains the PC with squared standard deviations greater than or equal to 18.\n\nSimply note the size fo the variance of each PC to apply Kaiser’s rule. To construct a screeplot, we use the function fviz_eig.psych which is a modified version of function called fviz_eig from the factoextra package. fviz_eig.psych was created to work with object created from principal(), where as fviz_eig does not work with such objects.\n\n\n\n\n\n\nR functions\n\n\n\n### fviz_eig.psych( `principal object` , choice , addlabels, ncp , xlab )\n# principal object:  Replace with the name of a  \n#                    principal object.\n# choice: Determine the scale of the height of the bars. \n#         Set equal to either \"variance\" or \"eigenvalue\". \n#         Default is \"variance\". \"eigenvalue\" will provide\n#         the square standard deviations of each PC. \"variance\"\n#         will provide the amount of variance explained by each PC\n# addlabels: If set to equal to TRUE, labels are added at the top of \n#            bars or points showing the information retained by each \n#            dimension. Default is FALSE.\n# ncp: Set equal the number of PCs to display. Default is 10.  \n# xlab: Label for x-axis. Default label is \"dimensions\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere we determine the number PCs to retain by applying Kaiser’s rule and using a scree plot. Recall that the principal object xylemPCA holds all the relevant PCA computations.\nThe y-axis can either display the squared standard deviations (variances) of the PCs or the percentage of variance explained but the shape does not change. Setting choice= \"variance\" and addlabels= TRUE allows one to apply Kaiser’s rule using the scree plot.\n\n\nR code\nVideo\n\n\n\n\nCode# The function will require the following information:\n# dataframe: Replace with 'xylemDFnum'\n# center: By default this is true, so it is not required.\n# scale.: Set equal to TRUE.\n\nsource(\"rfuns/fviz_eig.psych.R\") # provides fviz_eig.psych()\n\n# Recall the principal object is `xylemPCA`\n# The plot displays the prop of variance \n# attributed to each PC.\nfviz_eig.psych(xylemPCA, \n         choice= \"variance\" , \n         addlabels= TRUE, \n         xlab= \"Principal components\")\n\n\n\n\n\n\nCode \n\n# The plot displays the squared\n# standard deviations of each PC, and \n# it can be used to apply Kaiser's rule.\n# Note: One can also examine the squared \n# standard deviations of each PC by printing \n# xylemPCA and noting the values for \"SS loadings\".\nfviz_eig.psych(xylemPCA, \n         choice= \"eigenvalue\" , \n         addlabels= TRUE, \n         xlab= \"Principal components\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the scree plot shows that the squared standard deviations of the first three PCs are greater than 1. Further note that the first three PC consist of the components just before the line flattens out. In short, both the scree plot and Kaiser’s rule suggest retaining the first 3 PCs. While Kaiser’s rule is easy to apply, PCA must be done on the correlation matrix if applying this rule. The scree plot method does not rely on standardization, but identifying the elbow point may be subjective, and others may arrive at different conclusions based on their interpretation of the scree plot.\nNext, we focus on interpreting of the PCs."
  },
  {
    "objectID": "pca.html#pca-comuptations-using-r",
    "href": "pca.html#pca-comuptations-using-r",
    "title": "16  PCA",
    "section": "\n16.2 PCA comuptations using R",
    "text": "16.2 PCA comuptations using R\nThe function gpairs() (from the gpairs package) creates a scatterplot matrix for visualizing pairwise relationships between variables. The function cor() computes a basic correlation matrix, which show all pairwise correlations between variables, and the function corrplot.mixed() (from the corrplot package) creates a better visual representation of the correlation matrix.\n\n\n\n\n\n\nR functions\n\n\n\n### gpairs( dataframe,  diag.pars = list(fontsize = 9))\n# dataframe:  Replace with the name of the dataframe. \n#             All variables must be numerical.\n# diag.pars:  9 is the default font size for variables names. \n#             Decrease the number for smaller font sizes.\n#\n### cor( dataframe )\n# dataframe:  Replace with the name of the dataframe. \n#             All variables must be numerical.\n#\n#\n### corrplot.mixed( `cor object` )\n# cor object:  Replace with an object\n#              created from the cor() output\n#\n\n\nThe function principal() from the psych package helps compute the required elements of PCA. By default, this function will standardized the data and conduct PCA computations on the standardized data.\n\n\n\n\n\n\nR functions\n\n\n\n### principal( dataframe, nfactors , rotate, cor )\n# dataframe: Replace with the name of the dataframe. \n#            All variables must be numerical.\n# nfactors: Set equal to the number of variables in the data.\n# rotate:  Set equal to \"none\". \n# cor: By default, it is set to \"cor\" so that computations\n#      are done on the standarized data. Set equal to \"cov\"\n#      to conduct PCA computation on the original centered data.\n\n\nAlthough performing PCA on standardized data is not required, standardizing the data is recommended for PCA, especially when the variables in the dataset have different units or scales. Standardization transforms each variable to have a mean of 0 and a standard deviation of 1, and it prevents variables with larger scales from dominating the computed PCs.\nNote that the dataframe used for PCA must only contain numerical variables. The function select() from the dplyr package can assist with this task.\n\n\n\n\n\n\nR functions\n\n\n\n### select( dataframe ,'variable to remove', 'variable to remove' , ...)\n#  dataframe: Replace with the name of the  dataframe\n# 'variable to variable': Replace with the name of the variable remove or \n#                         the column number (negative version)\n# \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe case study provided in Chapter 6 provides various plant physiological measurements for 29 different plant species. This data was used by by Pratt et al. (2021) to investigate trade-offs (costs and benefits of different traits) among different xylem functions in shrub species and how they are influenced by minimum hydrostatic pressures experienced by plants in the field. The objective here is to use PCA to identify dominant modes of variation among these measurements that are related to xylem structure, function, and properties, as well as underlying factors or dimensions that may be driving the variability in the data.\n\n\nArctostaphylos glauca (Big Berry Manzanita). It is one of shrub species considered in the study. Source:calscape.org\n\n\nBelow is a list and small description of each variable:\n\nP75: the water potential at 75% loss of its hydraulic conductivity (MPa).\nKs: a measure of xylem-specific conductivity. conductivity (kg s\\(^{-1}\\) MPa\\(^{-1}\\) m\\(^{-1}\\)).\nStarch: amount of starch content in the xylem tissues (%).\nXylem density: measure of the density of xylem (dry mass/tissue volume) .\nFiber percentage: the proportion of fibers in the xylem tissue.\nVessel percentage: the proportion of vessels in the xylem tissue.\nParenchyma percentage: the proportion of parenchyma cells in the xylem tissue.\nPmin: Minimum level of dehydration a plant can experience (MPa).\nWater storage capacity: the capacity of the xylem tissue to store water (\\(\\triangle \\text{Relative water content}/\\triangle \\text{MPa}\\)).\n\nThese variables provide insight into how xylem functions. The goal here is to apply PCA to determine if key patterns in these data can be identified, as well as underlying factors or dimensions that may be driving the variation in the data.\nTo begin the analysis, the data are imported and explored using numerical and graphical data summaries.\n\n\nR code\nVideo\n\n\n\n\nCode# Load the openxlsx package\nlibrary( openxlsx ) # Provides read.xlsx() \n\n# Import the .xlsx file\nxylemDF <- read.xlsx(\"datasets/PlantWaterPhysiology.xlsx\")\n\n# Print the names of the variables\nnames( xylemDF )\n#>  [1] \"Species\"       \"Family\"        \"P75\"           \"Ks\"           \n#>  [5] \"starch\"        \"xylem_density\" \"fiber_%\"       \"vessel_%\"     \n#>  [9] \"par_%\"         \"Pmin\"          \"water_storage\"\n\n# For PCA, only numerical variables can be present \n# in the data. We can either create a copy of the \n# sheet with non-numerical variables removed and \n# re-import the data, or remove the variables \n# in R as shown below.\n\n# Load the dplyr package\nlibrary(dplyr) # Provides the select() function\n\n# Create a new dataframe that only consists of numerical variables\n# Note: columns 1 and 2 denote the columns we need to remove.\nxylemDFnum <- select(xylemDF, \n                     -1 ,  # remove column 1 \n                     -2 )  # remove column 2\n\n\n# summarize the variables of interest \nsummary( xylemDFnum )\n#>       P75                Ks           starch       xylem_density   \n#>  Min.   :-15.360   Min.   :0.29   Min.   : 2.210   Min.   :0.4600  \n#>  1st Qu.: -7.760   1st Qu.:0.72   1st Qu.: 3.240   1st Qu.:0.5800  \n#>  Median : -3.920   Median :1.02   Median : 4.000   Median :0.6400  \n#>  Mean   : -5.399   Mean   :1.48   Mean   : 5.069   Mean   :0.6417  \n#>  3rd Qu.: -2.500   3rd Qu.:2.17   3rd Qu.: 5.980   3rd Qu.:0.7100  \n#>  Max.   : -1.380   Max.   :3.79   Max.   :12.900   Max.   :0.7900  \n#>     fiber_%         vessel_%         par_%            Pmin       \n#>  Min.   :42.22   Min.   : 7.78   Min.   : 7.11   Min.   :-8.250  \n#>  1st Qu.:60.28   1st Qu.:14.66   1st Qu.:14.95   1st Qu.:-5.710  \n#>  Median :64.91   Median :16.87   Median :17.30   Median :-3.900  \n#>  Mean   :64.25   Mean   :17.77   Mean   :17.46   Mean   :-4.267  \n#>  3rd Qu.:67.62   3rd Qu.:18.78   3rd Qu.:20.43   3rd Qu.:-2.830  \n#>  Max.   :84.30   Max.   :35.78   Max.   :31.28   Max.   :-1.670  \n#>  water_storage  \n#>  Min.   : 5.42  \n#>  1st Qu.: 9.08  \n#>  Median :13.28  \n#>  Mean   :12.64  \n#>  3rd Qu.:14.54  \n#>  Max.   :24.61\n\n# Load the gpairs package \nlibrary( gpairs ) # Provides gpairs()\n\n# Create scatterplot matrix with decreased font size\ngpairs( data.frame( xylemDFnum ) ,\n         diag.pars = list(fontsize = 6) )\n\n\n\n\n\n\nCode\n# Some potential outliers seen in the matrix. Examine them more closely \n# using a boxplot and do the same for other potential cases\n\n# Load the lattice package \nlibrary(lattice) # Provides bwplot()\n\n# Create a boxplot for the \"water_storage\" variable to \n# identify potential outliers\nbwplot(  ~ water_storage , \n           data= xylemDFnum ) \n\n\n\n\n\n\nCode\n# Compute standard correlation matrix \ncor( xylemDFnum )\n#>                      P75         Ks      starch xylem_density    fiber_%\n#> P75            1.0000000  0.6486291  0.50004669   -0.36963666 -0.4072020\n#> Ks             0.6486291  1.0000000  0.33916342   -0.26232492 -0.4217329\n#> starch         0.5000467  0.3391634  1.00000000   -0.22135129 -0.5299457\n#> xylem_density -0.3696367 -0.2623249 -0.22135129    1.00000000  0.3273450\n#> fiber_%       -0.4072020 -0.4217329 -0.52994575    0.32734503  1.0000000\n#> vessel_%       0.3151310  0.3337739  0.39992711   -0.53097026 -0.7522285\n#> par_%          0.2842809  0.1413345  0.33729586   -0.07180259 -0.7026932\n#> Pmin           0.8182375  0.5039328  0.56823143   -0.47661507 -0.4143263\n#> water_storage  0.2441534  0.2623565  0.09817602   -0.61754995 -0.2019742\n#>                 vessel_%       par_%       Pmin water_storage\n#> P75            0.3151310  0.28428093  0.8182375    0.24415343\n#> Ks             0.3337739  0.14133448  0.5039328    0.26235648\n#> starch         0.3999271  0.33729586  0.5682314    0.09817602\n#> xylem_density -0.5309703 -0.07180259 -0.4766151   -0.61754995\n#> fiber_%       -0.7522285 -0.70269315 -0.4143263   -0.20197421\n#> vessel_%       1.0000000  0.18413958  0.3928702    0.40577689\n#> par_%          0.1841396  1.00000000  0.1935726   -0.06949848\n#> Pmin           0.3928702  0.19357260  1.0000000    0.40564362\n#> water_storage  0.4057769 -0.06949848  0.4056436    1.00000000\n\n# Store the correlation matrix\nM <- cor( xylemDFnum )\n\n# Load the corrplot package \nlibrary( corrplot ) # Provides corrplot.mixed()\n\n# Better visualization  of the \n# correlation matrix\ncorrplot.mixed( M )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot and correlation matrix show that the variables have both positive and negative correlations. Many of the variables show clear linear associations, although the strength of the relationship varies depending on which variables are compared. The scatterplot matrix shows some potential outliers and such cases may be further explored using boxplots. In such cases, a transformation such as the natural log transformation may help reduce the influence of the outliers. Despite the presence of some potential outliers, we proceed with PCA without applying any transformations for the purpose of illustrating the method on this data.\nA good first step in PCA is it to compute the standard deviation for each PC and determine how many PCs should be retained. To compute the standard deviation for each PC, we conduct PCA computations on the data using principal().\n\n\nR code\nVideo\n\n\n\n\nCode# Load the psych package\nlibrary( psych ) # Provides principal()\n\n# The function will require the following information:\n# dataframe: Replace with 'xylemDFnum'\n# nfactors: Set equal to 9\n# rotate: Set equal to \"none\"\n# cor: By default this is \"cor\", so it does not \n#      need to be provided.\n# Note: The result of principal() is saved to xylemPCA\nxylemPCA <- principal( xylemDFnum, \n                       nfactor= 9,\n                       rotate= \"none\") # 9 variables\n\nxylemPCA # Print xylemPCA object\n#> Principal Components Analysis\n#> Call: principal(r = xylemDFnum, nfactors = 9, rotate = \"none\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9 h2       u2\n#> P75            0.78  0.01  0.51 -0.03 -0.14 -0.17  0.12  0.24 -0.03  1  1.1e-16\n#> Ks             0.66 -0.04  0.39 -0.56  0.17  0.11 -0.21 -0.07  0.03  1  2.2e-16\n#> starch         0.67  0.31  0.16  0.49  0.27  0.24 -0.22  0.04  0.01  1  0.0e+00\n#> xylem_density -0.63  0.52  0.29 -0.11  0.21  0.28  0.34  0.02  0.02  1  4.4e-16\n#> fiber_%       -0.78 -0.43  0.39  0.14 -0.07 -0.01 -0.08  0.06  0.14  1  8.9e-16\n#> vessel_%       0.72 -0.08 -0.49 -0.05  0.41 -0.15  0.16  0.06  0.09  1  0.0e+00\n#> par_%          0.44  0.70 -0.24 -0.07 -0.48  0.06 -0.04  0.01  0.09  1 -6.7e-16\n#> Pmin           0.81 -0.14  0.38  0.23 -0.11 -0.07  0.23 -0.23  0.02  1  6.7e-16\n#> water_storage  0.50 -0.67 -0.24 -0.03 -0.21  0.42  0.11  0.06  0.00  1  8.9e-16\n#>               com\n#> P75           2.2\n#> Ks            3.2\n#> starch        3.5\n#> xylem_density 3.9\n#> fiber_%       2.3\n#> vessel_%      2.8\n#> par_%         2.9\n#> Pmin          2.1\n#> water_storage 3.3\n#> \n#>                        PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8  PC9\n#> SS loadings           4.14 1.52 1.18 0.65 0.63 0.39 0.32 0.13 0.04\n#> Proportion Var        0.46 0.17 0.13 0.07 0.07 0.04 0.04 0.01 0.00\n#> Cumulative Var        0.46 0.63 0.76 0.83 0.90 0.95 0.98 1.00 1.00\n#> Proportion Explained  0.46 0.17 0.13 0.07 0.07 0.04 0.04 0.01 0.00\n#> Cumulative Proportion 0.46 0.63 0.76 0.83 0.90 0.95 0.98 1.00 1.00\n#> \n#> Mean item complexity =  2.9\n#> Test of the hypothesis that 9 components are sufficient.\n#> \n#> The root mean square of the residuals (RMSR) is  0 \n#>  with the empirical chi square  0  with prob <  NA \n#> \n#> Fit based upon off diagonal values = 1\n\n\n\n\nSee previous video.\n\n\n\nThe printing the output of xylemPCA (a principal object) provides the following:\n\nThe squared standard deviation (variance) for each PC (denoted as SS loadings in the output).\nThe loadings for each PC. Recall that these can be interpreted as the correlations between the variables and the components. They are denoted as Standardized loadings (pattern matrix) based upon correlation matrix.\nThe proportion of variance for each PC (denoted as Proportion Var).\nThe cumulative proportion of variance at a given PC (denoted as Cumulative Var)7.\n\nNote that the first PC explains 46% of the variation, the first two PCs together explain about 63% of the variation, and the first three PCs explain about 76% of the variation, and so on. The next step in PCA is to determine an acceptably large percentage of variance that is explained by the PCs."
  },
  {
    "objectID": "a3-basicsR.html#sec-importdata",
    "href": "a3-basicsR.html#sec-importdata",
    "title": "Appendix C — Some basic tasks in R",
    "section": "\nC.1 Importing data",
    "text": "C.1 Importing data\n.csv and .xlsx files\nFor importing data, it is assumed the data are stored in a .csv file as described in Section 7.3. To import .csv files, the function read.csv() is used and this function comes with R. Note that if your data are stored in an excel sheet (.xlsx), the function read.csv() can be replaced with read.xlsx. This functions is from the openxlsx package. If your excel sheet is a .xls file, convert it using excel to .xlsx. Installation of R packages are discussed later in this section.\nImporting data via a URL\nImporting data here will be illustrated using the “MLB salaries (2010)” data set. This .csv file of this dataset is located at the following URL: http://www.csub.edu/~emontoya2/datasets/mlb2010.csv.\nThis approach only works if the .csv has a url location. The “MLB salaries (2010)” data set is located at the following URL:\nLink: http://www.csub.edu/~emontoya2/datasets/mlb2010.csv\nTo import, run\n\nCodemlbDataImport = read.csv( \"http://www.csub.edu/~emontoya2/datasets/mlb2010.csv\" )\n\n\nNote that read.csv() reads in the data into R and we are assigning the resulting data set to an object called mlbDataImport. Since this object consist of the dataset, such objects are called a data frame. A data frame is the most common way of storing data in R. For our purpose, the term dataframe and data set will be used interchangeably. Note that the mlbDataImport data frame should now appear under the “Environment tab”.\nFurther note that read.csv() only requires the location of the file or file path (assuming the data was stored as discussed in Section 7.3). Since the data is located at this URL, we use the URL. If the data is saved on your computer, then replace the url with the file path or with file.choose().\n\n\n\n\n\n\nURLs\n\n\n\nThe URL must be in quotations and NO spaces at the start after the quotation or right before the end of the quotation. For example, you will get an error if you use the following urls\n\n\" http://www.csub.edu/~emontoya2/datasets/mlb2010.csv\"\n\"http://www.csub.edu/~emontoya2/datasets/mlb2010.csv \"\n\" http://www.csub.edu/~emontoya2/datasets/mlb2010.csv \"\n\nThey look similar to what we used but it has spaces within the quotation marks!\n\n\nAn R function called summary() can provide the names of the variables and other information by using :\n\nCodesummary( mlbDataImport ) \n#>     player              team             position             salary       \n#>  Length:828         Length:828         Length:828         Min.   :  400.0  \n#>  Class :character   Class :character   Class :character   1st Qu.:  418.3  \n#>  Mode  :character   Mode  :character   Mode  :character   Median : 1093.8  \n#>                                                           Mean   : 3281.8  \n#>                                                           3rd Qu.: 4250.0  \n#>                                                           Max.   :33000.0\n\n\nNote that this data frame has four variables but that R treats the categorical variables as character variables. By default, R treats any categorical variables with categories labeled by letters or words as character variables. If the categories are instead labeled with numbers, R considers the categorical variable to be a numerical variable. Instructions on how to tell R to treat categorical variables as categorical and not numerical/character will be covered later.\nImporting data from your computer\nDownload the .csv file below onto your computer. Make sure to note where the file is saved. Depending on the browser settings, after clicking on the link below, the file will either open in the browser and it will be downloaded.\nhttp://www.csub.edu/~emontoya2/datasets/sampledata.csv\nAfter saving this .csv file, one can take a “point and click” approach to importing data. Like most software, RStudio has a toolbar where you can access many commands. To import data using the toolbar menu, select File>Import Dataset> From Text (base). Then select your .csv file. If your data are stored in an excel spreadsheet (.xlsx) in tidy form, then select “From Excel”.\nAlternatively, if you know the file path for the spreadsheet, you can run following\n\nCode# by putting in \"file.choose()\", this tells R that you want to browse for and select the csv file\nImportFromDrive = read.csv( file.choose() )\n\n\n\n\n\nThis automatically opens a window that allows you to browse for your .csv file. Select your file and click on Open.\nThe data set ImportFromDrive should appear in the Environment tab in the upper right panel where it displays the number of observations and variables. One may also replace file.choose() with the location of the file in quotation marks."
  },
  {
    "objectID": "casestudies.html#module-outline",
    "href": "casestudies.html#module-outline",
    "title": "Module: Case Studies",
    "section": "Module outline",
    "text": "Module outline\n\nThe chapters in this module cover various small examples of analyzing data from case studies or research projects that are referenced throughout the modules to show how statistical software and tools can be utilized.\nModule chapters:\n\nCase Study 1\nCase Study 2\nCase Study 3\nCase Study 4\nCase Study 5\nCase Study 6"
  },
  {
    "objectID": "datacs.html#sec-csdelta",
    "href": "datacs.html#sec-csdelta",
    "title": "1  Case study 1",
    "section": "\n1.1 Delta smelt",
    "text": "1.1 Delta smelt\n\n\n\n\nFigure: Delta smelt Source:https://calfish.ucdavis.edu/species/\n\n\n\n\nThe Delta smelt is a small fish that smells like peeled cucumbers, with lifespan of about a year. It is a species that lives in Sacramento-San Joaquin River Delta, where more than half of fresh water in the state is moved through the delta for usage by residents, as well as the agriculture and industry sectors. The delta smelt is keystone species that is sensitive to changes in it’s habitat and its population is endangered. Its decline is of concern since the Delta smelt’s population serves as an indicator of the health of the Delta’s ecosystem as this fish is an important part of the food chain.\nLight intensity is the amount of illumination at the water surface. The turbidity of water is measure haziness or cloudiness of the water. Both of these environmental factors have been found to influence fish behavior. The effects of these environmental factors on the feeding, growth, and survival of larvae were investigated on the Delta smelt, where such understanding would be vital for improving the conservation of this endangered fish species (Tigan et al. (2020)). For their investigation, three sets of rearing trials were conducted where larvae were cultured under different levels of turbidity (measures in nephelometric turbidity units (NTUs)) and light intensity (measured in \\(\\mu mol/m^2/s\\)) . Delta smelt larvae feeding activity was observed throughout the initial adjustment of light and turbidity levels to ascertain feeding ability and behavior at the different levels.\nFor brevity, we focus on the following question: Do levels of light intensity (measured in \\(\\mu mol/m^2/s\\)) affect survival rates of early-stage Delta smelt larvae? \nWe address this question using data on the percentage of early-stage Delta smelt larvae (0–40 days post hatch) that survived when reared under different conditions (low, medium, high) of light intensities, measured in \\(\\mu mol/m^2/s\\).  The data is shown in the table below.\n\n\n\n\n\n\n\nSummary of findings\nNumerical and graphical summaries of the survival of all early-stage Delta smelt larvae (0–40 dph) in the study reared under different light intensities are provided below.\n\n\n\n\n\n(a)\n\n\n\n\nFigure 1.1: Survival of all early-stage Delta smelt larvae (0–40 dph) in the study reared under different light intensities.\n\n\n\n\n\n\n\nTable 1.1:  A comparison of the mean and standard deviation of Survival of all early-stage Delta smelt larvae (0–40 dph) in the study reared under different light intensities. \n \n Light \n    Mean \n    SD \n  \n\n\n low \n    59.606 \n    12.809 \n  \n\n med \n    75.518 \n    12.242 \n  \n\n high \n    58.185 \n    6.345 \n  \n\n\n\n\n\n\n\n\nWhen reared under different conditions of light intensities, the data summaries show that early-stage Delta smelt larvae (0–40 days post hatch) tended to have a higher percentage of survival under medium light intensity (sample mean survival percentage of 75.518) and similar percentages of survival under low and high light intensity. Is the sample mean survival percentage under medium light intensity different enough compared to the mean under low and high light intensity to convince you that survival rates are affected by the strength of light intensity? Using statistical methods to be discussed later, the data provides moderate evidence that survival rates are affected by the strength of light intensity (p-value=.059)\nScope of inference\nTreatments (low, medium, and high light intensities) were imposed on the subjects (early-stage Delta smelt larvae), but the treatment levels were not randomly assigned. Although the data provides suggestive evidence that light intensity affects survival rates, the analysis can not say whether changes in light intensity cause changes in survival rates. While there is a suggestive association between the treatment and rate of survival, other possible explanation may exist. Because the subjects were not randomly selected from some larger population, our findings that survival rates are affected by the strength of light intensity only apply to those subjects in the sample and not some larger population.\n\n\n\n\nTigan, Galen, William Mulvaney, Luke Ellison, Andrew Schultz, and Tien-Chieh Hung. 2020. “Effects of Light and Turbidity on Feeding, Growth, and Survival of Larval Delta Smelt (Hypomesus Transpacificus, Actinopterygii, Osmeridae).” Hydrobiologia 847 (13): 2883–94. https://doi.org/10.1007/s10750-020-04280-4."
  },
  {
    "objectID": "datacs2.html#sec-csmoti",
    "href": "datacs2.html#sec-csmoti",
    "title": "2  Case study 2",
    "section": "\n2.1 Case study on motivation: Does it arise from outside or inside an individual?",
    "text": "2.1 Case study on motivation: Does it arise from outside or inside an individual?\n\n\n\n\nSource: Wikipedia\n\n\n\n\nReward systems are utilized in schools and the workplace, but are rewards operating in the opposite way from what is intended? Do external incentives promote creativity? To address this, researchers investigated whether people would tend to display more creativity when they are thinking about intrinsic or extrinsic motivations Ramsey and Schafer (2013). In this study, undergraduate students of similar creative writing experience were randomly assigned to one of two groups . One group received intrinsic questionnaires and the other received extrinsic questionnaires.\nThe intent of this treatment (the questionnaires) was to have student establish a thought pattern concerning a type of motivation (intrinsic – doing something because it is rewarding or enjoyable; extrinsic – doing something because we want to earn a reward or avoid punishment). Researchers speculated that those who were thinking about intrinsic motivations would display more creativity than subjects who were thinking about extrinsic motivations. All subjects were instructed to write a Haiku, and those poems were evaluated for creativity by a panel of judges\nThe resulting experimental data is shown below which displays the creativity score (Score) and the type of questionnaires (Treatment) assigned for a given undergraduate. Do the data provide evidence that creativity scores are affected by type of motivation?\n\n\n\n\n\n\n\nSummary of findings\nNumerical and graphical summaries of the creativity scores for each motivation type are provided below.\n\n\n\n\n\n(a)\n\n\n\n\nFigure 2.1: Boxplot (a) and histogram (b) of creativity by motivation type\n\n\n\n\n\n\n\nTable 2.1:  The median, IQR, mean, and standard deviation of creativity scores for each group \n \n Treatment \n    Mean \n    SD \n  \n\n\n Extrinsic \n    15.74 \n    5.25 \n  \n\n Intrinsic \n    19.88 \n    4.44 \n  \n\n\n\n\n\nThe creativity scores tended to be lower for the students in the extrinsic group (-4.14 difference in smaple means), with both groups showing similar variablity in scores. Are the scores between the two groups different enough to convince you that those thinking about intrinsic motivations display more creativity than subjects who were thinking about extrinsic motivations? If not, how much larger should the mean be? Answers to such questions require inferential methods that will be discussed in a future chapter.\nFor now, be aware that the data provides convincing evidence that subjects thinking about intrinsic motivations would receive a higher creativity scores than those thinking about extrinsic motivation (two-sided p-value = .006). The 95% confidence interval of (-7.010, -1.277) shows that the mean creativity score for the extrinsic group is expected to be between 1.277 and 7.010 lower than the mean creativity score for the intrinsic group.\nScope of inference\nWe may conclude that the differences in creativity scores between groups was caused by motivation questionnaire type since the treatment (motivation questionnaire type) was randomly assigned to the undergraduates. We may only infer this for the subjects the study and not a larger population since the students were not randomly selected from a population.\n\n\n\n\nAmabile, Teresa M. 1985. “Motivation and Creativity: Effects of Motivational Orientation on Creative Writers.” Journal of Personality and Social Psychology 48 (2): 393–99. https://doi.org/10.1037/0022-3514.48.2.393.\n\n\nRamsey, F. L., and D. W. Schafer. 2013. The Statistical Sleuth: A Course in Methods of Data Analysis. \"Cengage Learning\"."
  },
  {
    "objectID": "datacs3.html#sec-cspm",
    "href": "datacs3.html#sec-cspm",
    "title": "3  Case study 3",
    "section": "\n3.1 Case study on air pollution",
    "text": "3.1 Case study on air pollution\n\n\n\n\nLeft: Smog blanketing Bakersfield. Right: Size comparisons of PM particles\n\n\n\n\nThe effects of air pollution are diverse and numerous, such as increased mortality to increased sensitivity of an ecosystem at high concentrations. The pollutants that pose among the highest risk to California are ground level ozone and particulate matter (PM). California continues to mandate ambient air quality standards that tend to be more stringent than national standards. However, reducing air pollution concentrations to acceptable levels remains an on-going challenge in California where seven and eight of its cities (including Bakersfield and other from the Central Valley) rank in the top 10 of the highest levels of ozone and particulate matter (PM) pollution, respectively (American Lung Association, 2017).\nPM refers to tiny particles of solid or semi-solid material found in the atmosphere, and it is one of the six common air pollutants identified by the U.S. Environmental Protection Agency (EPA, 2018), and it varies in size, but most monitoring is for two size ranges referred to as PM2.5 and PM10 (particulate matter that is less than 10 micrometers in diameter). For this case study, daily measurements of PM10 in micrograms per cubic meter (\\(\\mu g/m^3\\)) are considered from various monitoring stations in three counties (Kern, Tulare, and Fresno) for the year 2021. The monitoring stations provide the daily average PM10 levels, measurements of temperature (\\(^oF\\)) and wind speed (knots or kts).\nIn addition, daily measurements of the U.S. Air Quality Index (an index for reporting air quality) from various monitoring stations for same counties and year are considered. The higher the Air Quality Index (AQI) value, the greater the level of air pollution and the greater the health concern. Visit AirNow for more details regarding the AQI. More details regarding PM10, as well as access to PM and AQI data, are publicly available through the EPA Air Data website (https://www.epa.gov/outdoor-air-quality-data).\nThe following questions are explored for the 2021 data:\n\nDo daily PM10 levels differ for the three counties?\nIs daily PM10 related to daily temperature across all three counties?\n\n\n\n\n\n\n\n\nSummary of findings\nThe sample means show that Tulare county had the highest mean daily PM10 as well as the highest average daily temperature. The boxplots show similar behavior with Tulare county having the highest median daily PM10. The boxplots also reflect outlying values for all counties and similar variation in daily PM10 for the three counties. With regard to the scatterplot, daily Pm10 and daily temperature have a positive linear relationship with the variability of daily PM10 varying less at the lowest or highest daily temperatures. The scatterplot also reflects clear outlying observations.\n\n\n\n\n\nTable 3.1:  The average and standard deviation of daily PM10 and daily temperature for each county. \n \n county \n    mean PM10 \n    mean temperature \n    sd PM10 \n    sd temperature \n  \n\n\n Fresno \n    41.10 \n    65.87 \n    31.88 \n    14.17 \n  \n\n Kern \n    35.07 \n    67.49 \n    32.26 \n    16.01 \n  \n\n Tulare \n    51.54 \n    67.77 \n    32.37 \n    14.22 \n  \n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 3.1: Left panel displays boxplots of PM10 against county. Right panel is a scatterplot of PM10 against temperature\n\n\nAre the features reflected in the numerical and graphically summaries distinct enough to convince one that these levels of air quality measurements do differ by county? Regarding Pm10 and temperature, while they appear to be a positively associated, but is it a strong enough relationship to say that the positive relationship shown is due to more than just random variation?\nUsing statistical methods to be addressed later, the data provides convincing evidence that average daily PM10 levels vary across the three counties (p-value \\(\\approx 0\\)), and that the data provides strong evidence that there that daily PM10 (linearly) related to daily temperature across all three counties (p-value \\(\\approx 0\\)).\nScope of inference\nThis case study make use of observational data, which provided strong evidence that there is an association between the three counties and daily levels of PM10, as well as strong evidence of an association between daily PM10 and daily temperature. The counties were not randomly selected, so these conclusions only apply to the three counties considered here."
  },
  {
    "objectID": "datacs4.html#sec-csob",
    "href": "datacs4.html#sec-csob",
    "title": "4  Case study 4",
    "section": "\n4.1 Case study on patterns of obesity across rural and urban regions",
    "text": "4.1 Case study on patterns of obesity across rural and urban regions\n\n\n\n\nSource: https://hopkinsdiabetesinfo.org/glossary/body-mass-index/\n\n\n\n\nMotivated by the work of “Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults” (2019), a case study by Wright et al. (2020) explored global patterns of obesity across different regions. The questions in their case study were related to the association between Body mass index (BMI) rates with region (rural vs urban) and countries. BMI may be considered a measurement of health, where BMI is defined as an individual’s weight divided by the individual’s height. The data used for the case study is described “Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults” (2019), where the data consisted of the following variables: - Country: Countries in the sample - Sex: Men or women. The data recorded only contained data for groups of individuals described as men or women. - Region: Type of region (rural or urban) - Year: 1985 or 2017 - BMI: Averaged BMI values for the given year, sex, region, and country. We use the data to assess if BMI rates for urban and rural differ, whether BMI rates differ by sex. The data is shown in table below.\n\n\n\n\n\n\n\nSummary of findings\nNumerical and graphical summaries of the BMI under different regions and sex are provided below:\n\n\n\n\n\n(a)\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 4.1: Survival of all early-stage Delta Smelt larvae (0–40 dph) in the study reared under different light intensities.\n\n\n\n\nA comparison of the mean and standard deviation of BMI rates for each region and sex\n\n\n\n\n\n\n Sex \n    Mean \n    SD \n  \n\n\n Men \n    24.500 \n    2.676 \n  \n\n Women \n    25.262 \n    2.918 \n  \n\n\n\n\n\n\n\n\n\n Region \n    Mean \n    SD \n  \n\n\n Rural \n    24.453 \n    2.947 \n  \n\n Urban \n    25.304 \n    2.632 \n  \n\n\n\n\n\nThe graphical and numerical summaries show that the BMI rates for urban areas tend to be higher in the sample. Regarding sex, men in the sample tend to have lower BMI rates. However, are the samples means different enough to suggest that BMI rates are affected by region, or that one sex has a higher BMI rate than the other? Using statistical methods to be addressed later, the data provides convincing evidence that region and sex are jointly significant in affecting BMI rates (p-value$<$0)\nScope of inference\nThis case study make use of observational data, which provided convincing evidence that there is an association between BMI rates with both region and sex. The sample were not randomly selected, so these conclusions only apply the men and women from the counties considered in the sample.\n\n\n\n\n“Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults.” 2019. Nature 569 (7755): 260–64.\n\n\nWright, Carrie, Leah Jager, Margaret Taub, and Stephanie Hicks. 2020. “Exploring Global Patterns of Obesity Across Rural and Urban Regions.” Open Case Studies Version v1.0.0. https://github.com/opencasestudies/ocs-bp-rural-and-urban-obesity."
  },
  {
    "objectID": "datacs5.html#sec-csss",
    "href": "datacs5.html#sec-csss",
    "title": "5  Case study 5",
    "section": "\n5.1 Case study on school shootings",
    "text": "5.1 Case study on school shootings\n\n\n\n\nStudents from Miguel Contreras Learning Center High School in Los Angeles demonstrate in front of City Hall after walking out of school to protest U.S. gun violence on May 31, 2022. Photo by Lucy Nicholson, Reuters Source: https://calmatters.org/newsletters/whatmatters/2022/07/mass-shooting-california-gun-laws/\n\n\n\n\nSchool shootings occur more often in the US than any other country (Mosechkin and Krukovskiy (2019)). The K-12 School Shooting Database (Riedman (2022)) contains data on every instance a gun is brandished, is fired, or a bullet hits K-12 school property. Recently, Hilaire et al. (2022) used this data source to investigates the relationship between perpetrators’ race and how shootings are reported by the media. They found that there were differences by race in the characteristics of school shootings and media reporting of school shootings. Reeping et al. (2022) found that more permissive firearm laws and higher rates of gun ownership were associated with higher rates of school shootings. The latest raw data (Jan 1970-Nov 2022) was provided by the founder and maintainer of the K-12 School Shooting Database, David Riedman. The provided data set was processed and then augmented with state gun ownership data (Learish and Fieldstadt (2022)). Using the latest data, we explore whether the highest level (Local, Regional, or National) of media coverage of the incident differs across age groups, and we examine if the number of victims is associated with the type of weapon(s) used.\nIn all, after augmenting the data provided on shooting incidents, the data consists of 46 variables. The variables included:\n\n\nSchool_Level: education-level of the school attacked as (1) high school, (2) junior high school, (3) middle school, (4) elementary school, (5) other.\n\nYear: year\n\nMonth: month\n\nQuarter: quarter of the year\n\nYear: 1985 or 2017\n\nNumber_Victims: Number of victims killed or wounded\n\nMedia_Attention: Highest level of media coverage (local, regional, or national)\n\nTime_Period: Time period in which the incident occurred (After school, Afternoon classes, … , Sport Event)\n\nagegroup: Age of the shooter (child, teen, adult)\n\nState: State where in incident occurred.\n\nWeapon_Type: The type of weapon used ()\n\nThe reader is referred to K-12 School Shooting Database data methodology page for more details regarding variables in the data set.\nThe data is shown in table below.\n\n\n\n\n\n\n\nSummary of findings\nNumerical and graphical summaries of the relevant variables are provided below:\n\n\n\n\n\n(a) Barchart\n\n\n\n\n\n\n(b) Violin boxplot\n\n\n\n\nFigure 5.1: Graphically summaries\n\n\n\n\nTop table: Distribution of the 'Media Attention' levels. Bottom table: The distribution of 'Media Attention' conditional on the levels of 'Age Group'.\n\n\n\n\n\n\n Media Attention \n    Proportion \n  \n\n\n International \n    0.0351391 \n  \n\n Local \n    0.5797950 \n  \n\n National \n    0.1800878 \n  \n\n Regional \n    0.2049780 \n  \n\n Total \n    1.0000000 \n  \n\n\n\n\n\n\n\n\n\n\n\nMedia Attention\n\n\n   \n    International \n    Local \n    National \n    Regional \n  \n\n\n\n Adult \n    0.4166667 \n    0.4469697 \n    0.2845528 \n    0.3500000 \n  \n\n Child \n    0.0000000 \n    0.0328283 \n    0.0243902 \n    0.0428571 \n  \n\n Teen \n    0.5833333 \n    0.5202020 \n    0.6910569 \n    0.6071429 \n  \n\n Total \n    1.0000000 \n    1.0000000 \n    1.0000000 \n    1.0000000 \n  \n\n\n\n\n\n\n\n\n\n\nTable 5.1:  A comparison of the median and interquartile range of the number victims under each weapon type used by the shooter. Only data in which the weapon was known was used. \n \n Weapon_Type \n    Median \n    IQR \n  \n\n\n Handgun \n    1.0 \n    1.0 \n  \n\n Multiple Handguns \n    1.5 \n    3.0 \n  \n\n Multiple Rifles \n    3.5 \n    5.0 \n  \n\n Other \n    0.0 \n    1.0 \n  \n\n Rifle \n    1.0 \n    2.5 \n  \n\n Shotgun \n    1.0 \n    3.0 \n  \n\n\n\n\n\nThe bar graph suggests that the level of media attention in the data varies by age group. The tables show that highest level of media attention varies, with most incidents receiving at most local media attention. The second table reflects the proportion of the type of highest media attention but conditional on the age group of the shooter. The proportions reflected in these tables are consistent with the idea that the highest level of media attention received is associated with the age of the shooter. The violin boxplot reflects that the number of victims do differ depending on the type of weapon(s) used and that are more outlying observations when rifles or shotguns was the weapon used. Using statistical methods to be addressed later, the data provides moderate evidence that the level of media attention is associated with the age group of the shooter (p-value \\(\\approx\\) 0.0324), and that there is strong evidence that the number of victims varies significantly with the type of weapon(s) used (p-value \\(\\approx\\) 0).\nScope of inference\nThis case study make use of observational data, which provided convincing evidence that there is an association between both sets of variables considered. The sample were not randomly selected, so these conclusions only apply to the sample.\n\n\n\n\nHilaire, Breahannah, Laurie O Campbell, Viki P Kelchner, Eric D Laguardia, and Cassandra Howard. 2022. “Not Another School Shooting: Media, Race, and Gun Violence in k-12 Schools.” Education and Urban Society, 00131245221092739.\n\n\nLearish, J., and E. Fieldstadt. 2022. “Gun Map: Ownership by State.” CBS News. https://www.cbsnews.com/pictures/gun-ownership-rates-by-state/.\n\n\nMosechkin, Ilya N, and Vladimir Y Krukovskiy. 2019. “Victimological Measures for Preventing School Shootings: Expert View.” International Journal of Criminal Justice Sciences 14 (2): 256–66.\n\n\nReeping, Paul M, Louis Klarevas, Sonali Rajan, Ali Rowhani-Rahbar, Justin Heinze, April M Zeoli, Monika K Goyal, Marc A Zimmerman, and Charles C Branas. 2022. “State Firearm Laws, Gun Ownership, and k-12 School Shootings: Implications for School Safety.” Journal of School Violence 21 (2): 132–46.\n\n\nRiedman, David. 2022. “K-12 School Shooting Database.” https://k12ssdb.org."
  },
  {
    "objectID": "data.html#module-outline",
    "href": "data.html#module-outline",
    "title": "Module: Data basics and drawing statistical conclusions",
    "section": "Module outline",
    "text": "Module outline\n\nThe chapters in this module cover the following topics:\n\nProperties of data\nStudy design\nScope of inference\nInference permitted based on the study design\n\nModule chapters:\n\nCase studies\n\nData summaries\nDrawing statistical conclusions"
  },
  {
    "objectID": "drawingconclusions.html#populations-samples-and-collecting-data",
    "href": "drawingconclusions.html#populations-samples-and-collecting-data",
    "title": "8  Drawing statistical conclusions",
    "section": "\n8.1 Populations, samples, and collecting data",
    "text": "8.1 Populations, samples, and collecting data\nThe first step in conducting research is to identify topics or questions that are to be investigated. However, it is important to consider how data are collected so that they are reliable and help achieve the research goals. Important concepts related to data collection:\n\nPopulation: The entire collection of objects or individuals that you want to study.\nIndividual, case, or subject : A person or an object that is a member of the population being studied\nSample: A part of the population that is selected for the study.\n\nIf the goal is to draw conclusions about the population of interest, rather than just the cases or subjects in the sample, a representative sample obtained through random sampling is required.\nCommon sampling techniques include:\n\nSimple random sampling (SRS) – Randomly and independently select cases from the population, each case is equally likely to be selected.\n\n\n\nFigure 8.1: Simple random sampling\n\n\n\nStratified sampling – Population is divided into groups called strata. The strata are homogeneous and an SRS is taken from each stratum.\n\n\n\nFigure 8.2: Stratified sampling\n\n\n\nCluster sampling – Population is divided into groups called clusters that are not necessarily homogeneous. A random sample of clusters is selected and all observations from each sampled cluster are used.\n\n\n\nFigure 8.3: Cluster sampling\n\n\n\n\n\n\n\n\nA note about the size of the sample\n\n\n\nCommon misconception: If the size of a sample is relatively small compared to the population size, the sample cannot possibly accurately reflect the population. Not true! The random selection process allows you to be confident that the resulting sample will reflect the population, even when the sample consists of only a small fraction of the population.\nStatistical inferential methods always account for the sample size. The larger the sample size, the more information is available, which results in a more precise answer compared to a smaller sample size with less information."
  },
  {
    "objectID": "twosample.html#module-outline",
    "href": "twosample.html#module-outline",
    "title": "Module: Two sample methods",
    "section": "Module outline",
    "text": "Module outline\nThis module discuss two-sample inference but first discusses the basic frame work for inference through the introduction to things that one should consider when comparing aspects of:\n\nTwo distinct populations (e.g, delivery times of Uber Eat vs. Doordash)\nTwo different treatments applied to one population (e.g., the effect of taking a drug vs. a placebo on depression).\n\nThis involves the process of sampling data from a population or obtaining data via a randomize experiment to determine what can be inferred about the true effect or population based on sample results. Statistical inference helps us answer two questions about the population or experiment:\n\nHow strong is the evidence of an effect?\nHow large is the effect?\n\nThe first question is addressed using hypothesis testing, while the second question is address by a confidence interval.\nWhile this framework is illustrated through two-sample method, this framework will extends to any inferential procedures discussed in the modules.\n\nModule chapters:\n\nhypothesis testing and confidence interval framework\nInference for comparing two independent means using\n\nt-based methods\nnon-parametric methods\nsimulation methods"
  },
  {
    "objectID": "twoindptmeans.html#wilcoxon-based-methods-comparing-two-medians",
    "href": "twoindptmeans.html#wilcoxon-based-methods-comparing-two-medians",
    "title": "12  Two sample methods",
    "section": "\n12.2 Wilcoxon-based methods comparing two medians",
    "text": "12.2 Wilcoxon-based methods comparing two medians\nWhen assumption 2 and/or 3 do not hold, t-based methods are no longer applicable. The Wilcoxon-based methods do not require the normality assumption when the samples are small nor is it affected by outliers. Further, the distributions from which each data were sampled do not have to be known5 for these methods, but these methods do require for the distributions to have approximately the same shape. Since the distribution does not have to be a known distribution, it called a non-parametric method6. In short, these methods require assumptions 1 and 4, but also require for the distributions from which each data were sampled to have approximately the same shape and for each sample to be of at least size 10.\nThe Wilcoxon rank sum test, also called the Mann-Whitney test, can be applied to compare whether the difference between true medians from two independent populations is really different from zero. It can also be used to determine if an effect (when comparing two treatments via medians) is significant or not, as opposed to the size of the difference or effect being due to random chance.\nThe test statistic for the Wilcoxon rank sum test depends on the ranks of the observed data, and its value will be provided by software. However, some details regarding the computation of the test statistic are provided below:\n\nList the observations for both samples from smallest to largest across both groups.\nAssign the numbers \\(1\\) to \\(n_A+n_B=N\\) to the observations (across both groups) with 1 assigned to the smallest observations and \\(N\\) to the largest observation. These are called ranks of the observations.\nIf there are ties (due to repeated values) in the combined data set, the ranks for the observations in a tie are taken to be the average of the ranks for those observations.\nLet \\(W\\) denote the sum of the ranks for the observations from group A.\n\nThe test statistic then takes the form\n\\[U=W-\\frac{n_A(n_A+1)}{2}\\]\nWhereas the null hypothesis of the two-sample t test is equal means, the null hypothesis of the Wilcoxon test7 is taken as equal medians:\n\\[H_0: \\eta_A - \\eta_B=0 \\qquad H_a: \\eta_A -\\eta_B \\neq 0\\]\nOne may also consider the test in terms of an effect \\(\\delta_\\eta=\\eta_A - \\eta_B\\),\n\\[H_0: \\delta_\\eta=0 \\qquad H_a: \\delta_\\eta \\neq 0\\]\nUnder \\(H_0\\), the null distribution of U is called the Wilcoxon distribution and it is used to compute the p-value8. The function two.wilcox.test()9 computes the test statistic and p-value10, as well as a CI.\nThe CI provided by this method is for the difference in medians between two randomly chosen observations (one from each group)11. This still provides some sense of how different the medians are from each other. However, if both distributions from which the data were sampled are symmetric, then the CI provided is for a difference in medians.\n\n\n\n\n\n\nR functions\n\n\n\n### two.wilcox.test( y ~ x , data , first.level,\n###            direction, conf.level)\n# y: Replace y with the name of the resposne (measured) \n#  variable of interest\n# x: Replace x with the name of the factor or grouping\n#  variable that distinguishes the different populations \n#  or treatments\n# data: Set equal to the dataframe being used.\n# first.level: Set equal to a level/category from the grouping \n#              variable. It determines how the difference in \n#              sample means is computed.  It should be consisent \n#              with the formulation of the hypothesis.\n# direction: Set equal to the sign in the alternative: \"two.sided\", \n#            \"greater\" , or \"less\"\n# conf.level: Set equal to the confidence level for the CI (default \n#             is .95). The function will always provide a CI by default.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe apply the two-sample Wilcoxon-based methods (hypothesis testing and CI) to the data described in the case study provided in Chapter 3 to determine if there is a significant difference in PM10 concentration between Kern and Fresno county and to get a sense of how different the true medians are. The four assumptions of the data discussed earlier in this chapter were explored when illustrating the t-based methods. Although the data consists of large samples and the independence and equal variance assumptions are reasonable, the presence of many outliers in the data should be noted. The Wilcoxon-based methods are not affected by outliers but the distributions from which each sample were obtained should have the same shape. This is explored via a violin boxplot.\n\n\nR code\nVideo\n\n\n\n\nCodelibrary(lattice) # Provides the bwplot() function\n\n# Create a violin plot \nbwplot( PM10 ~ county , \n    data= KernFresnoPM10df , \n    xlab= \"County\" , \n    ylab= \"PM10\" ,\n    main= \"PM10 vs county\" , \n    panel= panel.violin ) # set equal to panel.violin to create violin plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe violin boxplot makes it clearer that the distribution of PM10 in both counties approximately have the same shape.\nLet \\(\\eta_K\\) denote the true median PM10 levels in Kern county. \\(\\eta_F\\) is defined analogous for Fresno county. Since an aim is to determine if there is significant difference between PM10 levels in Kern and Fresno county, the hypothesis is\n\\[H_0: \\eta_K - \\eta_F=0 \\qquad H_a: \\eta_K- \\eta_F \\neq 0\\]\nThe following code uses two.wilcox.test() to compute the test statistic, the corresponding p-value, and a 99% confidence interval:\n\n\nR code\nVideo\n\n\n\n\nCode# Source the function two.wilcox.test() so that it's R's memory.\nsource(\"rfuns/two.wilcox.test.R\")\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: The county that appears first in the hypothesis\n#                (Kern county), set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n\n\ntwo.wilcox.test(PM10 ~ county ,           # y ~ x\n                data = KernFresnoPM10df , # Specify the data frame to use\n                first.level = \"Kern\" ,    # Specify the first level in the hypothesis\n                direction = \"two.sided\" , # Specify a two-sided alternative\n                conf.level = .99 )        # Set the confidence level to .99\n#>      Theoretical-based two-sample test for independent samples \n#>                               \n#> formula:  PM10 ~ county \n#> sample median of  Kern  group: 28 \n#> sample median of  Fresno  group: 34 \n#> sample IQR of  Kern  group: 30 \n#> sample IQR of  Fresno  group: 29 \n#> \n#> method:  Wilcoxon rank sum test with continuity correction \n#> difference between groups: ( Kern  group ) - (  Fresno  group ) \n#> obs test statistic: U=  759219.5              p-value = 0 \n#> obs standardized test statistic: Z=  -7.048 \n#> direction: two.sided \n#> \n#> difference in location (pseudomedian):  -6.000007 \n#> confidence level:  0.99 \n#> CI:(  -8.000049 ,  -3.999966 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output includes the following information:\n\nThe sample median and IQR under each level of the grouping/factor variable.\nThe value of the observed statistic\np-value\nA confidence interval\n\nBased on the output, the observed value of the test statistic is \\(U=759219.5\\). The p-value is extremely small (0). Since p-value\\(\\leq\\alpha=.01\\), we have significant evidence to reject \\(H_0\\). Therefore, we have strong evidence that there is significant difference between PM10 levels in Kern and Fresno county at the designated monitoring sites.\nThe output also provides a 99% CI for median of difference between all pairs of observations between both groups (not the difference in group medians). Based on the CI, we are 99% confident that the median difference between two randomly chosen PM10 levels from each county is anywhere between -8.000 and -4.000. Since the interval does not contain \\(0\\), it is clear that the PM10 levels at Fresno county are higher than in Kern county at the designated monitoring sites at 99% confidence. Note that this conclusion only applies to the monitoring sites, since PM10 levels were not measured at random locations within each county."
  },
  {
    "objectID": "basics.html#video-1",
    "href": "basics.html#video-1",
    "title": "7  Basics of data",
    "section": "\n7.5 Video",
    "text": "7.5 Video"
  },
  {
    "objectID": "a3-basicsR.html#exporting-graphs",
    "href": "a3-basicsR.html#exporting-graphs",
    "title": "Appendix C — Some basic tasks in R",
    "section": "\nC.5 Exporting graphs",
    "text": "C.5 Exporting graphs\nThere are 3 methods for exporting a plot in R. After creating a plot, go to the Plots tab in the lower right pane. Find the “Export” button located in the upper right corner of the plot tab. Click on the “Export” button and three options will be given:\n\n“Save Plot as Image”: This option allows you to save the plot as an image file (such as png or jpg). Click on this option, and a box will open where you can choose the file format, resolution, and dimensions for the exported image. After selecting the options, click “Save” to save the image file to your computer.\n“Save Plot as PDF”: This option allows you to save the plot as a PDF file. Click on this option, and a box will open where you can choose the file name, dimensions, and other options for the exported PDF. After adjusting the settings, click “Save” to save the PDF file to your computer.\n“Copy Plot to Clipboard”: This option allows you to copy the plot to your clipboard. You can then paste the plot directly into other applications (such as Word, PowerPoint, etc.)as an image."
  },
  {
    "objectID": "catsum.html#video-3",
    "href": "catsum.html#video-3",
    "title": "9  Summarizing categorical data",
    "section": "\n9.3 Video",
    "text": "9.3 Video"
  },
  {
    "objectID": "numsum.html#video-4",
    "href": "numsum.html#video-4",
    "title": "10  Summarizing numerical data",
    "section": "\n10.2 Video",
    "text": "10.2 Video"
  },
  {
    "objectID": "index.html#organization-of-the-materials",
    "href": "index.html#organization-of-the-materials",
    "title": "Training modules on selected statistical methods",
    "section": "Organization of the materials",
    "text": "Organization of the materials\nThe materials are organized based on the typical order in which these topics are introduced in practice, though the section on principal component analysis can be studied after covering basic exploratory data analysis. Sections generally follow a similar pattern:\n\nIntroduction of core material\nPresentation of relevant R functions\nApplication and illustration of core material and R code through case studies\n\nThose new to R should work through the appendices. If you have used R in the past, but have not imported .csv or .xlsx files, or used a formula expression of the form response ~ explanatory, then sections Section C.1 and Section C.2 should be reviewed.\nMost sections include embedded R code. R functions are generally described only when first introduced. Additionally, code to import specific data sets or load particular R packages may appear multiple times in different modules. Any subsequent uses of a given R function or its description may not be provided, but readers can search for the R function name using the search bar in the upper left corner of the module to find where it was first introduced and where it has been used."
  },
  {
    "objectID": "a3-basicsR.html#using-custom-r-functions",
    "href": "a3-basicsR.html#using-custom-r-functions",
    "title": "Appendix C — Some basic tasks in R",
    "section": "\nC.6 Using custom R functions",
    "text": "C.6 Using custom R functions\nSometimes, this resource will require the usage of what may be called custom functions. Some of these custom functions are wrappers1 for R functions that come with R, while others are entirely new functions that were created. The reason for creating these custom functions is to provide a more consistent and streamlined way to perform certain tasks.\nAny time custom functions are used, they must first be sourced. For example, suppose a function called my_function() is provided by the file my_functions.R, and that this file is located in the following file path or location: C:/Users/YourName/Documents/my_functions.R. This function is saved in the “Documents” folder. If the code below is executed, it would source the file my_function.R so that the R my_function() function is in R’s memory:\nsource(\"C:/Users/YourName/Documents/my_functions.R\")\nNote that any time you start a new RStudio session, you would need to source the my_functions.R file again in order to use the my_function() function. This is because R does not automatically remember custom functions that were sourced in previous sessions.\nTo use the custom functions provided in this resource, you will need to download them from the repository and update the file path in the source() function to reflect the location where you saved the functions on your own system."
  },
  {
    "objectID": "twoindptmeans.html#sample-size-estimation-and-power-analysis",
    "href": "twoindptmeans.html#sample-size-estimation-and-power-analysis",
    "title": "12  Two sample methods",
    "section": "\n12.4 Sample size estimation and power analysis",
    "text": "12.4 Sample size estimation and power analysis\nIdeally, estimating sample size for a study is one of the first steps that researchers take prior to collecting data. Knowing the sample size required to detect a desired effect at the beginning of a project allows one to manage their data collection efforts. Further, this allows for one to determine how much statistical power the test will have to detect an effect.\nThe R function pwr.t2n.test() from the pwr R package can be used to calculate statistical power for a two sample t-test when the sample sizes, significance level (\\(\\alpha\\)), and effect size are provided. The function pwr.t.test from the same package will provide the sample sizes required for a given power level, significant level (\\(\\alpha\\)), and effect size. Note that variability in each sample is assumed to be about the same. It is assumed the appropriate assumptions about the data are met12.\n\n\n\n\n\n\nR functions\n\n\n\n### power.t2n.test( n1 , n2 , d , sig.level , \n###                    power, alternative)\n# n1: Set equal to the number of observations in first sample.\n# n2: Set equal to the effect size.\n# d: Set equal to the effect size.\n# sig.level: Set equal to the desired alpha value (significance level)\n# power: Set equal to the desired power (a number between 0 and 1). \n# alternative: The direction of the alternative. Set equal to \"two.sided\".\n#\n#\n### power.t.test(power, d , sig.level , \n###                  alternative , type)\n# power: Set equal to the desired power.\n# type: Set equal to \"two.sample\" .\n#\n\n\nThe effect size refers to Cohen’s \\(d\\), which is defined as the difference between the true means divided by the pooled standard deviation. Cohen’s \\(d\\) is typically interpreted as follows:\n\nSmall effect size: \\(d = 0.20\\)\nMedium effect size: \\(d = 0.50\\)\nLarge effect size: \\(d = 0.80\\) or higher.\n\nThese are suggested guidelines and may vary slightly depending on the specific field of research or context of the study.\n\n\n\n\n\n\nNote\n\n\n\nRefer to the data described in the case study provided in Chapter 3. Let’s suppose the goal is to determine the statistical power of a two sample t-test if one wanted to detect a medium-sized effect when the sample size from each county is 100 at \\(\\alpha=.01\\).\n\nCode\nlibrary(pwr) # provides pwr.t2n.test\n\n# Note:  The output of this function call will be the statistical power for the t-test.\npwr.t2n.test(n1 = 100 ,          # Set the sample size for group 1\n             n2 = 100 ,          # Set the sample size for group 2\n             d = 0.50 ,          # Set Cohen's d\n             sig.level = 0.01 ,  # Set the significance level for the test\n             alternative = \"two.sided\") # Specify a two-sided alternative hypothesis\n#> \n#>      t test power calculation \n#> \n#>              n1 = 100\n#>              n2 = 100\n#>               d = 0.5\n#>       sig.level = 0.01\n#>           power = 0.8238225\n#>     alternative = two.sided\n\n\nThe statistical power of this test is \\(.824\\). To further increase the power, one may increase the effect size (the larger it is, the easier it is to detect), increase the value of \\(\\alpha\\) (make it easier to reject \\(H_0\\) and find a significant effect), and/or increase the sample sizes. The sample data consisted of at least 1000 observations from each county, so set each sample to \\(1000\\):\n\nCodepwr.t2n.test(n1 = 1000 ,\n             n2 = 1000 , \n             d = .50 , \n             sig.level = 0.01 ,\n             alternative = \"two.sided\" )\n#> \n#>      t test power calculation \n#> \n#>              n1 = 1000\n#>              n2 = 1000\n#>               d = 0.5\n#>       sig.level = 0.01\n#>           power = 1\n#>     alternative = two.sided\n\n\nIf instead you seek the required sample size for a given power, the argument power could be set to the desired power in the function pwr.t.test():\n\nCode# Note:  The output of this function call will be the required sample size for the t-test.\npwr.t.test(power = 0.90 ,         # Set the desired power level\n           d = 0.50 ,             # Set the standardized mean difference between groups\n           sig.level = 0.01 ,     # Set the significance level for the test\n           type = \"two.sample\" ,  # Set a two-sample t-test\n           alternative = \"two.sided\") # Specify a two-sided alternative hypothesis\n#> \n#>      Two-sample t test power calculation \n#> \n#>               n = 120.7055\n#>               d = 0.5\n#>       sig.level = 0.01\n#>           power = 0.9\n#>     alternative = two.sided\n#> \n#> NOTE: n is number in *each* group\n\n\n\n\nThis module provides a few methods for independent two-sample inference. Parametric methods (such as t-based methods) are generally more powerful (assuming all assumptions are reasonably met) than nonparametric methods, such as Wilcoxon-based methods. Nonparametric tests are based on fewer assumptions compared to their parametric counterparts. The cost of fewer assumptions is that nonparametric tests are generally less powerful than their parametric counterparts. Nonparametric tests are used in cases where parametric tests are not appropriate.\n\n\n\n\nRamsey, F. L., and D. W. Schafer. 2013. “The Statistical Sleuth: A Course in Methods of Data Analysis.” Cengage Learning 30 (4): 413–14. https://doi.org/10.1080/00224065.1998.11979882."
  },
  {
    "objectID": "preface.html#organization-of-the-materials",
    "href": "preface.html#organization-of-the-materials",
    "title": "Preface",
    "section": "Organization of the materials",
    "text": "Organization of the materials\nThe materials are organized based on the typical order in which these topics are introduced in practice, though the section on principal component analysis can be studied after covering basic exploratory data analysis. Sections generally follow a similar pattern:\n\nIntroduction of core material\nPresentation of relevant R functions\nApplication and illustration of core material and R code through case studies"
  },
  {
    "objectID": "preface.html#r-and-rstudio",
    "href": "preface.html#r-and-rstudio",
    "title": "Preface",
    "section": "R and RStudio",
    "text": "R and RStudio\nThose new to R and RStudio should work through the appendices. If you have used R or RStudio in the past, but have not imported .csv or .xlsx files, or used a formula expression of the form response ~ explanatory, then Chapter C should be reviewed.\nMost sections include embedded R code. R functions are generally described only when first introduced. Additionally, code to import specific data sets or load particular R packages may appear multiple times in different modules. Any subsequent uses of a given R function or its description may not be provided, but readers can search for the R function name using the search bar in the upper left corner of the module to find where it was first introduced and where it has been used."
  },
  {
    "objectID": "preface.html#callouts-blocks",
    "href": "preface.html#callouts-blocks",
    "title": "Preface",
    "section": "Callouts blocks",
    "text": "Callouts blocks\nThroughout this resource, you will encounter callout boxes in various colors, each of which serves a specific purpose. The callout boxes are color-coded as follows:\n\nOrange: Data assumptions related to the statistical method discussed.\nBlue: Case studies or examples to illustrate the statistical method using real-world data using R.\nGreen: R functions and their arguments.\nRed: Notation or terminology that is important for understanding the statistical method, or additional notes, tips, or warnings related to the material.\n\nThe color-coded system will hopefully make it easier to navigate and understand the statistical methods presented in this resource"
  },
  {
    "objectID": "anova.html#footnotes",
    "href": "anova.html#footnotes",
    "title": "13  Analysis of variance (ANOVA)",
    "section": "",
    "text": "The MST is also commonly referred to as the mean square between groups (MSG).↩︎\nThe formulation of Welch’s F-test for single-factor ANOVA is much more complicated but a weighted ratio of mean squares is the basic idea.↩︎\nsfaov() is a wrapper function for aov(), oneway.test(), and TukeyHSD() and it incorporates additional options. A wrapper function is a function that calls another function or a set of functions to simplify or extend tasks↩︎\nSS(Treatment) refers to the sum of squares Treatment. The rest of table elements are defined in an analogous manner for the SS, df, and MS. SS(Total) is a measure of the total variation measures in the data, ignoring the groups or treatments. The sum of squares treatment or SS(Treatment) measures far the individual sample means are from the overall sample, and the sum of squares error error or SS(Error) measures how far observations are from the sample mean of its group.↩︎\nsfkw() is a wrapper function for kruskal.test() but it incorporates additional options.↩︎\nSignificance or \\(\\alpha\\) level equals one minus the confidence level. For example, if \\(\\alpha=.05\\), the corresponding confidence level is \\(1-\\alpha=1-.05=.95\\).↩︎\nThe aim of Dunn’s test is to provide a set of p-values that represent the probability of observing the observed difference in median ranks between two groups by chance alone. Since Dunn’s test does not rely on any distributional assumptions like Welch’s F-test or the standard F-test, it is not possible to calculate confidence intervals for the differences between the groups.↩︎\nThere are R functions to compute for the Kruskal-Wallis test or Welch’s F-test, these functions may be difficult to use for those not familiar with power analysis.↩︎"
  },
  {
    "objectID": "pca.html#video-3",
    "href": "pca.html#video-3",
    "title": "16  PCA",
    "section": "\n16.5 Video",
    "text": "16.5 Video"
  },
  {
    "objectID": "pca.html#r-code-3",
    "href": "pca.html#r-code-3",
    "title": "16  PCA",
    "section": "\n16.5 R code",
    "text": "16.5 R code\n\nCode# Recall the principal object is `xylemPCA`.\n# Save the result to an object\nPCAloadings <- loadings( xylemPCA )\n\n# display the loadings\nPCAloadings # loadings smaller than .1 are not displayed\n#> \n#> Loadings:\n#>               PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8    PC9   \n#> P75            0.783         0.511        -0.139 -0.175  0.122  0.243       \n#> Ks             0.661         0.394 -0.560  0.174  0.114 -0.209              \n#> starch         0.675  0.309  0.158  0.491  0.274  0.242 -0.218              \n#> xylem_density -0.630  0.521  0.287 -0.114  0.209  0.282  0.335              \n#> fiber_%       -0.781 -0.425  0.393  0.141                              0.143\n#> vessel_%       0.721        -0.492         0.408 -0.154  0.164              \n#> par_%          0.444  0.704 -0.242        -0.482                            \n#> Pmin           0.813 -0.142  0.376  0.230 -0.109         0.229 -0.234       \n#> water_storage  0.500 -0.672 -0.244        -0.210  0.422  0.112              \n#> \n#>                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9\n#> SS loadings    4.14 1.523 1.180 0.650 0.627 0.392 0.318 0.132 0.039\n#> Proportion Var 0.46 0.169 0.131 0.072 0.070 0.044 0.035 0.015 0.004\n#> Cumulative Var 0.46 0.629 0.760 0.832 0.902 0.946 0.981 0.996 1.000"
  },
  {
    "objectID": "a3-basicsR.html#adapting-the-r-code-provided-for-your-data-and-interest",
    "href": "a3-basicsR.html#adapting-the-r-code-provided-for-your-data-and-interest",
    "title": "Appendix C — Some basic tasks in R",
    "section": "\nC.4 Adapting the R code provided for your data and interest",
    "text": "C.4 Adapting the R code provided for your data and interest\nIn this section, we demonstrate how to adapt the provided code to your own data frame and variables of interest by using formula expressions with examples from two different datasets. Recall that most functions will have a layout of task( y ~ x , data) or task( ~ x , data). For this illustration, we use two dataframes, ChickWeight and trees dataframe. Both of these dataframes are built-in with R. Note the following variable names in each dataframe:\n\nCode# \"ChickWeight\" measurements on weight, time since birth, \n# age, and diet plan of chicks\nnames( ChickWeight )\n#> [1] \"weight\" \"Time\"   \"Chick\"  \"Diet\"\n\n# \"trees\" dataset contains the height, girth,\n# and volume measurements for 31 trees\nnames( trees ) # this dataset comes built-in with R\n#> [1] \"Girth\"  \"Height\" \"Volume\"\n\n\nA univariate summary:\nThe following computes the mean weight of the chicks:\n\nCode# Compute the mean weight\nmean( ~ weight  , \n      data=  ChickWeight )\n#> [1] 121.8183\n\n\nNote that weight is spelled weight and not Weight or WEIGHT since it appears as weight in the dataframe.\nThe code below computes the mean volume of the 31 trees:\n\nCode# Compute the mean\nmean( ~ Volume  , \n      data=  trees )\n#> [1] 30.17097\n\n\nNotice the similarity between these two commands. The only differences are the names of the data frames and the variable names (spelled exactly as they appear in the respective data frames, keeping in mind that R is case sensitive). The general form used here is: task( ~ x, data )\nA bivariate summary:\nThe following creates a scatterplot of the weight of a chick against the time since birth:\n\nCode# Load the R package lattice\nlibrary(lattice) # provides xyplot() \n\n# create the scatterplot\nxyplot(weight ~ Time  , data=  ChickWeight  )\n\n\n\n\n\n\n\nThe code below plots the volume of a tree against height of a tree:\n\nCode# create the scatterplot\nxyplot( Volume ~ Height  , data=  trees  )\n\n\n\n\n\n\n\nNotice the similarity between these two commands. The only differences are the names of the data frames and the variable names (spelled exactly as they appear in the respective data frames, keeping in mind that R is case sensitive). The general form used here is: task( y ~ x, data )"
  }
]