{"title":"Two sample methods","markdown":{"headingText":"Two sample methods","headingAttr":{"id":"sec-indpttwosamp","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n![AI art generated by keywords \"two-sample t-test\"](images/twosampmod/histart.jpg){#fig-hist_art}\n\nConsider comparing independent populations in regard to the mean or median of some attribute, or two treatments by comparing the true mean or median response under each treatment in an experiment. In this section, the following hypothesis tests (HT) for such comparisons are illustrated:\n\n- *t*-based or t-test based for comparing two means  \n\n- Wilcoxon rank sum test for comparing two medians  \n\n- Randomization test for comparing two means or two medians \n\nFor confidence intervals (CIs), an overview of the two-sample $t$-based CI for comparing means and Wilcoxon-based CI for comparing medians is provided. Equations/formulas used in HT or CIs will be kept at a minimum as the focus is on using software to carry out these methods. That being said, those new to or not comfortable with HT or CIs should consider reviewing @sec-sifwork. These methods will be illustrated using a case study.\n\nFor any statistical inferential method to provide meaningful results, several assumptions are made about the data on which the method is intended to be used.\n\n:::{.callout-warning}\n## Data assumptions\n\nThe methods discussed in this section one or more of the following assumptions about the data.\n\n- **Assumption 1**. *Independence of the observations*. The two groups or populations being compared should be independent of each other. This means that the observations in one group should not be related or dependent on the observations in the other group.\n\n- **Assumption 2**. *No clear outliers* in either sample. \n\n- **Assumption 3**. *Normality* or *large sample*. The data for each sample should be roughly normally distributed. If the sample size is large enough, the normality assumption is not needed.\n\n- **Assumption 4**. *Equal variance or spread*. The variability in each sample should be about the same. \n\nWhile these assumptions are assessed within the illustrations of the inferential methods using a case study, assumption 1 can be assessed based on study design, histograms and boxplots for assumption 2, quantile-quantile plots for normality component of assumption 3, and boxplots, histograms, and sample standard deviation for assumption 4.  \n\n:::\n\n\nIn these sections, we use R functions to conduct the inferential methods described earlier and provide detailed explanations of their use. While other functions may be used for tasks such as importing data and summarizing data numerically or graphically, we do not discuss them in this section as they have been covered in a previous module.\n\n:::{.callout-important}\n## Notation\n\nTo improve readability, we minimize the use of notation. However, when comparing two populations or treatments denoted by A and B, we use the following notation:\n\n- $\\mu_A$ and $\\mu_B$ denote the population or true mean response under A and B, respectively.\n \n- $\\bar{y}_A$ and $\\bar{y}_B$ denote the sample mean of the corresponding samples obtained from A and B.\n\n- $\\eta_A$ and $\\eta_B$ denote the population or true median response under A and B, respectively.\n \n- $m_A$ and $m_B$ denote the sample median of the corresponding samples obtained from A and B.\n\n:::\n \n \n## Two sample t-based methods for comparing two means \n\nThe independent two-sample t-test is a commonly used hypothesis test for comparing two groups or populations. There are two versions of this test: Student's t-test and Welch's t-test. Both tests can be used to compare whether the difference between the true averages (from two independent populations) is statistically significant or to determine if an effect (when comparing two treatments) is significant. This helps to determine if the difference or effect is due to chance or not. The t-based confidence interval for the difference in true averages provides a range of plausible values for the difference at a specified level of significance. These methods are generally referred to as t-based methods.\n\n\nStudent's two-sample t-based methods assume assumptions 1, 2, 3, and 4 hold. The version of the t-based methods used here are called the Welch two-sample t-based methods, which do not require homogeneity of variances assumption. The Welch two-sample t-test statistic takes the following form:\n\n$$T=\\frac{(\\bar{y}_{A}-\\bar{y}_{B}) -(\\mu_{A}-\\mu_{B})}{\\sqrt{\\frac{s_{A}^2}{n_{A}}+\\frac{s_{A}^2}{n_{B}}}},$$\nwhere $n$ and $s$ denote the sample size and sample standard deviation from each group.\n\nThe data should consist of two variables of interest: a numerical variable that measures the outcome or response of interest and a factor or grouping variable with two levels. The grouping variable distinguishes the different populations or treatments (A or B). The hypothesis takes the form\n\n$$H_0: \\mu_A - \\mu_B=0 \\qquad H_a: \\mu_A -\\mu_B \\neq 0$$\n\nOne may also consider the test in terms of an effect $\\delta_\\mu=\\mu_A - \\mu_B$, \n\n$$H_0: \\delta_\\mu=0 \\qquad H_a: \\delta_\\mu \\neq 0$$\n\nUnder $H_0$, the null distribution of T is a t-distribution with a certain degree of freedom[^1], and it is used to compute the p-value. The function `two.mean.test()`[^2] computes the test statistic and p-value for the two-sample t-test, as well as provide a confidence interval for $\\mu_A - \\mu_B$.\n\n[^1]: For the Welch t-test, the degree of freedom can be tedious to compute. Software will provide the corresponding degrees of freedom and p-value. \n\n[^2]: For a two-sample t-test, `two.mean.test()` is a wrapper function of the R function `t.test`, but `two.mean.test` allows the user to choose the order of the difference.\n\n::: {.callout-tip icon=false}\n\n## R functions\n```\n### two.mean.test( y ~ x , data , first.level,\n###            direction, conf.level,\n###            welch)\n# y: Replace y with the name of the resposne (measured) \n#  variable of interest.\n# x: Replace x with the name of the factor or grouping\n#  variable that distinguishes the different populations \n#  or treatments.\n# data: Set equal to the dataframe name.\n# first.level: Set equal to the level/category from the \n#       grouping variable. It determines how the difference \n#       in sample means is computed.  It should be consistent \n#       with the formulation of the hypothesis.\n# direction: Set equal to the sign in the alternative: \"two.sided\",\n#             \"greater\" , or \"less\".\n# conf.level: Set equal to the confidence level for the CI (default is .95). \n#               The function will always provide a CI by default.\n# welch: Set equal to TRUE (default) for Welch's t-test. Set\n#        to FALSE for Student's t-test.\n```\n:::\n\nMany of the R functions used in this resource will have identical arguments. Therefore, while the arguments will be provided for any functions used, common arguments such as `data`, `direction`, ... will generally only be described once within a given module. \n\n \n::: callout-note\n\nWe apply two-sample t-based methods (hypothesis testing and CI) to the data described in the case study provided in @sec-casestudy3. The goal is to determine if there is a significant difference in PM10 concentration between Kern and Fresno counties and to estimate the difference between the true means.\n\nThe data provided in the case study consists of several variables (`PM10`, `AQI`, ..., `county`, and `City.Name`). A subset of the data was created so that it included  only `PM10` (daily PM10 levels) and `county` (county in which the monitoring site is located) for the two counties of interest. To start, the data are imported and numerical and graphical summaries of the data would be created. However, graphical and numerical data summaries are covered in other modules. The focus here is to assess model assumptions either numerically or graphically. \n\n::: {.panel-tabset}\n## R code  \n\n```{r }\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\n# Import the data and save the data in a \n# dataframe called 'KernFresnoPM10df'\nKernFresnoPM10df <- read.csv(\"datasets/KernFresnoDailyPM10.2021.csv\")\n\n# Load the 'dplyr' package, which provides the 'mutate()' function.\nlibrary(dplyr) \n\n# Tell R that 'county' should be treated as a factor variable by using mutate().\nKernFresnoPM10df <- mutate( KernFresnoPM10df, \n              county= as.factor( county ) )\n\n\n# Print a summary of the data frame, including \n# variable names and other information.\nsummary( KernFresnoPM10df ) \n\n\n# Load the 'lattice' package, which provides the 'bwplot()' function.\nlibrary(lattice) \n\n# Create a box plot of Air Quality Index (AQI) by county levels using the 'bwplot()' function\nbwplot(AQI ~ county,                   #  y ~ x\n       data = KernFresnoPM10df,        # set the data frame to use\n       xlab = \"County\",                # set the x-axis label\n       ylab = \"AQI\",                   # set the y-axis label\n       main = \"AQI Distribution by County\", # set the plot title\n       fill = c(\"gray\", \"brown\"))      # fill the boxes with specified colors\n\n# Load the 'mosaic' package, which allows using \n# formula expressions in 'mean()' and 'sd()'\nlibrary( mosaic ) \n\n# Compute the sample mean for each county\nmean( AQI ~ county , \n   data= KernFresnoPM10df )\n\n# Compute the sample standard deviation for each county\nsd( AQI ~ county , \n  data= KernFresnoPM10df )\n\n```\n\n\n## Video\n\n```{r}\n#| echo: false\n#| eval: true\n\nlibrary(\"vembedr\")\n\nembed_youtube(\"zFTeJ559NIY\")\n\n```\n\n:::\n\nThe difference in samples means between Kern and Fresno county is \n\n$$30.217-35.201=\\bar{x}_K-\\bar{x}_F=-4.984.$$\n\nWhile the observed difference serves as one estimate of $\\mu_K - \\mu_F$, a confidence interval (CI) provides a better sense of the size of the difference between $\\mu_K$ and $\\mu_F$. Additionally, a hypothesis test is used to determine whether this difference is significant, but this depends on certain assumptions about the data. While the samples are not necessarily random, they are independent from each other in the sense that the measurements in Kern county are not affected by the measurements in Fresno county. Although random sampling is ideal, the t-based methods can still be used with non-random samples, but any conclusions drawn only apply to the specific sample and not necessarily to a larger population[^3]. According to the `summary` output, the sample size for each group is large. A common rule of thumb is that a sample is considered large if it contains more than 30 observations[^4]. The boxplots indicate that the variability in PM10 concentration appears to be about the same for both counties, and that there are several outliers present in both counties.\n\n[^3]: It turns out that the null distribution of a t-based method approximates the null distribution of a randomization test fairly well, which does not require random samples.\n\n[^4]: The rule assumes all other assumptions are met. Although a common rule, it can fail when the distribution of the data resembles a heavy-tailed distribution. Please see @sleuth for further discussion on this issue.  \n\nAlthough the boxplots suggest the variability is about the same in both groups, for illustration, we also assess numerically using the sample standard deviations.  The standard deviations for Fresno and Kern are 22.18 and 23.17, respectively, and they appear to be similar. One rule of thumb for comparing the variability in two groups is to take the ratio of their sample standard deviations (22.18/23.17 or 23.17/22.18). If the ratio falls between 0.5 and 2, then the variability in both groups is considered similar enough. However, it is important to note that graphical assessment of assumptions is generally sufficient and this rule of thumb should not be used if outliers are present.\n\n\nOverall, not all data assumptions are met. Alternative methods to be discussed would be more appropriate for this data, but for illustration we analyze the data using the two-sample t-based methods. Let $\\mu_K$ denote the true mean PM10 levels in Kern county. $\\mu_F$ is defined analogous for Fresno county. Since an aim is to determine if there is significant difference between PM10 levels in Kern and Fresno county, the hypothesis is \n\n$$H_0: \\mu_K - \\mu_F=0 \\qquad H_a: \\mu_K- \\mu_F \\neq 0$$\n\nThe following code uses `two.mean.test()` to compute the test statistic, the corresponding p-value, and a 99% confidence interval: \n\n::: {.panel-tabset}\n## R code  \n\n```{r }\n#| label: fig-ttestpm10\n#| fig-cap: The null distribution\n#| layout-ncol: 1\n#| echo: true\n#| message: false\n#| warning: false\n\n# Source the function two.mean.test() so that it's R's memory.\nsource(\"rfuns/two.mean.test.R\")\n\n# Recall that the data was imported and stored in \"KernFresnoPM10df\"\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: The county that appears first in the hypothesis\n#                (Kern county), so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n# conf.level: Set equal .99 since we seek a 99% CI\n# welch: The Welch t-test is the default method here, so using \n#     the default value (TRUE)\n\n\ntwo.mean.test( PM10 ~ county ,          # y ~ x\n               data = KernFresnoPM10df ,# specify the data frame to use\n               first.level = \"Kern\" ,   # specify the first level in the hypothesis\n               direction = \"two.sided\" ,# specify a two-sided alternative\n               conf.level = .99 )       # set the confidence level to .99\n\n```\n\n## Video  \n```{r}\n#| echo: false\n#| eval: true\n\nlibrary(\"vembedr\")\n\nembed_youtube(\"CoCyvWPDEPs\")\n\n```\n\n:::\n\n\nThe output includes the following information: \n\n- The sample mean and standard deviation under each level of the grouping/factor variable. \n\n- The value of the observed statistic: $T=-4.10$\n\n- p-value \n\n- A confidence interval\n\nBased on the output, the observed value of the test statistic is $T=-4.10$. The p-value is extremely small (0). Since p-value$\\leq\\alpha=.01$, we have significant evidence to reject $H_0$. Based on the size of the p-value, we may also say that there is very strong evidence to reject $H_0$. By setting $\\alpha = 0.01$, we require \"stronger evidence\" to reject $H_0$ than if we had used a larger significance level. Therefore, we have strong evidence that there is significant difference between PM10 levels in Kern and Fresno county at the designated monitoring sites. \n\nThe output also provides a 99% CI for the difference in true means. The CI is $(-9.270, -2.794)$, and we are 99% confidence that the difference in true means is anywhere between $-9.270$ and $-2.794$. Since the interval does not contain $0$, it is clear that the true average PM10 levels at Fresno county are higher than in Kern county at the designated monitoring sites at 99% confidence. Note that this conclusion only applies to the monitoring sites, since PM10 levels were not measured at random locations within each county. \n\n \n\n:::\n\n\n\n## Wilcoxon-based methods comparing two medians  \n\nWhen assumption 2 and/or 3 do not hold, t-based methods are no longer applicable. The Wilcoxon-based methods do not require the normality assumption when the samples are small nor is it affected by outliers. Further, the distributions from which each data were sampled do not have to be known[^5] for these methods, but these methods do require for the distributions to have approximately the same shape. Since the distribution does not have to be a known distribution, it called a non-parametric method[^6]. In short, these methods require assumptions 1 and 4, but also require for the distributions from which each data were sampled to have approximately the same shape and for each sample to be of at least size 10.\n\n[^5]: Known distributions are parametric distributions in that they can be expressed using math formulas.  Parametric methods assume that the underlying population data follows a known distribution\n\n[^6]: Non-parametric methods assume less things about the data compare to parametric methods. \n\nThe Wilcoxon rank sum test, also called the Mann-Whitney test, can be applied to compare whether the difference between true medians from two independent populations is really different from zero. It can also be used to determine if an effect (when comparing two treatments via medians) is significant or not, as opposed to the size of the difference or effect being due to random chance.\n\nThe test statistic for the Wilcoxon rank sum test depends on the ranks of the observed data, and its value will be provided by software. However, some details regarding the computation of the test statistic are provided below:\n\n1. List the observations for both samples from smallest to largest across both groups.\n\n2. Assign the numbers $1$ to $n_A+n_B=N$ to the observations (across both groups) with 1 assigned to the smallest observations and $N$ to the largest observation. These are called ranks of the observations.\n\n3. If there are ties (due to repeated values) in the combined data set, the ranks for the observations in a tie are taken to be the average of the ranks for those observations.\n\n4. Let $W$ denote the sum of the ranks for the observations from group A.\n\nThe test statistic then takes the form \n\n$$U=W-\\frac{n_A(n_A+1)}{2}$$\nWhereas the null hypothesis of the two-sample t test is equal means, the null hypothesis of the Wilcoxon test[^7] is taken as equal medians:\n\n$$H_0: \\eta_A - \\eta_B=0 \\qquad H_a: \\eta_A -\\eta_B \\neq 0$$ \n\nOne may also consider the test in terms of an effect $\\delta_\\eta=\\eta_A - \\eta_B$, \n\n$$H_0: \\delta_\\eta=0 \\qquad H_a: \\delta_\\eta \\neq 0$$\n\nUnder $H_0$, the null distribution of U is called the Wilcoxon distribution and it is used to compute the p-value[^8]. The function `two.wilcox.test()`[^9] computes the test statistic and p-value[^10], as well as a CI. \n\nThe CI provided by this method is for the difference in medians between two randomly chosen observations (one from each group)[^11]. This still provides some sense of how different the medians are from each other. However, if both distributions from which the data were sampled are symmetric, then the CI provided is for a difference in medians. \n\n[^7]: If the data distributions do not have approximately the same shape, this test may be used but it is no longer testing a difference in medians, but rather it tests if the median of all pairwise differences (pseudomedian) is not equal to zero. The pseudomedian is not commonly used in practice, but never the less, it can be used to show that two groups are different under this measure. \n\n\n[^8]: The distribution is discrete. However, if a sample size is larger than 50 and there are ties, the distribution is approximated well by a standard normal distribution (default for most software).\n\n\n[^9]: The R function `two.wilcox.test` is a wrapper function for the R function `wilcox.test()`. \n\n[^10]: If there are ties or the sample size is large (rule of thumb is more than 20 in a given group is considered large), the p-value is approximated using a standard normal distribution. The larger the sample, the better the approximation. However, this approximation may be poor especially when the sample size are not very large and/or the underlying distributions of the two groups are highly skewed or have heavy tailed. \n\n[^11]: Recall that the mean and median are measures of central location of a distribution. This CI is for a measure of central location called the pseudomedian, which is the median of differences between all pairs of observations between both groups (not the difference in group medians). For symmetric distributions, the median and pseudomedian coincide.  \n\n::: {.callout-tip icon=false}\n\n## R functions\n```\n### two.wilcox.test( y ~ x , data , first.level,\n###            direction, conf.level)\n# y: Replace y with the name of the resposne (measured) \n#  variable of interest\n# x: Replace x with the name of the factor or grouping\n#  variable that distinguishes the different populations \n#  or treatments\n# data: Set equal to the dataframe being used.\n# first.level: Set equal to a level/category from the grouping \n#              variable. It determines how the difference in \n#              sample means is computed.  It should be consisent \n#              with the formulation of the hypothesis.\n# direction: Set equal to the sign in the alternative: \"two.sided\", \n#            \"greater\" , or \"less\"\n# conf.level: Set equal to the confidence level for the CI (default \n#             is .95). The function will always provide a CI by default.\n```\n:::\n\n\n\n\n \n::: callout-note\n\nWe apply the two-sample Wilcoxon-based methods (hypothesis testing and CI) to the data described in the case study provided in @sec-casestudy3 to determine if there is a significant difference in PM10 concentration between Kern and Fresno county and to get a sense of how different the true medians are. The four assumptions of the data discussed earlier in this chapter were explored when illustrating the t-based methods. Although the data consists of large samples and the independence and equal variance assumptions are reasonable, the presence of many outliers in the data should be noted. The Wilcoxon-based methods are not affected by outliers but the distributions from which each sample were obtained should have the same shape. This is explored via a violin boxplot.\n\n\n::: {.panel-tabset}\n## R code\n\n```{r }\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\nlibrary(lattice) # Provides the bwplot() function\n\n# Create a violin plot \nbwplot( PM10 ~ county , \n    data= KernFresnoPM10df , \n    xlab= \"County\" , \n    ylab= \"PM10\" ,\n    main= \"PM10 vs county\" , \n    panel= panel.violin ) # set equal to panel.violin to create violin plot\n\n```\n\n## Video\n```{r}\n#| echo: false\n#| eval: true\n\nlibrary(\"vembedr\")\n\nembed_youtube(\"CxlMXEWbi3U\")\n\n```\n\n:::\n\nThe violin boxplot makes it clearer that the distribution of PM10 in both counties approximately have the same shape. \n\nLet $\\eta_K$ denote the true median PM10 levels in Kern county. $\\eta_F$ is defined analogous for Fresno county. Since an aim is to determine if there is significant difference between PM10 levels in Kern and Fresno county, the hypothesis is \n\n$$H_0: \\eta_K - \\eta_F=0 \\qquad H_a: \\eta_K- \\eta_F \\neq 0$$\n\nThe following code uses `two.wilcox.test()` to compute the test statistic, the corresponding p-value, and a 99% confidence interval: \n\n \n\n::: {.panel-tabset}\n## R code\n\n```{r }\n#| label: fig-wtestpm10\n#| fig-cap: The null distribution\n#| layout-ncol: 1\n#| echo: true\n#| message: false\n#| warning: false\n\n# Source the function two.wilcox.test() so that it's R's memory.\nsource(\"rfuns/two.wilcox.test.R\")\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: The county that appears first in the hypothesis\n#                (Kern county), set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n\n\ntwo.wilcox.test(PM10 ~ county ,           # y ~ x\n                data = KernFresnoPM10df , # Specify the data frame to use\n                first.level = \"Kern\" ,    # Specify the first level in the hypothesis\n                direction = \"two.sided\" , # Specify a two-sided alternative\n                conf.level = .99 )        # Set the confidence level to .99\n\n\n```\n\n## Video\n```{r}\n#| echo: false\n#| eval: true\n\nlibrary(\"vembedr\")\n\nembed_youtube(\"75m-E5v5w20\")\n\n```\n\n:::\n\nThe output includes the following information: \n\n- The sample median and IQR under each level of the grouping/factor variable. \n\n- The value of the observed statistic \n\n- p-value \n\n- A confidence interval\n\nBased on the output, the observed value of the test statistic is $U=759219.5$. The p-value is extremely small (0). Since p-value$\\leq\\alpha=.01$, we have significant evidence to reject $H_0$.  Therefore, we have strong evidence that there is significant difference between PM10 levels in Kern and Fresno county at the designated monitoring sites. \n\nThe output also provides a 99% CI for median of difference between all pairs of observations between both groups (not the difference in group medians). Based on the CI, we are 99% confident that the median difference between two randomly chosen PM10 levels from each county is anywhere between -8.000 and -4.000.  Since the interval does not contain $0$, it is clear that the PM10 levels at Fresno county are higher than in Kern county at the designated monitoring sites at 99% confidence. Note that this conclusion only applies to the monitoring sites, since PM10 levels were not measured at random locations within each county. \n\n \n\n\n:::\n\n\n \n## A randomization test using a t-test test statistic \n\n\nRandomization methods seek to determine if the observed data in each group is different due to an effect of a factor, compared to what would be expected by random chance if the groups were the same and the factor had no effect on the response. These methods can test whether the observed data in each group are different from a randomization distribution generated by randomly allocating the observed data into the two groups. If the observed data are in fact the same (no effect), then it should be just as likely as any ordering generated by random allocation of the data between groups. In other words, if the factor being studied has no effect on the response, then the behavior of the data should be similar to what we would expect if the observations were randomly assigned to the two groups (randomized group assignment). If, however, the groups are different due to a significant factor effect, then the behavior of the data would not be similar to what we would expect if the observations were randomly assigned to the two groups.\"\n\n\nIf assumption 2, 3, and/or 3 do not hold, t-based or Wilcoxon methods are no longer applicable. Randomization technically does not require any assumptions, although some assumptions or conditions may be needed depending on how the effect is being quantified (differences in mean, medians, or other measures). Here, we use randomization tests to determine if an effect is significant when using a difference in means and difference in medians. Keep in mind that randomization tests do not have sample size requirements. However, the less data you have, the less power the test has to detect a significant effect or significant difference between two groups.\n\n\n### Comparing means {-}\n\nHere we use a randomization test when quantifying the attribute using the mean. As a test statistic, we use the form of the test statistic provided by Welch's t-test for the randomization test. The interest here is in determining if an effect (when measures as a difference in means) is significant. Since the test statistic uses both the sample mean and sample standard deviation, this version of the randomization test will require assumption 2. The R function used to conduct the randomization test has similar syntax to that of the t-based methods, but with additional arguments.\n\n\n::: {.callout-tip icon=false}\n\n## R functions\n```\n### two.mean.test( y ~ x , data , first.level,\n###            direction, randtest, \n###            nshuffles)\n# randtest: Set equal to TRUE to obtain the test statistic and p-value\n#      for the randomization test (for comparing means).\n# nshuffles: Set equal to the desired number of randomizations. \n```\n\n:::\n\n::: callout-note\n\nDiscussion of the t-based methods showed that outliers are present in the data from the case study discussed in @sec-casestudy3, so this test is not appropriate for the data. However, for illustration we carry out the test on the data. The aim is to determine if there is significant county effect on PM10 levels when considering Kern and Fresno county. Let $\\delta_\\mu=\\mu_K - \\mu_F$. The hypothesis is \n\n$$H_0: \\delta_\\mu=0 \\qquad H_a: \\delta_\\mu \\neq 0$$\n\nThe following code uses `two.mean.test()` to compute the test statistic and the corresponding p-value for a randomization test involving a difference in means. \n\n::: {.panel-tabset}\n## R code\n\n```{r }\n#| label: fig-rttestpm10\n#| fig-cap: The null distribution\n#| layout-ncol: 1\n#| echo: true\n#| eval: false\n#| message: false\n#| warning: false\n\n# Data was imported earlier in the section \n# and 'mutate' converted 'county' into a factor variable.\n\n# Source the function two.mean.test() so that it's R's memory.\nsource(\"rfuns/two.mean.test.R\")\n\n# The function will require the following information:\n# y: Replace with 'PM10'\n# x: Replace with 'county'\n# data: Set equal to a 'KernFresnoPM10df'\n# first.level: In the hypothesis, Kern county appeared first in the\n#       difference, so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n# randtest: Set equal to TRUE. \n# nshuffles: Set equal to 30,000 \n\n\ntwo.mean.test(PM10 ~ county ,           # specify the variables for the test\n              data = KernFresnoPM10df , # specify the data frame to use\n              first.level = \"Kern\" ,    # specify the first level in the hypothesis\n              direction = \"two.sided\" , # specify a two-sided alternative\n              randtest = TRUE ,         # perform a simulation-based test\n              nshuffles = 30000 )       # set the number of randomizations to 30,000\n\n\n\n#>   Simulation based two-sample test for independent samples \n#>               \n#> formula: PM10 ~ county \n#> sample mean of Kern group: 35.06717 \n#> sample mean of Fresno group: 41.09943 \n#> sample sd of Kern group: 32.26383 \n#> sample sd of Fresno group: 31.88051 \n\n#> difference between groups: ( Kern group ) - ( Fresno group ) \n#> obs t-test statistic: -4.807498       p-value = 0 \n#> df= N/A \n#> direction: two.sided \n\n```\n\n## Video\n```{r}\n#| echo: false\n#| eval: true\n\nlibrary(\"vembedr\")\n\nembed_youtube(\"Kbkt9vf7eiU\")\n\n```\n\n:::\n\nThe output is nearly identical as when conducting a Welch t-test in that it provides some summary statistics, the observed value of the test statistic, and p-value. These type of randomization methods do not provide CIs so one can not assess the size of the effect. At $\\alpha=.01$, we obtain the same decision and conclusion as with the previous tests. \n \n\n:::\n\n\n### Comparing medians {-}\n\nHere we quantify the attribute using the median. For the test statistic in the randomization test, we use the form of the test statistic provided by the Wilcox rank sum test. The interest here is in determining if an effect (when measures as a difference in medians) is significant.  The R function to carry out this test is the same function as with the wilcoxon-based methods, but with some additional arguments to conduct the randomization test.\n\n::: {.callout-tip icon=false}\n\n## R functions\n```\n### two.wilcox.test( y ~ x , data , first.level,\n###            direction, conf.level)\n###            randtest, nshuffles)\n# randtest: Set equal to TRUE to obtain the test statistic and p-value\n#      for the randomization test (for comparing medians).\n# nshuffles: Set equal to the desired number of randomizations. \n```\n:::\n\n::: callout-note\nDiscussion of the wilcoxon-based methods showed that the data described the case study provided in @sec-casestudy3 are appropriate for the data. However, given the large sample, the Wilcox rank sum test approximates the p-value using the standard normal distribution. This approximation generally works well, but the quality of the approximation declines when the underlying distributions of the two groups are highly skewed or have heavy tails, or as the number of ties increases. Note below that the histogram shows that the PM10 levels in both counties are right skewed. Further, there are over 2000 ties in the data. Therefore, this randomization test provides a more accurate result for this data.\n\n \n```{r }\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\n# Data was imported earlier in the section \n# and 'mutate' converted 'county' into a factor variable.\n\n# Load the 'lattice' package\nlibrary(lattice) # provides the histogram() function\nhistogram(~ PM10 | county ,         # ~ \"variable\" | \"grouping factor\"\n          data = KernFresnoPM10df , # Specify the data frame to use\n          xlab = \"PM10\" ,           # Label the x-axis\n          ylab = \"Frequency\" ,      # Label the y-axis\n          main = \"PM10 by county\" ) # Add a title to the plot\n\n```\n \n\n\nTo determine if there is significant county effect on PM10 levels when comparing Kern and Fresno county, let $\\delta_\\eta=\\eta_K - \\eta_F$. The hypotheses are \n\n$$H_0: \\delta_\\eta=0 \\qquad H_a: \\delta_\\eta \\neq 0$$\n\nThe following code uses `two.wilcox.test()` to compute the test statistic and the corresponding p-value for a randomization test involving a difference in medians \n\n::: {.panel-tabset}\n## R code\n\n```{r }\n#| label: fig-rtmedtestpm10\n#| fig-cap: The null distribution\n#| layout-ncol: 1\n#| echo: true\n#| eval: false\n#| message: false\n#| warning: false\n\n# Data was imported earlier in the section \n# and 'mutate' converted 'county' into a factor variable.\n\n# Source the function two.wilcox.test() so that it's R's memory.\nsource(\"rfuns/two.wilcox.test.R\")\n\n# The function will require the following information:\n# y: replace with 'PM10'\n# x: replace with 'county'\n# data: set equal to a 'KernFresnoPM10df'\n# first.level: In the hypothesis, Kern county appeared first in the\n#       difference, so set equal to \"Kern\"\n# direction: It is a two-sided alternative, so set equal to \"two.sided\"\n# randtest: Set equal to TRUE. \n# nshuffles: Set equal to 30,000 \n\ntwo.wilcox.test(PM10 ~ county ,           # Specify the variables for the test\n                data = KernFresnoPM10df , # Specify the data frame to use\n                first.level = \"Kern\" ,    # Specify the first level in the hypothesis\n                direction = \"two.sided\" , # Specify a two-sided alternative\n                randtest = TRUE ,         # Perform a simulation-based test\n                nshuffles = 30000 )       # Set the number of randomizations to 30,000\n#>   Simulation based two-sample test for independent samples \n              \n#> formula: PM10 ~ county \n#> sample median of Kern group: 28 \n#> sample median of Fresno group: 34 \n#> sample IQR of Kern group: 30 \n#> sample IQR of Fresno group: 29 \n\n#> method: randomization test \n#> difference between groups: ( Kern group ) - ( Fresno group ) \n#> obs test statistic: U= 759219.5       p-value = 0 \n#> direction: two.sided \n\n```\n\n## Video\n```{r}\n#| echo: false\n#| eval: true\n\nlibrary(\"vembedr\")\n\nembed_youtube(\"JkBLLFUfkbg\")\n\n```\n\n:::\n\nThe output is nearly identical as when conducting the Wilcox rank-sum test in that it provides summary statistics, the observed value of the test statistic, and p-value. These types of randomization methods do not provide CIs. At $\\alpha=.01$, we obtain the same decision and conclusion as with the previous tests. \n\n \n\n\n:::\n\n\n## Sample size estimation and power analysis \n\nIdeally, estimating sample size for a study is one of the first steps that researchers take prior to collecting data.  Knowing the sample size required to detect a desired effect at the beginning of a project allows one to manage their data collection efforts. Further, this allows for one to determine how much statistical power the test will have to detect an effect. \n\nThe R function `pwr.t2n.test()` from the `pwr` R package can be used to calculate statistical power for a two sample t-test when the sample sizes, significance level ($\\alpha$), and effect size are provided. The function `pwr.t.test` from the same package will provide the sample sizes required for a given power level, significant level ($\\alpha$), and effect size. Note that variability in each sample is assumed to be about the same. It is assumed the appropriate assumptions about the data are met[^12].\n\n[^12]: The R function `power.welch.t.test()` and `sim.ssize.wilcox.test()` from the `MKpower` R package provides statistical power calculations for the two-sample Welch t-test and two-sample Wilcox rank sum test, respectively. However, these functions may be difficult to use for those not familiar with power analysis.\n \n::: {.callout-tip icon=false}\n\n## R functions\n```\n### power.t2n.test( n1 , n2 , d , sig.level , \n###                    power, alternative)\n# n1: Set equal to the number of observations in first sample.\n# n2: Set equal to the effect size.\n# d: Set equal to the effect size.\n# sig.level: Set equal to the desired alpha value (significance level)\n# power: Set equal to the desired power (a number between 0 and 1). \n# alternative: The direction of the alternative. Set equal to \"two.sided\".\n#\n#\n### power.t.test(power, d , sig.level , \n###                  alternative , type)\n# power: Set equal to the desired power.\n# type: Set equal to \"two.sample\" .\n#\n```\n\n:::\n\nThe effect size refers to Cohen's $d$, which is defined as the difference between the true means divided by the pooled standard deviation. Cohen's $d$ is typically interpreted as follows:\n\n- Small effect size: $d = 0.20$\n\n- Medium effect size: $d = 0.50$ \n\n- Large effect size: $d = 0.80$ or higher.\n\nThese are suggested guidelines and may vary slightly depending on the specific field of research or context of the study. \n\n \n\n::: callout-note\n\nRefer to the data described in the case study provided in @sec-casestudy3. Let's suppose the goal is to determine the statistical power of a two sample t-test if one wanted to detect a medium-sized effect when the sample size from each county is 100 at $\\alpha=.01$. \n```{r }\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\n\nlibrary(pwr) # provides pwr.t2n.test\n\n# Note:  The output of this function call will be the statistical power for the t-test.\npwr.t2n.test(n1 = 100 ,          # Set the sample size for group 1\n             n2 = 100 ,          # Set the sample size for group 2\n             d = 0.50 ,          # Set Cohen's d\n             sig.level = 0.01 ,  # Set the significance level for the test\n             alternative = \"two.sided\") # Specify a two-sided alternative hypothesis\n\n```\n \nThe statistical power of this test is $.824$. To further increase the power, one may increase the effect size (the larger it is, the easier it is to detect), increase the value of $\\alpha$ (make it easier to reject $H_0$ and find a significant effect), and/or increase the sample sizes. The sample data consisted of at least 1000 observations from each county, so set each sample to $1000$:\n```{r }\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\npwr.t2n.test(n1 = 1000 ,\n             n2 = 1000 , \n             d = .50 , \n             sig.level = 0.01 ,\n             alternative = \"two.sided\" )\n\n```\n\nIf instead you seek the required sample size for a given power, the argument `power` could be set to the desired power in the function `pwr.t.test()`:\n```{r }\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\n# Note:  The output of this function call will be the required sample size for the t-test.\npwr.t.test(power = 0.90 ,         # Set the desired power level\n           d = 0.50 ,             # Set the standardized mean difference between groups\n           sig.level = 0.01 ,     # Set the significance level for the test\n           type = \"two.sample\" ,  # Set a two-sample t-test\n           alternative = \"two.sided\") # Specify a two-sided alternative hypothesis\n\n```\n\n:::\n \n\nThis module provides a few methods for independent two-sample inference. Parametric methods (such as t-based methods) are generally more powerful (assuming all assumptions are reasonably met) than nonparametric methods, such as Wilcoxon-based methods. Nonparametric tests are based on fewer assumptions compared to their parametric counterparts. The cost of fewer assumptions is that nonparametric tests are generally less powerful than their parametric counterparts. Nonparametric tests are used in cases where parametric tests are not appropriate. \n\n "},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"twoindptmeans.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.35","bibliography":["references.bib"],"knitr":{"opts_chunk":{"collapse":true,"comment":"#>","echo":true,"message":false,"fig.align":"center","warning":false,"prompt":false,"eval":true,"results":"show"}},"theme":"cyborg","cover-image":"cover.jpeg"},"extensions":{"book":{"multiFile":true}}}}}